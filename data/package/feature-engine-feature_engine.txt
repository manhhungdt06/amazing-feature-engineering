Directory structure:
└── feature_engine/
    ├── __init__.py
    ├── dataframe_checks.py
    ├── py.typed
    ├── tags.py
    ├── _base_transformers/
    │   ├── __init__.py
    │   ├── base_numerical.py
    │   └── mixins.py
    ├── _check_init_parameters/
    │   ├── __init__.py
    │   ├── check_init_input_params.py
    │   ├── check_input_dictionary.py
    │   └── check_variables.py
    ├── _prediction/
    │   ├── __init__.py
    │   ├── base_predictor.py
    │   ├── target_mean_classifier.py
    │   └── target_mean_regressor.py
    ├── creation/
    │   ├── __init__.py
    │   ├── base_creation.py
    │   ├── cyclical_features.py
    │   ├── decision_tree_features.py
    │   ├── math_features.py
    │   └── relative_features.py
    ├── datasets/
    │   ├── __init__.py
    │   └── titanic.py
    ├── datetime/
    │   ├── __init__.py
    │   ├── _datetime_constants.py
    │   ├── datetime.py
    │   └── datetime_subtraction.py
    ├── discretisation/
    │   ├── __init__.py
    │   ├── arbitrary.py
    │   ├── base_discretiser.py
    │   ├── decision_tree.py
    │   ├── equal_frequency.py
    │   ├── equal_width.py
    │   └── geometric_width.py
    ├── encoding/
    │   ├── __init__.py
    │   ├── _helper_functions.py
    │   ├── base_encoder.py
    │   ├── count_frequency.py
    │   ├── decision_tree.py
    │   ├── mean_encoding.py
    │   ├── one_hot.py
    │   ├── ordinal.py
    │   ├── rare_label.py
    │   ├── similarity_encoder.py
    │   └── woe.py
    ├── imputation/
    │   ├── __init__.py
    │   ├── arbitrary_number.py
    │   ├── base_imputer.py
    │   ├── categorical.py
    │   ├── drop_missing_data.py
    │   ├── end_tail.py
    │   ├── mean_median.py
    │   ├── missing_indicator.py
    │   └── random_sample.py
    ├── outliers/
    │   ├── __init__.py
    │   ├── artbitrary.py
    │   ├── base_outlier.py
    │   ├── trimmer.py
    │   └── winsorizer.py
    ├── pipeline/
    │   ├── __init__.py
    │   └── pipeline.py
    ├── preprocessing/
    │   ├── __init__.py
    │   ├── match_categories.py
    │   └── match_columns.py
    ├── scaling/
    │   ├── __init__.py
    │   └── mean_normalization.py
    ├── selection/
    │   ├── __init__.py
    │   ├── _selection_constants.py
    │   ├── base_recursive_selector.py
    │   ├── base_selection_functions.py
    │   ├── base_selector.py
    │   ├── drop_constant_features.py
    │   ├── drop_correlated_features.py
    │   ├── drop_duplicate_features.py
    │   ├── drop_features.py
    │   ├── drop_psi_features.py
    │   ├── information_value.py
    │   ├── mrmr.py
    │   ├── probe_feature_selection.py
    │   ├── recursive_feature_addition.py
    │   ├── recursive_feature_elimination.py
    │   ├── shuffle_features.py
    │   ├── single_feature_performance.py
    │   ├── smart_correlation_selection.py
    │   └── target_mean_selection.py
    ├── timeseries/
    │   ├── __init__.py
    │   └── forecasting/
    │       ├── __init__.py
    │       ├── base_forecast_transformers.py
    │       ├── expanding_window_features.py
    │       ├── lag_features.py
    │       └── window_features.py
    ├── transformation/
    │   ├── __init__.py
    │   ├── arcsin.py
    │   ├── boxcox.py
    │   ├── log.py
    │   ├── power.py
    │   ├── reciprocal.py
    │   └── yeojohnson.py
    ├── variable_handling/
    │   ├── __init__.py
    │   ├── _variable_type_checks.py
    │   ├── check_variables.py
    │   ├── dtypes.py
    │   ├── find_variables.py
    │   └── retain_variables.py
    └── wrappers/
        ├── __init__.py
        └── wrappers.py

================================================
FILE: feature_engine/__init__.py
================================================
import pathlib

import feature_engine

PACKAGE_ROOT = pathlib.Path(feature_engine.__file__).resolve().parent
VERSION_PATH = PACKAGE_ROOT / "VERSION"

name = "feature_engine"

with open(VERSION_PATH, "r") as version_file:
    __version__ = version_file.read().strip()

================================================
FILE: feature_engine/dataframe_checks.py
================================================

from typing import List, Tuple, Union

import numpy as np
import pandas as pd
from scipy.sparse import issparse
from sklearn.utils.validation import _check_y, check_consistent_length, column_or_1d

def check_X(X: Union[np.generic, np.ndarray, pd.DataFrame]) -> pd.DataFrame:
    if isinstance(X, pd.DataFrame):
        if not X.columns.is_unique:
            raise ValueError("Input data contains duplicated variable names.")
        X = X.copy()

    elif isinstance(X, (np.generic, np.ndarray)):
        if X.ndim == 0:
            raise ValueError(
                "Expected 2D array, got scalar array instead:\narray={}.\n"
                "Reshape your data either using array.reshape(-1, 1) if "
                "your data has a single feature or array.reshape(1, -1) "
                "if it contains a single sample.".format(X)
            )
        if X.ndim == 1:
            raise ValueError(
                "Expected 2D array, got 1D array instead:\narray={}.\n"
                "Reshape your data either using array.reshape(-1, 1) if "
                "your data has a single feature or array.reshape(1, -1) "
                "if it contains a single sample.".format(X)
            )

        X = pd.DataFrame(X)
        X.columns = [f"x{i}" for i in range(X.shape[1])]

    elif issparse(X):
        raise TypeError("This transformer does not support sparse matrices.")

    else:
        raise TypeError(
            f"X must be a numpy array or pandas dataframe. Got {type(X)} instead."
        )

    if X.empty:
        raise ValueError(
            "0 feature(s) (shape=%s) while a minimum of %d is required." % (X.shape, 1)
        )

    return X

def check_y(
    y: Union[np.generic, np.ndarray, pd.Series, pd.DataFrame, List],
    y_numeric: bool = False,
) -> pd.Series:

    if y is None:
        raise ValueError(
            "requires y to be passed, but the target y is None",
            "Expected array-like (array or non-string sequence), got None",
            "y should be a 1d array",
        )

    elif isinstance(y, pd.Series):
        if y.isnull().any():
            raise ValueError("y contains NaN values.")
        if y.dtype != "O" and not np.isfinite(y).all():
            raise ValueError("y contains infinity values.")
        if y_numeric and y.dtype == "O":
            y = y.astype("float")
        y = y.copy()

    elif isinstance(y, pd.DataFrame):
        if y.isnull().any().any():
            raise ValueError("y contains NaN values.")
        if not np.isfinite(y).all().all():
            raise ValueError("y contains infinity values.")
        y = y.copy()

    else:
        try:
            y = column_or_1d(y)
            y = _check_y(y, multi_output=False, y_numeric=y_numeric)
            y = pd.Series(y).copy()
        except ValueError:
            y = _check_y(y, multi_output=True, y_numeric=y_numeric)
            y = pd.DataFrame(y).copy()
    return y

def check_X_y(
    X: Union[np.generic, np.ndarray, pd.DataFrame],
    y: Union[np.generic, np.ndarray, pd.Series, List],
    y_numeric: bool = False,
) -> Tuple[pd.DataFrame, pd.Series]:

    def _check_X_y(X, y):
        X = check_X(X)
        y = check_y(y, y_numeric=y_numeric)
        check_consistent_length(X, y)
        return X, y

    if isinstance(X, pd.DataFrame) and isinstance(y, (pd.Series, pd.DataFrame)):
        X, y = _check_X_y(X, y)
        if X.index.equals(y.index) is False:
            raise ValueError("The indexes of X and y do not match.")

    if isinstance(X, pd.DataFrame) and not isinstance(y, (pd.Series, pd.DataFrame)):
        X, y = _check_X_y(X, y)
        y.index = X.index

    elif not isinstance(X, pd.DataFrame) and isinstance(y, (pd.Series, pd.DataFrame)):
        X, y = _check_X_y(X, y)
        X.index = y.index

    else:
        X, y = _check_X_y(X, y)

    return X, y

def _check_X_matches_training_df(X: pd.DataFrame, reference: int) -> None:

    if X.shape[1] != reference:
        raise ValueError(
            "The number of columns in this dataset is different from the one used to "
            "fit this transformer (when using the fit() method)."
        )

    return None

def _check_contains_na(
    X: pd.DataFrame,
    variables: List[Union[str, int]],
) -> None:

    if X[variables].isnull().any().any():
        raise ValueError(
            "Some of the variables in the dataset contain NaN. Check and "
            "remove those before using this transformer."
        )

def _check_optional_contains_na(
    X: pd.DataFrame, variables: List[Union[str, int]]
) -> None:

    if X[variables].isnull().any().any():
        raise ValueError(
            "Some of the variables in the dataset contain NaN. Check and "
            "remove those before using this transformer or set the parameter "
            "`missing_values='ignore'` when initialising this transformer."
        )

def _check_contains_inf(X: pd.DataFrame, variables: List[Union[str, int]]) -> None:

    if np.isinf(X[variables]).any().any():
        raise ValueError(
            "Some of the variables to transform contain inf values. Check and "
            "remove those before using this transformer."
        )

================================================
FILE: feature_engine/py.typed
================================================
[Empty file]

================================================
FILE: feature_engine/tags.py
================================================
import sklearn
from sklearn.utils.fixes import parse_version

sklearn_version = parse_version(parse_version(sklearn.__version__).base_version)

def _return_tags():
    tags = {
        "preserves_dtype": [],
        "_xfail_checks": {
            "check_complex_data": "Test not needed.",
            "check_dtype_object": "Feature-engine transformers use dtypes to select "
            "between numerical and categorical variables. Feature-engine trusts the "
            "user casts the variables appropriately",
            "check_transformer_data_not_an_array": "Ok to fail",
            "check_sample_weights_not_an_array": "Ok to fail",
            "check_methods_sample_order_invariance": "Test does not work on dataframes",
            "check_fit_idempotent": "Test does not work on dataframes.",
            "check_fit2d_predict1d": "Test not relevant, Feature-engine transformers "
            "only work with dataframes.",
        },
    }

    if sklearn_version > parse_version("1.6"):
        msg1 = "against Feature-engines design."
        msg2 = "Our transformers do not preserve dtype."
        all_fail = {
            "check_do_not_raise_errors_in_init_or_set_params": msg1,
            "check_transformer_preserve_dtypes": msg2,
            "check_n_features_in_after_fitting": "not sure why it fails, we do check.",
        }
        tags["_xfail_checks"].update(all_fail)
    return tags

================================================
FILE: feature_engine/_base_transformers/__init__.py
================================================
[Empty file]

================================================
FILE: feature_engine/_base_transformers/base_numerical.py
================================================

import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.utils.validation import check_is_fitted

from feature_engine._base_transformers.mixins import GetFeatureNamesOutMixin
from feature_engine.dataframe_checks import (
    _check_contains_inf,
    _check_contains_na,
    _check_X_matches_training_df,
    check_X,
)
from feature_engine.tags import _return_tags
from feature_engine.variable_handling import (
    check_numerical_variables,
    find_numerical_variables,
)

class BaseNumericalTransformer(
    TransformerMixin, BaseEstimator, GetFeatureNamesOutMixin
):

    def fit(self, X: pd.DataFrame) -> pd.DataFrame:

        X = check_X(X)

        if self.variables is None:
            self.variables_ = find_numerical_variables(X)
        else:
            self.variables_ = check_numerical_variables(X, self.variables)

        _check_contains_na(X, self.variables_)
        _check_contains_inf(X, self.variables_)

        self.feature_names_in_ = X.columns.tolist()

        self.n_features_in_ = X.shape[1]

        return X

    def _check_transform_input_and_state(self, X: pd.DataFrame) -> pd.DataFrame:

        check_is_fitted(self)

        X = check_X(X)

        _check_X_matches_training_df(X, self.n_features_in_)

        _check_contains_na(X, self.variables_)
        _check_contains_inf(X, self.variables_)

        X = X[self.feature_names_in_]

        return X

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["variables"] = "numerical"
        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        return tags

================================================
FILE: feature_engine/_base_transformers/mixins.py
================================================
from typing import Dict, List, Union

import pandas as pd
from numpy import ndarray
from numpy.typing import ArrayLike
from sklearn.utils.validation import check_is_fitted

from feature_engine.dataframe_checks import (
    _check_contains_inf,
    _check_contains_na,
    check_X,
    check_X_y,
)
from feature_engine.variable_handling import check_numerical_variables

class TransformXyMixin:
    def transform_x_y(self, X: pd.DataFrame, y: pd.Series):
        X, y = check_X_y(X, y)
        X = self.transform(X)
        y = y.loc[X.index]
        return X, y

class FitFromDictMixin:
    def _fit_from_dict(self, X: pd.DataFrame, user_dict_: Dict) -> pd.DataFrame:
        X = check_X(X)

        variables = list(user_dict_.keys())
        self.variables_ = check_numerical_variables(X, variables)

        _check_contains_na(X, self.variables_)
        _check_contains_inf(X, self.variables_)

        self.feature_names_in_ = X.columns.tolist()

        self.n_features_in_ = X.shape[1]

        return X

class GetFeatureNamesOutMixin:
    def get_feature_names_out(
        self,
        input_features: Union[List[Union[str, int]], ArrayLike] = None,
    ) -> List[Union[str, int]]:
        check_is_fitted(self)

        if input_features is not None:
            if self.feature_names_in_ == [f"x{i}" for i in range(self.n_features_in_)]:

                if len(input_features) == self.n_features_in_:
                    if isinstance(input_features, list):
                        feature_names = input_features
                    else:
                        feature_names = list(input_features)

                    feature_names = self._add_new_feature_names(feature_names)

                    feature_names = self._remove_feature_names(
                        feature_names, indices=True
                    )

                    return feature_names

                else:
                    raise ValueError(
                        "The number of input_features does not match the number of "
                        "features seen in the dataframe used in fit."
                    )
            else:
                msg = "input_features is not equal to feature_names_in_"
                if isinstance(input_features, list):
                    if input_features != self.feature_names_in_:
                        raise ValueError(msg)
                elif isinstance(input_features, ndarray) or isinstance(
                    input_features, pd.core.indexes.base.Index
                ):
                    if list(input_features) != self.feature_names_in_:
                        raise ValueError(msg)
                else:
                    raise ValueError(
                        "input_features must be a list or an array. "
                        "Got {input_features} instead."
                    )

        feature_names = self.feature_names_in_

        feature_names = self._add_new_feature_names(feature_names)

        feature_names = self._remove_feature_names(feature_names, indices=False)

        return feature_names

    def _add_new_feature_names(self, feature_names):
        if hasattr(self, "_get_new_features_name") and callable(
            self._get_new_features_name
        ):
            feature_names = feature_names + self._get_new_features_name()

            if self.drop_original is True and self.variables_ is not None:
                feature_names = [f for f in feature_names if f not in self.variables_]

        return feature_names

    def _remove_feature_names(self, feature_names, indices=False) -> List:
        if hasattr(self, "features_to_drop_"):
            if indices is True:
                mask = self.get_support(indices=True)
                feature_names = [feature_names[i] for i in mask]
            else:
                feature_names = [
                    f for f in feature_names if f not in self.features_to_drop_
                ]
        return feature_names

================================================
FILE: feature_engine/_check_init_parameters/__init__.py
================================================
[Empty file]

================================================
FILE: feature_engine/_check_init_parameters/check_init_input_params.py
================================================
def _check_param_missing_values(missing_values):
    if missing_values not in ["raise", "ignore"]:
        raise ValueError(
            "missing_values takes only values 'raise' or 'ignore'. "
            f"Got {missing_values} instead."
        )

def _check_param_drop_original(drop_original):
    if not isinstance(drop_original, bool):
        raise ValueError(
            "drop_original takes only boolean values True and False. "
            f"Got {drop_original} instead."
        )

================================================
FILE: feature_engine/_check_init_parameters/check_input_dictionary.py
================================================
from typing import Optional

def _check_numerical_dict(dict_: Optional[dict]) -> Optional[dict]:

    if isinstance(dict_, dict):
        if not all([isinstance(x, (float, int)) for x in dict_.values()]):
            raise ValueError(
                "All values in the dictionary must be integer or float. "
                f"Got {dict_} instead."
            )

    elif dict_ is not None:
        raise TypeError(
            f"The parameter can only take a dictionary or None. Got {dict_} instead."
        )
    return None

================================================
FILE: feature_engine/_check_init_parameters/check_variables.py
================================================
from typing import Any, List, Union

Variables = Union[None, int, str, List[Union[str, int]]]

def _check_variables_input_value(variables: Variables) -> Any:

    msg = (
        "`variables` should contain a string, an integer or a list of strings or "
        f"integers. Got {variables} instead."
    )
    msg_dupes = "The list entered in `variables` contains duplicated variable names."
    msg_empty = "The list of `variables` is empty."

    if variables is not None:
        if isinstance(variables, list):
            if not all(isinstance(i, (str, int)) for i in variables):
                raise ValueError(msg)
            if len(variables) == 0:
                raise ValueError(msg_empty)
            if len(variables) != len(set(variables)):
                raise ValueError(msg_dupes)
        else:
            if not isinstance(variables, (str, int)):
                raise ValueError(msg)
    return variables

================================================
FILE: feature_engine/_prediction/__init__.py
================================================
[Empty file]

================================================
FILE: feature_engine/_prediction/base_predictor.py
================================================
from typing import List, Union

import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator
from sklearn.pipeline import Pipeline
from sklearn.utils.validation import check_is_fitted

from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine.dataframe_checks import (
    _check_contains_inf,
    _check_contains_na,
    _check_X_matches_training_df,
    check_X,
    check_X_y,
)
from feature_engine.discretisation import (
    EqualFrequencyDiscretiser,
    EqualWidthDiscretiser,
)
from feature_engine.encoding import MeanEncoder
from feature_engine.tags import _return_tags
from feature_engine.variable_handling import find_categorical_and_numerical_variables

class BaseTargetMeanEstimator(BaseEstimator):

    def __init__(
        self,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        bins: int = 5,
        strategy: str = "equal_width",
    ):

        if not isinstance(bins, int):
            raise ValueError(f"bins must be an integer. Got {bins} instead.")

        if strategy not in ["equal_width", "equal_frequency"]:
            raise ValueError(
                "strategy takes only values 'equal_width' or 'equal_frequency'. "
                f"Got {strategy} instead."
            )

        self.variables = _check_variables_input_value(variables)
        self.bins = bins
        self.strategy = strategy

    def fit(self, X: pd.DataFrame, y: pd.Series):
        X, y = check_X_y(X, y)

        (
            self.variables_categorical_,
            self.variables_numerical_,
        ) = find_categorical_and_numerical_variables(X, self.variables)

        _check_contains_na(X, self.variables_numerical_)
        _check_contains_na(X, self.variables_categorical_)

        _check_contains_inf(X, self.variables_numerical_)

        if self.variables_categorical_ and self.variables_numerical_:
            self._pipeline = self._make_combined_pipeline()

        elif self.variables_categorical_:
            self._pipeline = self._make_categorical_pipeline()

        else:
            self._pipeline = self._make_numerical_pipeline()

        self._pipeline.fit(X, y)

        if self.variables_categorical_ and self.variables_numerical_:
            self.binner_dict_ = dict(
                self._pipeline.named_steps["discretiser"].binner_dict_
            )
            self.encoder_dict_ = dict(
                self._pipeline.named_steps["encoder_num"].encoder_dict_
            )
            tmp_dict = dict(self._pipeline.named_steps["encoder_cat"].encoder_dict_)
            self.encoder_dict_.update(tmp_dict)

        elif self.variables_categorical_:
            self.binner_dict_ = {}
            self.encoder_dict_ = dict(self._pipeline.encoder_dict_)

        else:
            self.binner_dict_ = dict(
                self._pipeline.named_steps["discretiser"].binner_dict_
            )
            self.encoder_dict_ = dict(
                self._pipeline.named_steps["encoder"].encoder_dict_
            )

        self.n_features_in_ = X.shape[1]
        self.feature_names_in_ = list(X.columns)

        return self

    def _make_numerical_pipeline(self):
        encoder = MeanEncoder(variables=self.variables_numerical_, unseen="raise")

        pipeline = Pipeline(
            [
                ("discretiser", self._make_discretiser()),
                ("encoder", encoder),
            ]
        )

        return pipeline

    def _make_categorical_pipeline(self):

        pipeline = MeanEncoder(variables=self.variables_categorical_, unseen="raise")

        return pipeline

    def _make_combined_pipeline(self):

        encoder_num = MeanEncoder(variables=self.variables_numerical_, unseen="raise")
        encoder_cat = MeanEncoder(variables=self.variables_categorical_, unseen="raise")

        pipeline = Pipeline(
            [
                ("discretiser", self._make_discretiser()),
                ("encoder_num", encoder_num),
                ("encoder_cat", encoder_cat),
            ]
        )

        return pipeline

    def _make_discretiser(self):
        if self.strategy == "equal_width":
            discretiser = EqualWidthDiscretiser(
                bins=self.bins,
                variables=self.variables_numerical_,
                return_boundaries=True,
            )
        else:
            discretiser = EqualFrequencyDiscretiser(
                q=self.bins,
                variables=self.variables_numerical_,
                return_boundaries=True,
            )

        return discretiser

    def _transform(self, X: pd.DataFrame) -> pd.DataFrame:
        check_is_fitted(self)

        X = check_X(X)

        _check_X_matches_training_df(X, self.n_features_in_)

        _check_contains_na(X, self.variables_numerical_)
        _check_contains_na(X, self.variables_categorical_)

        _check_contains_inf(X, self.variables_numerical_)

        X = X[self.feature_names_in_]

        X_tr = self._pipeline.transform(X)

        return X_tr

    def _predict(self, X: pd.DataFrame) -> np.ndarray:
        X_tr = self._transform(X)

        predictions = (
            X_tr[self.variables_numerical_ + self.variables_categorical_]
            .mean(axis=1)
            .to_numpy()
        )

        return predictions

    def _more_tags(self):
        return _return_tags()

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        return tags

================================================
FILE: feature_engine/_prediction/target_mean_classifier.py
================================================
import numpy as np
import pandas as pd
from sklearn.base import ClassifierMixin
from sklearn.utils.multiclass import check_classification_targets, unique_labels

from feature_engine._prediction.base_predictor import BaseTargetMeanEstimator

class TargetMeanClassifier(ClassifierMixin, BaseTargetMeanEstimator):

    def fit(self, X: pd.DataFrame, y: pd.Series):
        check_classification_targets(y)

        self.classes_ = unique_labels(y)

        if len(self.classes_) > 2:
            raise NotImplementedError(
                "This classifier is designed for binary classification only. "
                "The target has more than 2 unique values."
            )

        if any(x for x in self.classes_ if x not in [0, 1]):
            y = np.where(y == unique_labels(y)[0], 0, 1)

        return super().fit(X, y)

    def predict_proba(self, X: pd.DataFrame) -> np.ndarray:
        prob = self._predict(X)
        return np.vstack([1 - prob, prob]).T

    def predict_log_proba(self, X: pd.DataFrame) -> np.ndarray:
        return np.log(self.predict_proba(X))

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        y_pred = np.where(self._predict(X) > 0.5, self.classes_[1], self.classes_[0])
        return y_pred

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        tags.classifier_tags.multi_class = False
        return tags

================================================
FILE: feature_engine/_prediction/target_mean_regressor.py
================================================
import numpy as np
import pandas as pd
from sklearn.base import RegressorMixin
from sklearn.utils.multiclass import type_of_target

from feature_engine._prediction.base_predictor import BaseTargetMeanEstimator

class TargetMeanRegressor(RegressorMixin, BaseTargetMeanEstimator):

    def fit(self, X: pd.DataFrame, y: pd.Series):

        if type_of_target(y) == "binary":
            raise ValueError(
                "Trying to fit a regression to a binary target is not "
                "allowed by this transformer. "
            )

        return super().fit(X, y)

    def predict(self, X: pd.DataFrame) -> np.ndarray:
        return self._predict(X)

================================================
FILE: feature_engine/creation/__init__.py
================================================
from .cyclical_features import CyclicalFeatures
from .decision_tree_features import DecisionTreeFeatures
from .math_features import MathFeatures
from .relative_features import RelativeFeatures

__all__ = [
    "DecisionTreeFeatures",
    "MathFeatures",
    "RelativeFeatures",
    "CyclicalFeatures",
]

================================================
FILE: feature_engine/creation/base_creation.py
================================================
from typing import Optional

import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.utils.validation import check_is_fitted

from feature_engine._base_transformers.mixins import GetFeatureNamesOutMixin
from feature_engine._check_init_parameters.check_init_input_params import (
    _check_param_drop_original,
    _check_param_missing_values,
)
from feature_engine.dataframe_checks import (
    _check_contains_inf,
    _check_contains_na,
    _check_X_matches_training_df,
    check_X,
)
from feature_engine.tags import _return_tags
from feature_engine.variable_handling import (
    check_numerical_variables,
    find_numerical_variables,
)

class BaseCreation(TransformerMixin, BaseEstimator, GetFeatureNamesOutMixin):

    def __init__(
        self,
        missing_values: str = "raise",
        drop_original: bool = False,
    ) -> None:

        _check_param_missing_values(missing_values)
        _check_param_drop_original(drop_original)

        self.missing_values = missing_values
        self.drop_original = drop_original

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):

        X = check_X(X)

        if self.variables is None:
            self.variables_ = find_numerical_variables(X)
        else:
            self.variables_ = check_numerical_variables(X, self.variables)

        if hasattr(self, "reference"):
            check_numerical_variables(X, self.reference)

        if self.missing_values == "raise":
            _check_contains_na(X, self.variables_)
            _check_contains_inf(X, self.variables_)
            if hasattr(self, "reference"):
                _check_contains_na(X, self.reference)
                _check_contains_inf(X, self.reference)

        self.feature_names_in_ = X.columns.tolist()

        self.n_features_in_ = X.shape[1]

        return self

    def _check_transform_input_and_state(self, X: pd.DataFrame) -> pd.DataFrame:

        check_is_fitted(self)

        X = check_X(X)

        _check_X_matches_training_df(X, self.n_features_in_)

        if self.missing_values == "raise":
            _check_contains_na(X, self.variables_)
            _check_contains_inf(X, self.variables_)
            if hasattr(self, "reference"):
                _check_contains_na(X, self.reference)
                _check_contains_inf(X, self.reference)

        X = X[self.feature_names_in_]

        return X

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["allow_nan"] = True
        tags_dict["variables"] = "skip"
        tags_dict["_xfail_checks"][
            "check_parameters_default_constructible"
        ] = "transformer has 1 mandatory parameter"
        tags_dict["_xfail_checks"][
            "check_fit2d_1feature"
        ] = "this transformer works with datasets that contain at least 2 variables. \
        Otherwise, there is nothing to combine"
        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        tags.input_tags.allow_nan = True
        return tags

================================================
FILE: feature_engine/creation/cyclical_features.py
================================================
from typing import Dict, List, Optional, Union

import numpy as np
import pandas as pd

from feature_engine._base_transformers.base_numerical import BaseNumericalTransformer
from feature_engine._base_transformers.mixins import (
    FitFromDictMixin,
    GetFeatureNamesOutMixin,
)
from feature_engine._check_init_parameters.check_init_input_params import (
    _check_param_drop_original,
)
from feature_engine._check_init_parameters.check_input_dictionary import (
    _check_numerical_dict,
)
from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _drop_original_docstring,
    _variables_numerical_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_transform_docstring,
    _transform_creation_docstring,
)
from feature_engine._docstrings.substitute import Substitution

@Substitution(
    variables=_variables_numerical_docstring,
    drop_original=_drop_original_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit_transform=_fit_transform_docstring,
    transform=_transform_creation_docstring,
)
class CyclicalFeatures(
    BaseNumericalTransformer, FitFromDictMixin, GetFeatureNamesOutMixin
):

    def __init__(
        self,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        max_values: Optional[Dict[str, Union[int, float]]] = None,
        drop_original: Optional[bool] = False,
    ) -> None:

        _check_numerical_dict(max_values)
        _check_param_drop_original(drop_original)

        self.variables = _check_variables_input_value(variables)
        self.max_values = max_values
        self.drop_original = drop_original

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):
        if self.max_values is None:
            X = super().fit(X)
            self.max_values_ = X[self.variables_].max().to_dict()
        else:
            super()._fit_from_dict(X, self.max_values)
            self.max_values_ = self.max_values

        return self

    def transform(self, X: pd.DataFrame):
        X = self._check_transform_input_and_state(X)

        for variable in self.variables_:
            max_value = self.max_values_[variable]
            X[f"{variable}_sin"] = np.sin(X[variable] * (2.0 * np.pi / max_value))
            X[f"{variable}_cos"] = np.cos(X[variable] * (2.0 * np.pi / max_value))

        if self.drop_original:
            X.drop(columns=self.variables_, inplace=True)

        return X

    def _get_new_features_name(self) -> List:
        feature_names = [
            f"{var}_{suffix}" for var in self.variables_ for suffix in ["sin", "cos"]
        ]
        return feature_names

================================================
FILE: feature_engine/creation/decision_tree_features.py
================================================
import itertools
from typing import Any, Dict, Iterable, List, Optional, Union

import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.utils.multiclass import check_classification_targets, type_of_target
from sklearn.utils.validation import check_is_fitted

from feature_engine._base_transformers.mixins import GetFeatureNamesOutMixin
from feature_engine._check_init_parameters.check_init_input_params import (
    _check_param_drop_original,
    _check_param_missing_values,
)
from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _drop_original_docstring,
    _missing_values_docstring,
    _variables_numerical_docstring,
)
from feature_engine._docstrings.init_parameters.creation import _features_to_combine
from feature_engine._docstrings.methods import (
    _fit_transform_docstring,
    _transform_creation_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import (
    _check_contains_inf,
    _check_contains_na,
    _check_X_matches_training_df,
    check_X,
    check_X_y,
)
from feature_engine.tags import _return_tags
from feature_engine.variable_handling import (
    check_numerical_variables,
    find_numerical_variables,
)

@Substitution(
    variables=_variables_numerical_docstring,
    features_to_combine=_features_to_combine,
    missing_values=_missing_values_docstring,
    drop_original=_drop_original_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    transform=_transform_creation_docstring,
    fit_transform=_fit_transform_docstring,
)
class DecisionTreeFeatures(TransformerMixin, BaseEstimator, GetFeatureNamesOutMixin):

    def __init__(
        self,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        features_to_combine: Optional[Union[Iterable[Any], int]] = None,
        precision: Union[int, None] = None,
        cv=3,
        scoring: str = "neg_mean_squared_error",
        param_grid: Optional[Dict[str, Union[str, int, float, List[int]]]] = None,
        regression: bool = True,
        random_state: int = 0,
        missing_values: str = "raise",
        drop_original: bool = False,
    ) -> None:

        if precision is not None and (not isinstance(precision, int) or precision < 1):
            raise ValueError(
                "precision must be None or a positive integer. "
                f"Got {precision} instead."
            )

        if not isinstance(regression, bool):
            raise ValueError(
                f"regression must be a boolean value. Got {regression} instead."
            )

        _check_param_missing_values(missing_values)
        _check_param_drop_original(drop_original)

        self.variables = _check_variables_input_value(variables)
        self.features_to_combine = features_to_combine
        self.precision = precision
        self.cv = cv
        self.scoring = scoring
        self.param_grid = param_grid
        self.regression = regression
        self.random_state = random_state
        self.missing_values = missing_values
        self.drop_original = drop_original

    def fit(self, X: pd.DataFrame, y: pd.Series):
        if self.regression is True:
            if type_of_target(y) == "binary":
                raise ValueError(
                    "Trying to fit a regression to a binary target is not "
                    "allowed by this transformer. Check the target values "
                    "or set regression to False."
                )
        else:
            check_classification_targets(y)
            self._is_binary = type_of_target(y)

        X, y = check_X_y(X, y)

        if self.variables is None:
            variables_ = find_numerical_variables(X)
        else:
            variables_ = check_numerical_variables(X, self.variables)

        _check_contains_na(X, variables_)
        _check_contains_inf(X, variables_)

        if self.param_grid is not None:
            param_grid = self.param_grid
        else:
            param_grid = {"max_depth": [1, 2, 3, 4]}

        input_features = self._create_variable_combinations(
            how_to_combine=self.features_to_combine, variables=variables_
        )

        estimators_ = []
        for features in input_features:
            estimator = self._make_decision_tree(param_grid=param_grid)

            if isinstance(features, str):
                estimator.fit(X[features].to_frame(), y)
            else:
                estimator.fit(X[features], y)

            estimators_.append(estimator)

        self.variables_ = variables_
        self.input_features_ = input_features
        self.estimators_ = estimators_
        self.feature_names_in_ = X.columns.tolist()
        self.n_features_in_ = X.shape[1]

        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        check_is_fitted(self)

        X = check_X(X)

        _check_X_matches_training_df(X, self.n_features_in_)

        _check_contains_na(X, self.variables_)
        _check_contains_inf(X, self.variables_)

        X = X[self.feature_names_in_]

        if self.regression is True:
            for features, estimator in zip(self.input_features_, self.estimators_):
                if isinstance(features, str):
                    preds = estimator.predict(X[features].to_frame())
                    if self.precision is not None:
                        preds = np.round(preds, self.precision)
                    X.loc[:, f"tree({features})"] = preds
                else:
                    preds = estimator.predict(X[features])
                    if self.precision is not None:
                        preds = np.round(preds, self.precision)
                    X.loc[:, f"tree({features})"] = preds

        elif self._is_binary == "binary":
            for features, estimator in zip(self.input_features_, self.estimators_):
                if isinstance(features, str):
                    preds = estimator.predict_proba(X[features].to_frame())
                    if self.precision is not None:
                        preds = np.round(preds, self.precision)
                    X.loc[:, f"tree({features})"] = preds[:, 1]
                else:
                    preds = estimator.predict_proba(X[features])
                    if self.precision is not None:
                        preds = np.round(preds, self.precision)
                    X.loc[:, f"tree({features})"] = preds[:, 1]

        else:
            for features, estimator in zip(self.input_features_, self.estimators_):
                if isinstance(features, str):
                    preds = estimator.predict(X[features].to_frame())
                    X.loc[:, f"tree({features})"] = preds
                else:
                    preds = estimator.predict(X[features])
                    X.loc[:, f"tree({features})"] = preds

        if self.drop_original:
            X.drop(columns=self.variables_, inplace=True)

        return X

    def _make_decision_tree(self, param_grid: Dict):
        if self.regression is True:
            est = DecisionTreeRegressor(random_state=self.random_state)
        else:
            est = DecisionTreeClassifier(random_state=self.random_state)

        tree_model = GridSearchCV(
            est,
            cv=self.cv,
            scoring=self.scoring,
            param_grid=param_grid,
        )

        return tree_model

    def _create_variable_combinations(
        self,
        variables: List,
        how_to_combine: Optional[Union[Iterable[Any], int]] = None,
    ) -> List[Any]:
        combos = []
        if isinstance(how_to_combine, tuple):
            for feature in how_to_combine:
                if isinstance(feature, str):
                    combos.append([feature])
                else:
                    combos.append(list(feature))

        else:
            if how_to_combine is None:
                if len(variables) == 1:
                    combos = variables
                else:
                    for i in range(1, len(variables) + 1):
                        els = [list(x) for x in itertools.combinations(variables, i)]
                        combos += els

            elif isinstance(how_to_combine, int):
                for i in range(1, how_to_combine + 1):
                    els = [list(x) for x in itertools.combinations(variables, i)]
                    combos += els

            else:
                for i in how_to_combine:
                    els = [list(x) for x in itertools.combinations(variables, i)]
                    combos += els

        return [item[0] if len(item) == 1 else item for item in combos]

    def _get_new_features_name(self) -> List:
        feature_names = [f"tree({combo})" for combo in self.input_features_]
        return feature_names

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["requires_y"] = True
        tags_dict["variables"] = "numerical"
        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        return tags

================================================
FILE: feature_engine/creation/math_features.py
================================================
from typing import Any, List, Optional, Union

import pandas as pd

from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _drop_original_docstring,
    _missing_values_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_not_learn_docstring,
    _fit_transform_docstring,
    _transform_creation_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.creation.base_creation import BaseCreation

@Substitution(
    missing_values=_missing_values_docstring,
    drop_original=_drop_original_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit=_fit_not_learn_docstring,
    transform=_transform_creation_docstring,
    fit_transform=_fit_transform_docstring,
)
class MathFeatures(BaseCreation):

    def __init__(
        self,
        variables: List[Union[str, int]],
        func: Any,
        new_variables_names: Optional[List[str]] = None,
        missing_values: str = "raise",
        drop_original: bool = False,
    ) -> None:

        if (
            not isinstance(variables, list)
            or not all(isinstance(var, (int, str)) for var in variables)
            or len(variables) < 2
            or len(set(variables)) != len(variables)
        ):
            raise ValueError(
                "variables must be a list of strings or integers with at least 2 "
                f"different variables. Got {variables} instead."
            )

        if isinstance(func, dict):
            raise NotImplementedError(
                "func does not work with dictionaries in this transformer."
            )

        if new_variables_names is not None:
            if (
                not isinstance(new_variables_names, list)
                or not all(isinstance(var, str) for var in new_variables_names)
                or len(set(new_variables_names)) != len(new_variables_names)
            ):
                raise ValueError(
                    "new_variable_names should be None or a list of unique strings. "
                    f"Got {new_variables_names} instead."
                )

        if new_variables_names is not None:
            if isinstance(func, list):
                if len(new_variables_names) != len(func):
                    raise ValueError(
                        "The number of new feature names must coincide with the number "
                        "of functions."
                    )
            else:
                if len(new_variables_names) != 1:
                    raise ValueError(
                        "The number of new feature names must coincide with the number "
                        "of functions."
                    )

        super().__init__(missing_values, drop_original)

        self.variables = variables
        self.func = func
        self.new_variables_names = new_variables_names

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X = self._check_transform_input_and_state(X)

        new_variable_names = self._get_new_features_name()

        if len(new_variable_names) == 1:
            X[new_variable_names[0]] = X[self.variables].agg(self.func, axis=1)
        else:
            X[new_variable_names] = X[self.variables].agg(self.func, axis=1)

        if self.drop_original:
            X.drop(columns=self.variables, inplace=True)

        return X

    def _get_new_features_name(self) -> List:

        if self.new_variables_names is not None:
            feature_names = self.new_variables_names

        else:
            varlist = [f"{var}" for var in self.variables_]

            if isinstance(self.func, list):
                functions = [
                    fun if type(fun) is str else fun.__name__ for fun in self.func
                ]
                feature_names = [
                    f"{function}_{'_'.join(varlist)}" for function in functions
                ]
            else:
                feature_names = [f"{self.func}_{'_'.join(varlist)}"]

        return feature_names

================================================
FILE: feature_engine/creation/relative_features.py
================================================
from typing import List, Union

import pandas as pd

from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _drop_original_docstring,
    _missing_values_docstring,
    _variables_numerical_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_not_learn_docstring,
    _fit_transform_docstring,
    _transform_creation_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.creation.base_creation import BaseCreation

_PERMITTED_FUNCTIONS = [
    "add",
    "sub",
    "mul",
    "div",
    "truediv",
    "floordiv",
    "mod",
    "pow",
]

@Substitution(
    variables=_variables_numerical_docstring,
    missing_values=_missing_values_docstring,
    drop_original=_drop_original_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit=_fit_not_learn_docstring,
    transform=_transform_creation_docstring,
    fit_transform=_fit_transform_docstring,
)
class RelativeFeatures(BaseCreation):

    def __init__(
        self,
        variables: List[Union[str, int]],
        reference: List[Union[str, int]],
        func: List[str],
        fill_value: Union[int, float, None] = None,
        missing_values: str = "ignore",
        drop_original: bool = False,
    ) -> None:

        if (
            not isinstance(variables, list)
            or not all(isinstance(var, (int, str)) for var in variables)
            or len(set(variables)) != len(variables)
        ):
            raise ValueError(
                "variables must be a list of strings or integers. "
                f"Got {variables} instead."
            )

        if (
            not isinstance(reference, list)
            or not all(isinstance(var, (int, str)) for var in reference)
            or len(set(reference)) != len(reference)
        ):
            raise ValueError(
                "reference must be a list of strings or integers. "
                f"Got {reference} instead."
            )

        if (
            not isinstance(func, list)
            or any(fun not in _PERMITTED_FUNCTIONS for fun in func)
            or len(set(func)) != len(func)
        ):
            raise ValueError(
                "At least one of the entered functions is not supported or you entered "
                "duplicated functions. "
                "Supported functions are {}. ".format(", ".join(_PERMITTED_FUNCTIONS))
            )

        if fill_value is not None and not isinstance(fill_value, (float, int)):
            raise ValueError(
                "fill_value must be a float, integer or None. "
                f"Got {fill_value} instead."
            )
        super().__init__(missing_values, drop_original)
        self.variables = variables
        self.reference = reference
        self.func = func
        self.fill_value = fill_value

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:

        X = self._check_transform_input_and_state(X)

        methods_dict = {
            "add": self._add,
            "mul": self._mul,
            "sub": self._sub,
            "div": self._div,
            "truediv": self._truediv,
            "floordiv": self._floordiv,
            "mod": self._mod,
            "pow": self._pow,
        }

        for func in self.func:
            methods_dict[func](X)

        if self.drop_original:
            X.drop(
                columns=set(self.variables + self.reference),
                inplace=True,
            )

        return X

    def _sub(self, X):
        for reference in self.reference:
            varname = [f"{var}_sub_{reference}" for var in self.variables]
            X[varname] = X[self.variables].sub(X[reference], axis=0)
        return X

    def _add(self, X):
        for reference in self.reference:
            varname = [f"{var}_add_{reference}" for var in self.variables]
            X[varname] = X[self.variables].add(X[reference], axis=0)
        return X

    def _mul(self, X):
        for reference in self.reference:
            varname = [f"{var}_mul_{reference}" for var in self.variables]
            X[varname] = X[self.variables].mul(X[reference], axis=0)
        return X

    def _div(self, X):
        for reference in self.reference:
            zeros_ix, contains_zero = self._find_zeroes_in_reference(X, reference)

            if self.fill_value is None and contains_zero:
                self._raise_error_when_zero_in_denominator()

            varname = [f"{var}_div_{reference}" for var in self.variables]
            X[varname] = X[self.variables].div(X[reference], axis=0)

            if contains_zero:
                X.loc[zeros_ix, varname] = self.fill_value
        return X

    def _truediv(self, X):
        for reference in self.reference:
            zeros_ix, contains_zero = self._find_zeroes_in_reference(X, reference)

            if self.fill_value is None and contains_zero:
                self._raise_error_when_zero_in_denominator()

            varname = [f"{var}_truediv_{reference}" for var in self.variables]
            X[varname] = X[self.variables].truediv(X[reference], axis=0)

            if contains_zero:
                X.loc[zeros_ix, varname] = self.fill_value
        return X

    def _floordiv(self, X):
        for reference in self.reference:
            zeros_ix, contains_zero = self._find_zeroes_in_reference(X, reference)

            if self.fill_value is None and contains_zero:
                self._raise_error_when_zero_in_denominator()

            varname = [f"{var}_floordiv_{reference}" for var in self.variables]
            X[varname] = X[self.variables].floordiv(X[reference], axis=0)

            if contains_zero:
                X.loc[zeros_ix, varname] = self.fill_value
        return X

    def _mod(self, X):
        for reference in self.reference:
            zeros_ix, contains_zero = self._find_zeroes_in_reference(X, reference)

            if self.fill_value is None and contains_zero:
                self._raise_error_when_zero_in_denominator()

            varname = [f"{var}_mod_{reference}" for var in self.variables]
            X[varname] = X[self.variables].mod(X[reference], axis=0)

            if contains_zero:
                X.loc[zeros_ix, varname] = self.fill_value
        return X

    def _pow(self, X):
        for reference in self.reference:
            varname = [f"{var}_pow_{reference}" for var in self.variables]
            X[varname] = X[self.variables].pow(X[reference], axis=0)
        return X

    def _raise_error_when_zero_in_denominator(self):
        raise ValueError(
            "Some of the reference variables contain zeroes. Division by zero "
            "does not exist. Replace zeros before using this transformer for division "
            "or set `fill_value` to a number."
        )

    def _find_zeroes_in_reference(self, X, var):
        zero_ix = X[var] == 0
        zero_bool = (zero_ix).any()
        return zero_ix, zero_bool

    def _get_new_features_name(self) -> List:

        feature_names = [
            f"{var}_{fun}_{reference}"
            for fun in self.func
            for reference in self.reference
            for var in self.variables
        ]
        return feature_names

================================================
FILE: feature_engine/datasets/__init__.py
================================================
from .titanic import load_titanic

__all__ = ["load_titanic"]

================================================
FILE: feature_engine/datasets/titanic.py
================================================
import numpy as np
import pandas as pd
from sklearn.pipeline import Pipeline

from feature_engine.imputation import CategoricalImputer, MeanMedianImputer

def load_titanic(
    return_X_y_frame=False, predictors_only=False, handle_missing=False, cabin=None
):
    if not isinstance(return_X_y_frame, bool):
        raise ValueError(
            "return_X_y_frame takes only booleans True and False. "
            f"Got {return_X_y_frame} instead."
        )

    if not isinstance(predictors_only, bool):
        raise ValueError(
            "predictors_only takes only booleans True and False. "
            f"Got {predictors_only} instead."
        )

    if not isinstance(handle_missing, bool):
        raise ValueError(
            "handle_missing takes only booleans True and False. "
            f"Got {handle_missing} instead."
        )

    if cabin is not None:
        if not isinstance(cabin, str) or cabin not in ["letter_only", "drop"]:
            raise ValueError(
                "the parameter 'cabin' takes only values None, 'letter_only' and "
                f"'drop'. Got {cabin} instead."
            )

    df = pd.read_csv("https://www.openml.org/data/get_csv/16826755/phpMYEkMl")
    df = df.replace("?", np.nan)
    df["age"] = df["age"].astype("float64")
    df["fare"] = df["fare"].astype("float64")

    if predictors_only:
        df = df.drop(
            columns=["name", "ticket", "home.dest", "boat", "body"],
        )

    if handle_missing:
        pipeline = Pipeline(
            steps=[
                (
                    "categorical_imputer",
                    CategoricalImputer(imputation_method="missing"),
                ),
                ("mean_median_imputer", MeanMedianImputer(imputation_method="mean")),
            ]
        )

        df = pipeline.fit_transform(df)

    if cabin == "letter_only":
        df["cabin"] = df["cabin"].astype(str).str[0]
    elif cabin == "drop":
        df = df.drop(columns=["cabin"])

    if return_X_y_frame:
        X = df.drop(columns="survived")
        y = df["survived"]
        return X, y
    else:
        return df

================================================
FILE: feature_engine/datetime/__init__.py
================================================
"The module datetime computes features from dates and times."

from .datetime import DatetimeFeatures
from .datetime_subtraction import DatetimeSubtraction

__all__ = ["DatetimeFeatures", "DatetimeSubtraction"]

================================================
FILE: feature_engine/datetime/_datetime_constants.py
================================================
import numpy as np

FEATURES_SUPPORTED = [
    "month",
    "quarter",
    "semester",
    "year",
    "week",
    "day_of_week",
    "day_of_month",
    "day_of_year",
    "weekend",
    "month_start",
    "month_end",
    "quarter_start",
    "quarter_end",
    "year_start",
    "year_end",
    "leap_year",
    "days_in_month",
    "hour",
    "minute",
    "second",
]

FEATURES_DEFAULT = [
    "month",
    "year",
    "day_of_week",
    "day_of_month",
    "hour",
    "minute",
    "second",
]

FEATURES_SUFFIXES = {
    "month": "_month",
    "quarter": "_quarter",
    "semester": "_semester",
    "year": "_year",
    "week": "_week",
    "day_of_week": "_day_of_week",
    "day_of_month": "_day_of_month",
    "day_of_year": "_day_of_year",
    "weekend": "_weekend",
    "month_start": "_month_start",
    "month_end": "_month_end",
    "quarter_start": "_quarter_start",
    "quarter_end": "_quarter_end",
    "year_start": "_year_start",
    "year_end": "_year_end",
    "leap_year": "_leap_year",
    "days_in_month": "_days_in_month",
    "hour": "_hour",
    "minute": "_minute",
    "second": "_second",
}

FEATURES_FUNCTIONS = {
    "month": lambda x: x.dt.month,
    "quarter": lambda x: x.dt.quarter,
    "semester": lambda x: np.where(x.dt.month <= 6, 1, 2).astype(np.int64),
    "year": lambda x: x.dt.year,
    "week": lambda x: x.dt.isocalendar().week.astype(np.int64),
    "day_of_week": lambda x: x.dt.dayofweek,
    "day_of_month": lambda x: x.dt.day,
    "day_of_year": lambda x: x.dt.dayofyear,
    "weekend": lambda x: np.where(x.dt.dayofweek <= 4, 0, 1).astype(np.int64),
    "month_start": lambda x: x.dt.is_month_start.astype(np.int64),
    "month_end": lambda x: x.dt.is_month_end.astype(np.int64),
    "quarter_start": lambda x: x.dt.is_quarter_start.astype(np.int64),
    "quarter_end": lambda x: x.dt.is_quarter_end.astype(np.int64),
    "year_start": lambda x: x.dt.is_year_start.astype(np.int64),
    "year_end": lambda x: x.dt.is_year_end.astype(np.int64),
    "leap_year": lambda x: x.dt.is_leap_year.astype(np.int64),
    "days_in_month": lambda x: x.dt.days_in_month.astype(np.int64),
    "hour": lambda x: x.dt.hour,
    "minute": lambda x: x.dt.minute,
    "second": lambda x: x.dt.second,
}

================================================
FILE: feature_engine/datetime/datetime.py
================================================

from typing import List, Optional, Union

import pandas as pd
from pandas.api.types import is_datetime64_any_dtype as is_datetime
from pandas.api.types import is_numeric_dtype as is_numeric
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.utils.validation import check_is_fitted

from feature_engine._base_transformers.mixins import GetFeatureNamesOutMixin
from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_not_learn_docstring,
    _fit_transform_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import (
    _check_contains_na,
    _check_X_matches_training_df,
    check_X,
)
from feature_engine.datetime._datetime_constants import (
    FEATURES_DEFAULT,
    FEATURES_FUNCTIONS,
    FEATURES_SUFFIXES,
    FEATURES_SUPPORTED,
)
from feature_engine.variable_handling._variable_type_checks import (
    _is_categorical_and_is_datetime,
)
from feature_engine.variable_handling.check_variables import check_datetime_variables
from feature_engine.variable_handling.find_variables import find_datetime_variables

@Substitution(
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit=_fit_not_learn_docstring,
    fit_transform=_fit_transform_docstring,
)
class DatetimeFeatures(TransformerMixin, BaseEstimator, GetFeatureNamesOutMixin):

    def __init__(
        self,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        features_to_extract: Union[None, str, List[str]] = None,
        drop_original: bool = True,
        missing_values: str = "raise",
        dayfirst: bool = False,
        yearfirst: bool = False,
        utc: Union[None, bool] = None,
        format: Union[None, str] = None,
    ) -> None:

        if features_to_extract:
            if not (
                isinstance(features_to_extract, list) or features_to_extract == "all"
            ):
                raise ValueError(
                    "features_to_extract must be a list of strings or 'all'. "
                    f"Got {features_to_extract} instead."
                )
            elif isinstance(features_to_extract, list) and any(
                feat not in FEATURES_SUPPORTED for feat in features_to_extract
            ):
                raise ValueError(
                    "Some of the requested features are not supported. "
                    "Supported features are {}.".format(", ".join(FEATURES_SUPPORTED))
                )

        if not isinstance(drop_original, bool):
            raise ValueError(
                "drop_original takes only booleans True or False. "
                f"Got {drop_original} instead."
            )

        if missing_values not in ["raise", "ignore"]:
            raise ValueError(
                "missing_values takes only values 'raise' or 'ignore'. "
                f"Got {missing_values} instead."
            )

        if utc is not None and not isinstance(utc, bool):
            raise ValueError("utc takes only booleans or None. " f"Got {utc} instead.")

        self.variables = _check_variables_input_value(variables)
        self.drop_original = drop_original
        self.missing_values = missing_values
        self.dayfirst = dayfirst
        self.yearfirst = yearfirst
        self.utc = utc
        self.features_to_extract = features_to_extract
        self.format = format

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):
        X = check_X(X)

        if self.variables == "index":

            if not (
                is_datetime(X.index)
                or (
                    not is_numeric(X.index) and _is_categorical_and_is_datetime(X.index)
                )
            ):
                raise TypeError("The dataframe index is not datetime.")

            self.variables_ = []

        elif self.variables is None:
            self.variables_ = find_datetime_variables(X)

        else:
            self.variables_ = check_datetime_variables(X, self.variables)

        if self.missing_values == "raise":
            if self.variables == "index":
                self._check_index_contains_na(X.index)
            else:
                _check_contains_na(X, self.variables_)

        if self.features_to_extract is None:
            self.features_to_extract_ = FEATURES_DEFAULT
        elif isinstance(self.features_to_extract, str):
            self.features_to_extract_ = FEATURES_SUPPORTED
        else:
            self.features_to_extract_ = self.features_to_extract

        self.feature_names_in_ = X.columns.tolist()

        self.n_features_in_ = X.shape[1]

        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:

        check_is_fitted(self)

        X = check_X(X)

        _check_X_matches_training_df(X, self.n_features_in_)

        X = X[self.feature_names_in_]

        if self.variables == "index":
            if self.missing_values == "raise":
                self._check_index_contains_na(X.index)

            idx_datetime = pd.Series(
                pd.to_datetime(
                    X.index,
                    dayfirst=self.dayfirst,
                    yearfirst=self.yearfirst,
                    utc=self.utc,
                    format=self.format,
                ),
                index=X.index,
            )

            for feat in self.features_to_extract_:
                X[FEATURES_SUFFIXES[feat][1:]] = FEATURES_FUNCTIONS[feat](idx_datetime)

        else:
            if self.missing_values == "raise":
                _check_contains_na(X, self.variables_)

            datetime_df = pd.concat(
                [
                    pd.to_datetime(
                        X[variable],
                        dayfirst=self.dayfirst,
                        yearfirst=self.yearfirst,
                        utc=self.utc,
                        format=self.format,
                    )
                    for variable in self.variables_
                ],
                axis=1,
            )

            for var in self.variables_:
                for feat in self.features_to_extract_:
                    X[str(var) + FEATURES_SUFFIXES[feat]] = FEATURES_FUNCTIONS[feat](
                        datetime_df[var]
                    )
            if self.drop_original:
                X.drop(self.variables_, axis=1, inplace=True)

        return X

    def _get_new_features_name(self) -> List:

        if self.variables == "index":
            feature_names = [
                FEATURES_SUFFIXES[feat][1:] for feat in self.features_to_extract_
            ]
        else:
            feature_names = [
                str(var) + FEATURES_SUFFIXES[feat]
                for var in self.variables_
                for feat in self.features_to_extract_
            ]

        return feature_names

    def _check_index_contains_na(self, index: pd.Index):
        if index.isnull().any():
            raise ValueError(
                "The dataframe index contains missing data. "
                "Check and remove those before using this transformer "
                "or set missing_values to False."
            )

    def _more_tags(self):
        tags_dict = {"variables": "datetime"}
        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        return tags

================================================
FILE: feature_engine/datetime/datetime_subtraction.py
================================================
from typing import List, Optional, Union

import numpy as np
import pandas as pd
from pandas.api.types import is_datetime64_any_dtype as is_datetime
from sklearn.utils.validation import check_is_fitted

from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _missing_values_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_not_learn_docstring,
    _fit_transform_docstring,
    _transform_creation_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.creation.base_creation import BaseCreation
from feature_engine.dataframe_checks import (
    _check_contains_na,
    _check_X_matches_training_df,
    check_X,
)
from feature_engine.variable_handling.check_variables import check_datetime_variables
from feature_engine.variable_handling.find_variables import find_datetime_variables

_example = """
    >>> import pandas as pd
    >>> from feature_engine.datetime import DatetimeSubtraction
    >>> X = pd.DataFrame({
    >>>     "date1": ["2022-09-18", "2022-10-27", "2022-12-24"],
    >>>     "date2": ["2022-08-18", "2022-08-27", "2022-06-24"]})
    >>> dtf = DatetimeSubtraction(variables=["date1"], reference=["date2"])
    >>> dtf.fit(X)
    >>> dtf.transform(X)
            date1       date2  date1_sub_date2
    0  2022-09-18  2022-08-18             31.0
    1  2022-10-27  2022-08-27             61.0
    2  2022-12-24  2022-06-24            183.0
    DatetimeSubtraction() applies datetime subtraction between a group of datetime
    variables and one or more datetime features, adding the resulting variables to the
    dataframe.

    DatetimeSubtraction() works with variables cast as datetime or object. It subtracts
    the variables listed in the parameter `reference` from those listed in the
    parameter `variables`.

    More details in the :ref:`User Guide <datetime_subtraction>`.

    Parameters
    ----------
    variables: list
        The list of datetime variables that the reference variables will be subtracted
        from (left side of the subtraction operation).

    reference: list
        The list of datetime reference variables that will be subtracted from
        `variables` (right side of the subtraction operation).

    new_variables_names: list, default=None
        Names of the new variables. You have the option to pass a list with the names
        you'd like to assing to the new variables. If `None`, the transformer will
        assign arbitrary names.

    output_unit: string, default='D'
        The string representation of the output unit of the datetime differences.
        The default is `D` for day. This parameter is passed to `numpy.timedelta64`.
        Other possible values are  `Y` for year, `M` for month,  `W` for week,
        `h` for hour, `m` for minute, `s` for second, `ms` for millisecond,
        `us` or `μs` for microsecond, `ns` for nanosecond, `ps` for picosecond,
        `fs` for femtosecond and `as` for attosecond.

    {missing_values}

    drop_original: bool, default="False"
        If `True`, the variables listed in `variables` and `reference` will be dropped
        from the dataframe after the computation of the new features.

    dayfirst: bool, default="False"
        Specify a date parse order if arg is str or is list-like. If True, parses
        dates with the day first, e.g. 10/11/12 is parsed as 2012-11-10. Same as in
        `pandas.to_datetime`.

    yearfirst: bool, default="False"
        Specify a date parse order if arg is str or is list-like.
        Same as in `pandas.to_datetime`.

        - If True parses dates with the year first, e.g. 10/11/12 is parsed as
          2010-11-12.
        - If both dayfirst and yearfirst are True, yearfirst is preceded.

    utc: bool, default=None
        Return UTC DatetimeIndex if True (converting any tz-aware datetime.datetime
        objects as well). Same as in `pandas.to_datetime`.

    format: str, default None
        The strftime to parse time, e.g. "%d/%m/%Y". Check pandas `to_datetime()` for
        more information on choices. If you have variables with different formats pass
        “mixed”, to infer the format for each element individually. This is risky,
        and you should probably use it along with dayfirst, according to pandas'
        documentation.

    Attributes
    ----------
    variables_:
        The list with datetime variables from which the variables in `reference` will
        be substracted. It is created after the transformer corroborates that the
        variables in `variables` are, or can be parsed to datetime.

    reference_:
        The list with the datetime variables that will be subtracted from `variables_`.
        It is created after the transformer corroborates that the variables in
        `reference` are, or can be parsed to datetime.

    {feature_names_in_}

    {n_features_in_}

    Methods
    -------
    {fit}

    {fit_transform}

    {transform}

    Examples
    --------

    {example}
        This transformer does not learn any parameter.

        Parameters
        ----------
        X: pandas dataframe of shape = [n_samples, n_features]
            The training input samples. Can be the entire dataframe, not just the
            variables to transform.

        y: pandas Series, or np.array. Default=None.
            It is not needed in this transformer. You can pass y or None.
        Add new features.

        Parameters
        ----------
        X: pandas dataframe of shape = [n_samples, n_features]
            The data to transform.

        Returns
        -------
        X_new: Pandas dataframe
            The input dataframe plus the new variables.
        datetime_df = pd.concat(
            [
                pd.to_datetime(
                    X[variable],
                    dayfirst=self.dayfirst,
                    yearfirst=self.yearfirst,
                    utc=self.utc,
                    format=self.format,
                )
                for variable in set(self.variables_ + self.reference_)
            ],
            axis=1,
        )

        non_dt_columns = datetime_df.columns[~datetime_df.apply(is_datetime)].tolist()

        if non_dt_columns:
            raise ValueError(
                "ValueError: variable(s) "
                + (len(non_dt_columns) * "{} ").format(*non_dt_columns)
                + "could not be converted to datetime. Try setting utc=True"
            )
        return datetime_df

    def _sub(self, dt_df: pd.DataFrame):
        new_df = pd.DataFrame()
        for reference in self.reference_:
            new_varnames = [f"{var}_sub_{reference}" for var in self.variables_]
            new_df[new_varnames] = (
                dt_df[self.variables_]
                .sub(dt_df[reference], axis=0)
                .div(np.timedelta64(1, self.output_unit).astype("timedelta64[ns]"))
            )

        if self.new_variables_names is not None:
            new_df.columns = self.new_variables_names

        return new_df

    def _get_new_features_name(self) -> List:
        if self.new_variables_names is not None:
            feature_names = self.new_variables_names
        else:
            feature_names = [
                f"{var}_sub_{reference}"
                for reference in self.reference_
                for var in self.variables_
            ]
        return feature_names

================================================
FILE: feature_engine/discretisation/__init__.py
================================================

from .arbitrary import ArbitraryDiscretiser
from .decision_tree import DecisionTreeDiscretiser
from .equal_frequency import EqualFrequencyDiscretiser
from .equal_width import EqualWidthDiscretiser
from .geometric_width import GeometricWidthDiscretiser

__all__ = [
    "DecisionTreeDiscretiser",
    "EqualFrequencyDiscretiser",
    "EqualWidthDiscretiser",
    "ArbitraryDiscretiser",
    "GeometricWidthDiscretiser",
]

================================================
FILE: feature_engine/discretisation/arbitrary.py
================================================

import warnings
from typing import Dict, List, Optional, Union

import pandas as pd

from feature_engine._base_transformers.mixins import FitFromDictMixin
from feature_engine._docstrings.fit_attributes import (
    _binner_dict_docstring,
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.discretisers import (
    _precision_docstring,
    _return_boundaries_docstring,
    _return_object_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_not_learn_docstring,
    _fit_transform_docstring,
    _transform_discretiser_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.discretisation.base_discretiser import BaseDiscretiser
from feature_engine.tags import _return_tags

@Substitution(
    return_object=_return_object_docstring,
    return_boundaries=_return_boundaries_docstring,
    precision=_precision_docstring,
    binner_dict_=_binner_dict_docstring,
    transform=_transform_discretiser_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit=_fit_not_learn_docstring,
    fit_transform=_fit_transform_docstring,
)
class ArbitraryDiscretiser(BaseDiscretiser, FitFromDictMixin):

    def __init__(
        self,
        binning_dict: Dict[Union[str, int], List[Union[str, int]]],
        return_object: bool = False,
        return_boundaries: bool = False,
        precision: int = 3,
        errors: str = "ignore",
    ) -> None:

        if not isinstance(binning_dict, dict):
            raise ValueError(
                "binning_dict must be a dictionary with the interval limits per "
                f"variable. Got {binning_dict} instead."
            )

        if errors not in ["ignore", "raise"]:
            raise ValueError(
                "errors only takes values 'ignore' and 'raise'. "
                f"Got {errors} instead."
            )

        super().__init__(return_object, return_boundaries, precision)

        self.binning_dict = binning_dict
        self.errors = errors

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):
        X = super()._fit_from_dict(X, self.binning_dict)

        self.binner_dict_ = self.binning_dict

        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:

        X = super().transform(X)
        if X[self.variables_].isnull().sum().sum() > 0:

            nan_columns = (
                X[self.variables_].columns[X[self.variables_].isnull().any()].tolist()
            )

            if len(nan_columns) > 1:
                nan_columns_str = ", ".join(nan_columns)
            else:
                nan_columns_str = nan_columns[0]

            if self.errors == "ignore":
                warnings.warn(
                    f"During the discretisation, NaN values were introduced in "
                    f"the feature(s) {nan_columns_str}."
                )

            elif self.errors == "raise":
                raise ValueError(
                    "During the discretisation, NaN values were introduced in "
                    f"the feature(s) {nan_columns_str}."
                )

        return X

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["_xfail_checks"][
            "check_parameters_default_constructible"
        ] = "transformer has 1 mandatory parameter"
        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        return tags

================================================
FILE: feature_engine/discretisation/base_discretiser.py
================================================

import pandas as pd

from feature_engine._base_transformers.base_numerical import BaseNumericalTransformer

class BaseDiscretiser(BaseNumericalTransformer):

    def __init__(
        self,
        return_object: bool = False,
        return_boundaries: bool = False,
        precision: int = 3,
    ) -> None:

        if not isinstance(return_object, bool):
            raise ValueError(
                "return_object must be True or False. " f"Got {return_object} instead."
            )

        if not isinstance(return_boundaries, bool):
            raise ValueError(
                "return_boundaries must be True or False. "
                f"Got {return_boundaries} instead."
            )

        if not isinstance(precision, int) or precision < 1:
            raise ValueError(
                "precision must be a positive integer. " f"Got {precision} instead."
            )

        self.return_object = return_object
        self.return_boundaries = return_boundaries
        self.precision = precision

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:

        X = self._check_transform_input_and_state(X)

        if self.return_boundaries is True:
            for feature in self.variables_:
                X[feature] = pd.cut(
                    X[feature],
                    self.binner_dict_[feature],
                    precision=self.precision,
                    include_lowest=True,
                )
            X[self.variables_] = X[self.variables_].astype(str)

        else:
            for feature in self.variables_:
                X[feature] = pd.cut(
                    X[feature],
                    self.binner_dict_[feature],
                    labels=False,
                    include_lowest=True,
                )

            if self.return_object:
                X[self.variables_] = X[self.variables_].astype("O")

        return X

================================================
FILE: feature_engine/discretisation/decision_tree.py
================================================

from typing import Dict, List, Optional, Union

import numpy as np
import pandas as pd
from sklearn.model_selection import GridSearchCV
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor
from sklearn.utils.multiclass import check_classification_targets, type_of_target

from feature_engine._base_transformers.base_numerical import BaseNumericalTransformer
from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _variables_numerical_docstring,
)
from feature_engine._docstrings.methods import _fit_transform_docstring
from feature_engine._docstrings.substitute import Substitution
from feature_engine.tags import _return_tags

@Substitution(
    variables=_variables_numerical_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit_transform=_fit_transform_docstring,
)
class DecisionTreeDiscretiser(BaseNumericalTransformer):

    def __init__(
        self,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        bin_output: str = "prediction",
        precision: Union[int, None] = None,
        cv=3,
        scoring: str = "neg_mean_squared_error",
        param_grid: Optional[Dict[str, Union[str, int, float, List[int]]]] = None,
        regression: bool = True,
        random_state: Optional[int] = None,
    ) -> None:

        if bin_output not in ["prediction", "bin_number", "boundaries"]:
            raise ValueError(
                "bin_output takes values  'prediction', 'bin_number' or 'boundaries'. "
                f"Got {bin_output} instead."
            )

        if precision is not None and (not isinstance(precision, int) or precision < 1):
            raise ValueError(
                "precision must be None or a positive integer. "
                f"Got {precision} instead."
            )

        if bin_output == "boundaries" and precision is None:
            raise ValueError(
                "When `bin_output == 'boundaries', `precision` cannot be None. "
                "Change precision's value to a positive integer."
            )
        if not isinstance(regression, bool):
            raise ValueError(
                f"regression can only take True or False. Got {regression} instead."
            )

        self.bin_output = bin_output
        self.precision = precision
        self.cv = cv
        self.scoring = scoring
        self.regression = regression
        self.variables = _check_variables_input_value(variables)
        self.param_grid = param_grid
        self.random_state = random_state

    def fit(self, X: pd.DataFrame, y: pd.Series):
        if self.regression is True:
            if type_of_target(y) == "binary":
                raise ValueError(
                    "Trying to fit a regression to a binary target is not "
                    "allowed by this transformer. Check the target values "
                    "or set regression to False."
                )
        else:
            check_classification_targets(y)

        X = super().fit(X)

        if self.param_grid:
            param_grid = self.param_grid
        else:
            param_grid = {"max_depth": [1, 2, 3, 4]}

        binner_dict_ = {}
        scores_dict_ = {}

        for var in self.variables_:

            if self.regression:
                model = DecisionTreeRegressor(random_state=self.random_state)
            else:
                model = DecisionTreeClassifier(random_state=self.random_state)

            tree_model = GridSearchCV(
                model, cv=self.cv, scoring=self.scoring, param_grid=param_grid
            )

            tree_model.fit(X[var].to_frame(), y)

            binner_dict_[var] = tree_model
            scores_dict_[var] = tree_model.score(X[var].to_frame(), y)

        if self.bin_output != "prediction":
            for var in self.variables_:
                clf = binner_dict_[var].best_estimator_
                threshold = clf.tree_.threshold
                feature = clf.tree_.feature
                feature_threshold = threshold[feature == 0]
                thresholds = sorted(feature_threshold)
                thresholds = [-np.inf] + thresholds + [np.inf]
                binner_dict_[var] = thresholds

        self.binner_dict_ = binner_dict_
        self.scores_dict_ = scores_dict_
        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:

        X = self._check_transform_input_and_state(X)

        if self.bin_output == "prediction":
            for feature in self.variables_:
                if self.regression:
                    preds = self.binner_dict_[feature].predict(X[feature].to_frame())
                    if self.precision is None:
                        X[feature] = preds
                    else:
                        X[feature] = np.round(preds, self.precision)
                else:
                    tmp = self.binner_dict_[feature].predict_proba(
                        X[feature].to_frame()
                    )
                    preds = tmp[:, 1]
                    if self.precision is None:
                        X[feature] = preds
                    else:
                        X[feature] = np.round(preds, self.precision)

        elif self.bin_output == "boundaries":
            for feature in self.variables_:
                X[feature] = pd.cut(
                    X[feature],
                    self.binner_dict_[feature],
                    precision=self.precision,
                    include_lowest=True,
                )
            X[self.variables_] = X[self.variables_].astype(str)

        else:
            for feature in self.variables_:
                X[feature] = pd.cut(
                    X[feature],
                    self.binner_dict_[feature],
                    labels=False,
                    include_lowest=True,
                )

        return X

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["variables"] = "numerical"
        tags_dict["requires_y"] = True
        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        return tags

================================================
FILE: feature_engine/discretisation/equal_frequency.py
================================================

from typing import List, Optional, Union

import pandas as pd

from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _binner_dict_docstring,
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _variables_numerical_docstring,
)
from feature_engine._docstrings.init_parameters.discretisers import (
    _precision_docstring,
    _return_boundaries_docstring,
    _return_object_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_discretiser_docstring,
    _fit_transform_docstring,
    _transform_discretiser_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.discretisation.base_discretiser import BaseDiscretiser

@Substitution(
    return_object=_return_object_docstring,
    return_boundaries=_return_boundaries_docstring,
    precision=_precision_docstring,
    binner_dict_=_binner_dict_docstring,
    fit=_fit_discretiser_docstring,
    transform=_transform_discretiser_docstring,
    variables=_variables_numerical_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit_transform=_fit_transform_docstring,
)
class EqualFrequencyDiscretiser(BaseDiscretiser):

    def __init__(
        self,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        q: int = 10,
        return_object: bool = False,
        return_boundaries: bool = False,
        precision: int = 3,
    ) -> None:

        if not isinstance(q, int):
            raise ValueError(f"q must be an integer. Got {q} instead.")

        super().__init__(return_object, return_boundaries, precision)

        self.q = q
        self.variables = _check_variables_input_value(variables)

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):

        X = super().fit(X)

        self.binner_dict_ = {}

        for var in self.variables_:
            tmp, bins = pd.qcut(x=X[var], q=self.q, retbins=True, duplicates="drop")

            bins = list(bins)
            bins[0] = float("-inf")
            bins[len(bins) - 1] = float("inf")
            self.binner_dict_[var] = bins

        return self

================================================
FILE: feature_engine/discretisation/equal_width.py
================================================

from typing import List, Optional, Union

import pandas as pd

from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _binner_dict_docstring,
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _variables_numerical_docstring,
)
from feature_engine._docstrings.init_parameters.discretisers import (
    _precision_docstring,
    _return_boundaries_docstring,
    _return_object_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_discretiser_docstring,
    _fit_transform_docstring,
    _transform_discretiser_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.discretisation.base_discretiser import BaseDiscretiser

@Substitution(
    return_object=_return_object_docstring,
    return_boundaries=_return_boundaries_docstring,
    precision=_precision_docstring,
    binner_dict_=_binner_dict_docstring,
    fit=_fit_discretiser_docstring,
    transform=_transform_discretiser_docstring,
    variables=_variables_numerical_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit_transform=_fit_transform_docstring,
)
class EqualWidthDiscretiser(BaseDiscretiser):

    def __init__(
        self,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        bins: int = 10,
        return_object: bool = False,
        return_boundaries: bool = False,
        precision: int = 3,
    ) -> None:

        if not isinstance(bins, int):
            raise ValueError(f"bins must be an integer. Got {bins} instead.")

        super().__init__(return_object, return_boundaries, precision)

        self.bins = bins
        self.variables = _check_variables_input_value(variables)

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):

        X = super().fit(X)

        self.binner_dict_ = {}

        for var in self.variables_:
            tmp, bins = pd.cut(
                x=X[var],
                bins=self.bins,
                retbins=True,
                duplicates="drop",
                include_lowest=True,
            )

            bins = list(bins)
            bins[0] = float("-inf")
            bins[len(bins) - 1] = float("inf")
            self.binner_dict_[var] = bins

        return self

================================================
FILE: feature_engine/discretisation/geometric_width.py
================================================
from typing import List, Optional, Union

import numpy as np
import pandas as pd

from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _binner_dict_docstring,
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _variables_numerical_docstring,
)
from feature_engine._docstrings.init_parameters.discretisers import (
    _precision_docstring,
    _return_boundaries_docstring,
    _return_object_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_discretiser_docstring,
    _fit_transform_docstring,
    _transform_discretiser_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.discretisation.base_discretiser import BaseDiscretiser

@Substitution(
    return_object=_return_object_docstring,
    return_boundaries=_return_boundaries_docstring,
    precision=_precision_docstring,
    binner_dict_=_binner_dict_docstring,
    fit=_fit_discretiser_docstring,
    transform=_transform_discretiser_docstring,
    variables=_variables_numerical_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit_transform=_fit_transform_docstring,
    power="{1/n}",
    subindex="{i+1}",
)
class GeometricWidthDiscretiser(BaseDiscretiser):

    def __init__(
        self,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        bins: int = 10,
        return_object: bool = False,
        return_boundaries: bool = False,
        precision: int = 7,
    ):

        if not isinstance(bins, int):
            raise ValueError(f"bins must be an integer. Got {bins} instead.")

        super().__init__(return_object, return_boundaries, precision)

        self.bins = bins
        self.variables = _check_variables_input_value(variables)

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):

        X = super().fit(X)

        self.binner_dict_ = {}

        for var in self.variables_:
            min_, max_ = X[var].min(), X[var].max()
            increment = np.power(max_ - min_, 1.0 / self.bins)
            bins = np.r_[
                -np.inf, min_ + np.power(increment, np.arange(1, self.bins)), np.inf
            ]
            bins = np.sort(bins)
            bins = list(bins)
            self.binner_dict_[var] = bins

        return self

================================================
FILE: feature_engine/encoding/__init__.py
================================================

from .count_frequency import CountFrequencyEncoder
from .decision_tree import DecisionTreeEncoder
from .mean_encoding import MeanEncoder
from .one_hot import OneHotEncoder
from .ordinal import OrdinalEncoder
from .rare_label import RareLabelEncoder
from .similarity_encoder import StringSimilarityEncoder
from .woe import WoEEncoder

__all__ = [
    "CountFrequencyEncoder",
    "DecisionTreeEncoder",
    "MeanEncoder",
    "OneHotEncoder",
    "OrdinalEncoder",
    "RareLabelEncoder",
    "StringSimilarityEncoder",
    "WoEEncoder",
]

================================================
FILE: feature_engine/encoding/_helper_functions.py
================================================
def check_parameter_unseen(unseen, accepted_values):
    if not isinstance(accepted_values, list) or not all(
        isinstance(item, str) for item in accepted_values
    ):
        raise ValueError(
            "accepted_values should be a list of strings. "
            f" Got {accepted_values} instead."
        )
    if unseen not in accepted_values:
        raise ValueError(
            f"Parameter `unseen` takes only values {', '.join(accepted_values)}."
            f" Got {unseen} instead."
        )

================================================
FILE: feature_engine/encoding/base_encoder.py
================================================
import warnings
from typing import List, Union

import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.utils.validation import check_is_fitted

from feature_engine._base_transformers.mixins import GetFeatureNamesOutMixin
from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _missing_values_docstring,
    _variables_categorical_docstring,
)
from feature_engine._docstrings.init_parameters.encoders import _ignore_format_docstring
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import (
    _check_optional_contains_na,
    _check_X_matches_training_df,
    check_X,
)
from feature_engine.tags import _return_tags
from feature_engine.variable_handling import (
    check_all_variables,
    check_categorical_variables,
    find_all_variables,
    find_categorical_variables,
)

@Substitution(
    ignore_format=_ignore_format_docstring,
    variables=_variables_categorical_docstring,
)
class CategoricalInitMixin:

    def __init__(
        self,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        ignore_format: bool = False,
    ) -> None:

        if not isinstance(ignore_format, bool):
            raise ValueError(
                "ignore_format takes only booleans True and False. "
                f"Got {ignore_format} instead."
            )

        self.variables = _check_variables_input_value(variables)
        self.ignore_format = ignore_format

@Substitution(
    missing_values=_missing_values_docstring,
    ignore_format=_ignore_format_docstring,
    variables=_variables_categorical_docstring,
)
class CategoricalInitMixinNA:

    def __init__(
        self,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        missing_values: str = "raise",
        ignore_format: bool = False,
    ) -> None:

        if missing_values not in ["raise", "ignore"]:
            raise ValueError(
                "missing_values takes only values 'raise' or 'ignore'. "
                f"Got {missing_values} instead."
            )

        if not isinstance(ignore_format, bool):
            raise ValueError(
                "ignore_format takes only booleans True and False. "
                f"Got {ignore_format} instead."
            )

        self.variables = _check_variables_input_value(variables)
        self.ignore_format = ignore_format
        self.missing_values = missing_values

class CategoricalMethodsMixin(TransformerMixin, BaseEstimator, GetFeatureNamesOutMixin):

    def _check_na(self, X: pd.DataFrame, variables):
        if self.missing_values == "raise":
            _check_optional_contains_na(X, variables)

    def _check_or_select_variables(self, X: pd.DataFrame):
        if self.ignore_format is True:
            if self.variables is None:
                variables_ = find_all_variables(X)
            else:
                variables_ = check_all_variables(X, self.variables)
        else:
            if self.variables is None:
                variables_ = find_categorical_variables(X)
            else:
                variables_ = check_categorical_variables(X, self.variables)

        return variables_

    def _get_feature_names_in(self, X: pd.DataFrame):
        self.feature_names_in_ = X.columns.tolist()

        self.n_features_in_ = X.shape[1]

    def _check_transform_input_and_state(self, X: pd.DataFrame) -> pd.DataFrame:

        check_is_fitted(self)

        X = check_X(X)

        _check_X_matches_training_df(X, self.n_features_in_)

        X = X[self.feature_names_in_]

        return X

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:

        X = self._check_transform_input_and_state(X)

        if self.missing_values == "raise":
            _check_optional_contains_na(X, self.variables_)

        X = self._encode(X)

        return X

    def _encode(self, X: pd.DataFrame) -> pd.DataFrame:
        for feature in self.encoder_dict_.keys():
            X[feature] = X[feature].map(self.encoder_dict_[feature])

            if X[feature].dtype.name == "category":
                if all(isinstance(x, int) for x in X[feature]):
                    X[feature] = X[feature].astype("int")
                else:
                    X[feature] = X[feature].astype("float")

        if self.unseen == "encode":
            X[self.variables_] = X[self.variables_].fillna(self._unseen)
        else:
            self._check_nan_values_after_transformation(X)

        return X

    def _check_nan_values_after_transformation(self, X):

        if X[self.variables_].isnull().sum().sum() > 0:

            nan_columns = (
                X[self.encoder_dict_.keys()]
                .columns[X[self.encoder_dict_.keys()].isnull().any()]
                .tolist()
            )

            if len(nan_columns) > 1:
                nan_columns_str = ", ".join(nan_columns)
            else:
                nan_columns_str = nan_columns[0]

            if self.unseen == "ignore":
                warnings.warn(
                    "During the encoding, NaN values were introduced in the feature(s) "
                    f"{nan_columns_str}."
                )
            elif self.unseen == "raise":
                raise ValueError(
                    "During the encoding, NaN values were introduced in the feature(s) "
                    f"{nan_columns_str}."
                )

    def inverse_transform(self, X: pd.DataFrame) -> pd.DataFrame:

        X = self._check_transform_input_and_state(X)

        for feature in self.encoder_dict_.keys():
            inv_map = {v: k for k, v in self.encoder_dict_[feature].items()}
            X[feature] = X[feature].map(inv_map)

        return X

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["variables"] = "categorical"
        tags_dict["_xfail_checks"]["check_estimators_nan_inf"] = "transformer allows NA"
        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        return tags

================================================
FILE: feature_engine/encoding/count_frequency.py
================================================

from typing import List, Optional, Union

import pandas as pd

from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _missing_values_docstring,
    _variables_categorical_docstring,
)
from feature_engine._docstrings.init_parameters.encoders import (
    _ignore_format_docstring,
    _unseen_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_transform_docstring,
    _inverse_transform_docstring,
    _transform_encoders_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import check_X
from feature_engine.encoding._helper_functions import check_parameter_unseen
from feature_engine.encoding.base_encoder import (
    CategoricalInitMixinNA,
    CategoricalMethodsMixin,
)

_unseen_docstring = (
    _unseen_docstring
    + """ If `'encode'`, unseen categories will be encoded as 0 (zero)."""
)

@Substitution(
    ignore_format=_ignore_format_docstring,
    missing_values=_missing_values_docstring,
    variables=_variables_categorical_docstring,
    unseen=_unseen_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit_transform=_fit_transform_docstring,
    transform=_transform_encoders_docstring,
    inverse_transform=_inverse_transform_docstring,
)
class CountFrequencyEncoder(CategoricalMethodsMixin, CategoricalInitMixinNA):

    def __init__(
        self,
        encoding_method: str = "count",
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        missing_values: str = "raise",
        ignore_format: bool = False,
        unseen: str = "ignore",
    ) -> None:

        if encoding_method not in ["count", "frequency"]:
            raise ValueError(
                "encoding_method takes only values 'count' and 'frequency'. "
                f"Got {encoding_method} instead."
            )

        check_parameter_unseen(unseen, ["ignore", "raise", "encode"])
        super().__init__(variables, missing_values, ignore_format)
        self.encoding_method = encoding_method
        self.unseen = unseen

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):
        X = check_X(X)
        variables_ = self._check_or_select_variables(X)
        self._check_na(X, variables_)

        self.encoder_dict_ = {}

        for var in variables_:
            if self.encoding_method == "count":
                self.encoder_dict_[var] = X[var].value_counts().to_dict()

            elif self.encoding_method == "frequency":
                self.encoder_dict_[var] = X[var].value_counts(normalize=True).to_dict()
            else:
                raise ValueError(
                    "Unrecognized value for encoding_method. It should be 'count' or "
                    f"'frequency'. Got {self.encoding_method} instead."
                )

        if self.unseen == "encode":
            self._unseen = 0

        self.variables_ = variables_
        self._get_feature_names_in(X)
        return self

================================================
FILE: feature_engine/encoding/decision_tree.py
================================================

from typing import List, Optional, Union

import numpy as np
import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.utils.multiclass import check_classification_targets, type_of_target

from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _variables_categorical_docstring,
)
from feature_engine._docstrings.init_parameters.encoders import (
    _ignore_format_docstring,
    _unseen_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_transform_docstring,
    _inverse_transform_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import _check_contains_na, check_X_y
from feature_engine.discretisation import DecisionTreeDiscretiser
from feature_engine.encoding._helper_functions import check_parameter_unseen
from feature_engine.encoding.base_encoder import (
    CategoricalInitMixin,
    CategoricalMethodsMixin,
)
from feature_engine.encoding.ordinal import OrdinalEncoder
from feature_engine.tags import _return_tags

_unseen_docstring = (
    _unseen_docstring
    + """ If `'encode'` unseen categories will be encoded as `fill_value`."""
)

@Substitution(
    ignore_format=_ignore_format_docstring,
    variables=_variables_categorical_docstring,
    variables_=_variables_attribute_docstring,
    unseen=_unseen_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit_transform=_fit_transform_docstring,
    inverse_transform=_inverse_transform_docstring,
)
class DecisionTreeEncoder(CategoricalMethodsMixin, CategoricalInitMixin):

    def __init__(
        self,
        encoding_method: str = "arbitrary",
        cv=3,
        scoring: str = "neg_mean_squared_error",
        param_grid: Optional[dict] = None,
        regression: bool = True,
        random_state: Optional[int] = None,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        ignore_format: bool = False,
        precision: Optional[int] = None,
        unseen: str = "ignore",
        fill_value: Optional[float] = None,
    ) -> None:

        if encoding_method not in ["ordered", "arbitrary"]:
            raise ValueError(
                "`encoding_method` takes only values 'ordered' and 'arbitrary'."
                f" Got {encoding_method} instead."
            )

        if unseen == "encode" and (
            fill_value is None or not isinstance(fill_value, (int, float))
        ):
            raise ValueError(
                "When `unseen='encode'` you need to pass a number to `fill_value`. "
                f"Got {fill_value} instead."
            )

        if precision is not None and (not isinstance(precision, int) or precision < 0):
            raise ValueError(
                "Parameter `precision` takes integers or None. "
                f"Got {precision} instead."
            )

        check_parameter_unseen(unseen, ["ignore", "raise", "encode"])
        super().__init__(variables, ignore_format)
        self.encoding_method = encoding_method
        self.cv = cv
        self.scoring = scoring
        self.regression = regression
        self.param_grid = param_grid
        self.random_state = random_state
        self.precision = precision
        self.unseen = unseen
        self.fill_value = fill_value

    def fit(self, X: pd.DataFrame, y: pd.Series):
        X, y = check_X_y(X, y)

        if self.regression is True:
            if type_of_target(y) == "binary":
                raise ValueError(
                    "Trying to fit a regression to a binary target is not "
                    "allowed by this transformer. Check the target values "
                    "or set regression to False."
                )

        else:
            check_classification_targets(y)

        variables_ = self._check_or_select_variables(X)
        _check_contains_na(X, variables_)

        param_grid = self._assign_param_grid()

        encoder = OrdinalEncoder(
            encoding_method=self.encoding_method,
            variables=variables_,
            missing_values="raise",
            ignore_format=self.ignore_format,
        )

        tree = DecisionTreeDiscretiser(
            cv=self.cv,
            scoring=self.scoring,
            variables=variables_,
            param_grid=param_grid,
            regression=self.regression,
            random_state=self.random_state,
        )

        pipe = Pipeline(
            [
                ("encoder", encoder),
                ("tree", tree),
            ]
        )

        Xt = pipe.fit_transform(X, y)

        encoder_ = {}
        if self.precision is None:
            for var in variables_:
                encoder_[var] = dict(zip(X[var], Xt[var]))
        else:
            for var in variables_:
                encoder_[var] = dict(zip(X[var], np.round(Xt[var], self.precision)))

        if self.unseen == "encode":
            self._unseen = self.fill_value

        self.encoder_dict_ = encoder_
        self.variables_ = variables_
        self._get_feature_names_in(X)
        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X = self._check_transform_input_and_state(X)
        _check_contains_na(X, self.variables_)
        X = self._encode(X)

        return X

    def _assign_param_grid(self):
        if self.param_grid:
            param_grid = self.param_grid
        else:
            param_grid = {"max_depth": [1, 2, 3, 4]}
        return param_grid

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["variables"] = "categorical"
        tags_dict["requires_y"] = True
        tags_dict["_xfail_checks"]["check_estimators_nan_inf"] = "transformer allows NA"
        return tags_dict

================================================
FILE: feature_engine/encoding/mean_encoding.py
================================================
from typing import List, Union

import pandas as pd

from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _missing_values_docstring,
    _variables_categorical_docstring,
)
from feature_engine._docstrings.init_parameters.encoders import (
    _ignore_format_docstring,
    _unseen_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_transform_docstring,
    _inverse_transform_docstring,
    _transform_encoders_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import check_X_y
from feature_engine.encoding._helper_functions import check_parameter_unseen
from feature_engine.encoding.base_encoder import (
    CategoricalInitMixinNA,
    CategoricalMethodsMixin,
)

_unseen_docstring = (
    _unseen_docstring
    + """ If `'encode'`, unseen categories will be encoded with the prior."""
)

@Substitution(
    missing_values=_missing_values_docstring,
    ignore_format=_ignore_format_docstring,
    variables=_variables_categorical_docstring,
    unseen=_unseen_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit_transform=_fit_transform_docstring,
    transform=_transform_encoders_docstring,
    inverse_transform=_inverse_transform_docstring,
)
class MeanEncoder(CategoricalMethodsMixin, CategoricalInitMixinNA):

    def __init__(
        self,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        missing_values: str = "raise",
        ignore_format: bool = False,
        unseen: str = "ignore",
        smoothing: Union[int, float, str] = 0.0,
    ) -> None:
        super().__init__(variables, missing_values, ignore_format)
        if (
            not isinstance(smoothing, (str, float, int))
            or isinstance(smoothing, str)
            and (smoothing != "auto")
        ) or (isinstance(smoothing, (float, int)) and smoothing < 0):
            raise ValueError(
                f"smoothing must be greater than 0 or 'auto'. "
                f"Got {smoothing} instead."
            )
        self.smoothing = smoothing
        check_parameter_unseen(unseen, ["ignore", "raise", "encode"])
        self.unseen = unseen

    def fit(self, X: pd.DataFrame, y: pd.Series):

        X, y = check_X_y(X, y)
        variables_ = self._check_or_select_variables(X)
        self._check_na(X, variables_)

        self.encoder_dict_ = {}

        y_prior = y.mean()

        if self.unseen == "encode":
            self._unseen = y_prior

        if self.smoothing == "auto":
            y_var = y.var(ddof=0)
        for var in variables_:
            if self.smoothing == "auto":
                damping = y.groupby(X[var]).var(ddof=0) / y_var
            else:
                damping = self.smoothing
            counts = X[var].value_counts()
            counts.index = counts.index.infer_objects()
            _lambda = counts / (counts + damping)
            self.encoder_dict_[var] = (
                _lambda * y.groupby(X[var], observed=False).mean()
                + (1.0 - _lambda) * y_prior
            ).to_dict()

        self.variables_ = variables_
        self._get_feature_names_in(X)
        return self

    def inverse_transform(self, X: pd.DataFrame) -> pd.DataFrame:

        if self.unseen == "encode":
            raise NotImplementedError(
                "inverse_transform is not implemented for this transformer when "
                "`unseen='encode'`."
            )
        else:
            return super().inverse_transform(X)

    def _more_tags(self):
        tags_dict = super()._more_tags()
        tags_dict["requires_y"] = True
        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        return tags

================================================
FILE: feature_engine/encoding/one_hot.py
================================================

from typing import List, Optional, Union

import numpy as np
import pandas as pd

from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _variables_categorical_docstring,
)
from feature_engine._docstrings.init_parameters.encoders import _ignore_format_docstring
from feature_engine._docstrings.methods import _fit_transform_docstring
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import _check_contains_na, check_X
from feature_engine.encoding.base_encoder import (
    CategoricalInitMixin,
    CategoricalMethodsMixin,
)

@Substitution(
    ignore_format=_ignore_format_docstring,
    variables=_variables_categorical_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit_transform=_fit_transform_docstring,
)
class OneHotEncoder(CategoricalMethodsMixin, CategoricalInitMixin):

    def __init__(
        self,
        top_categories: Optional[int] = None,
        drop_last: bool = False,
        drop_last_binary: bool = False,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        ignore_format: bool = False,
    ) -> None:

        if top_categories and (
            not isinstance(top_categories, int) or top_categories < 0
        ):
            raise ValueError(
                "top_categories takes only positive integers. "
                f"Got {top_categories} instead"
            )

        if not isinstance(drop_last, bool):
            raise ValueError(
                f"drop_last takes only True or False. Got {drop_last} instead."
            )

        if not isinstance(drop_last_binary, bool):
            raise ValueError(
                "drop_last_binary takes only True or False. "
                f"Got {drop_last_binary} instead."
            )

        super().__init__(variables, ignore_format)
        self.top_categories = top_categories
        self.drop_last = drop_last
        self.drop_last_binary = drop_last_binary

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):

        X = check_X(X)
        variables_ = self._check_or_select_variables(X)
        _check_contains_na(X, variables_)

        self.encoder_dict_ = {}

        for var in variables_:

            if self.top_categories:
                self.encoder_dict_[var] = [
                    x
                    for x in X[var]
                    .value_counts()
                    .sort_values(ascending=False)
                    .head(self.top_categories)
                    .index
                ]

            else:
                category_ls = list(X[var].unique())

                if self.drop_last:
                    self.encoder_dict_[var] = category_ls[:-1]

                else:
                    self.encoder_dict_[var] = category_ls

        self.variables_binary_ = [var for var in variables_ if X[var].nunique() == 2]

        if self.drop_last_binary:
            for var in self.variables_binary_:
                category = X[var].unique()[0]
                self.encoder_dict_[var] = [category]

        self.variables_ = variables_
        self._get_feature_names_in(X)
        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:

        X = self._check_transform_input_and_state(X)

        _check_contains_na(X, self.variables_)

        for feature in self.variables_:
            for category in self.encoder_dict_[feature]:
                dummy_df = pd.DataFrame(
                    {f"{feature}_{category}": np.where(X[feature] == category, 1, 0)},
                    index=X.index,
                )
                X = pd.concat([X, dummy_df], axis=1)

        X.drop(labels=self.variables_, axis=1, inplace=True)

        return X

    def inverse_transform(self, X: pd.DataFrame):
        raise NotImplementedError(
            "inverse_transform is not implemented for this transformer."
        )

    def _get_new_features_name(self) -> List:
        feature_names = []
        for feature in self.variables_:
            for category in self.encoder_dict_[feature]:
                feature_names.append(f"{feature}_{category}")

        return feature_names

    def _add_new_feature_names(self, feature_names) -> List:
        feature_names = feature_names + self._get_new_features_name()
        feature_names = [f for f in feature_names if f not in self.variables_]

        return feature_names

================================================
FILE: feature_engine/encoding/ordinal.py
================================================

from typing import List, Optional, Union

import pandas as pd

from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _missing_values_docstring,
    _variables_categorical_docstring,
)
from feature_engine._docstrings.init_parameters.encoders import (
    _ignore_format_docstring,
    _unseen_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_transform_docstring,
    _inverse_transform_docstring,
    _transform_encoders_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import check_X, check_X_y
from feature_engine.encoding._helper_functions import check_parameter_unseen
from feature_engine.encoding.base_encoder import (
    CategoricalInitMixinNA,
    CategoricalMethodsMixin,
)

_unseen_docstring = (
    _unseen_docstring + """ If `'encode'`, unseen categories will be encoded as -1."""
)

@Substitution(
    missing_values=_missing_values_docstring,
    ignore_format=_ignore_format_docstring,
    variables=_variables_categorical_docstring,
    unseen=_unseen_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit_transform=_fit_transform_docstring,
    transform=_transform_encoders_docstring,
    inverse_transform=_inverse_transform_docstring,
)
class OrdinalEncoder(CategoricalMethodsMixin, CategoricalInitMixinNA):

    def __init__(
        self,
        encoding_method: str = "ordered",
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        missing_values: str = "raise",
        ignore_format: bool = False,
        unseen: str = "ignore",
    ) -> None:

        if encoding_method not in ["ordered", "arbitrary"]:
            raise ValueError(
                "encoding_method takes only values 'ordered' and 'arbitrary'"
            )

        check_parameter_unseen(unseen, ["ignore", "raise", "encode"])
        super().__init__(variables, missing_values, ignore_format)
        self.encoding_method = encoding_method
        self.unseen = unseen

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):

        if self.encoding_method == "ordered":
            X, y = check_X_y(X, y)
        else:
            X = check_X(X)

        variables_ = self._check_or_select_variables(X)
        self._check_na(X, variables_)

        self.encoder_dict_ = {}

        for var in variables_:
            if self.encoding_method == "ordered":
                t = y.groupby(X[var], observed=False).mean()
                t = t.sort_values(ascending=True).index

            elif self.encoding_method == "arbitrary":
                if self.missing_values == "ignore":
                    t = X[var].dropna().unique()
                else:
                    t = X[var].unique()
            else:
                raise ValueError(
                    "Unrecognized value for encoding_method. It should be 'arbitrary' "
                    f"or 'frequency'. Got {self.encoding_method} instead."
                )

            self.encoder_dict_[var] = {k: i for i, k in enumerate(t, 0)}

        if self.unseen == "encode":
            self._unseen = -1

        self.variables_ = variables_
        self._get_feature_names_in(X)
        return self

================================================
FILE: feature_engine/encoding/rare_label.py
================================================

import warnings
from typing import List, Optional, Union

import numpy as np
import pandas as pd

from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _missing_values_docstring,
    _variables_categorical_docstring,
)
from feature_engine._docstrings.init_parameters.encoders import _ignore_format_docstring
from feature_engine._docstrings.methods import _fit_transform_docstring
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import _check_optional_contains_na, check_X
from feature_engine.encoding.base_encoder import (
    CategoricalInitMixinNA,
    CategoricalMethodsMixin,
)

@Substitution(
    missing_values=_missing_values_docstring,
    ignore_format=_ignore_format_docstring,
    variables=_variables_categorical_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit_transform=_fit_transform_docstring,
)
class RareLabelEncoder(CategoricalMethodsMixin, CategoricalInitMixinNA):

    def __init__(
        self,
        tol: float = 0.05,
        n_categories: int = 10,
        max_n_categories: Optional[int] = None,
        replace_with: Union[str, int, float] = "Rare",
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        missing_values: str = "raise",
        ignore_format: bool = False,
    ) -> None:

        if not isinstance(tol, (int, float)) or tol < 0 or tol > 1:
            raise ValueError(f"tol takes values between 0 and 1. Got {tol} instead.")

        if not isinstance(n_categories, int) or n_categories < 0:
            raise ValueError(
                "n_categories takes only positive integer numbers. "
                f"Got {n_categories} instead."
            )

        if max_n_categories is not None:
            if (
                not isinstance(max_n_categories, int)
                or isinstance(max_n_categories, int)
                and max_n_categories < 0
            ):
                raise ValueError(
                    "max_n_categories takes only positive integer numbers. "
                    f"Got {max_n_categories} instead."
                )

        if not isinstance(replace_with, (str, int, float)):
            raise ValueError(
                "replace_with can should be a string, integer or float. "
                f"Got {replace_with} instead."
            )

        super().__init__(variables, missing_values, ignore_format)
        self.tol = tol
        self.n_categories = n_categories
        self.max_n_categories = max_n_categories
        self.replace_with = replace_with

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):

        X = check_X(X)
        variables_ = self._check_or_select_variables(X)
        self._check_na(X, variables_)

        self.encoder_dict_ = {}

        for var in variables_:
            if len(X[var].unique()) > self.n_categories:

                t = X[var].value_counts(normalize=True)

                freq_idx = t[t >= self.tol].index

                if self.max_n_categories:
                    self.encoder_dict_[var] = list(freq_idx[: self.max_n_categories])
                else:
                    self.encoder_dict_[var] = list(freq_idx)

            else:
                warnings.warn(
                    "The number of unique categories for variable {} is less than that "
                    "indicated in n_categories. Thus, all categories will be "
                    "considered frequent".format(var)
                )
                self.encoder_dict_[var] = list(X[var].unique())

        self.variables_ = variables_
        self._get_feature_names_in(X)
        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:

        X = self._check_transform_input_and_state(X)

        if self.missing_values == "raise":
            _check_optional_contains_na(X, self.variables_)
            with_nan = []
        else:
            with_nan = [np.nan]

        for feature in self.variables_:
            if self.ignore_format is True and isinstance(self.replace_with, str):
                num_vars = list(
                    X[self.variables_].select_dtypes(include="number").columns
                )
                X[num_vars] = X[num_vars].astype("O")

            if X[feature].dtype == "category":
                X[feature] = X[feature].cat.add_categories(self.replace_with)

            X.loc[~X[feature].isin(self.encoder_dict_[feature] + with_nan), feature] = (
                self.replace_with
            )

        return X

    def inverse_transform(self, X: pd.DataFrame):
        raise NotImplementedError(
            "inverse_transform is not implemented for this transformer."
        )

================================================
FILE: feature_engine/encoding/similarity_encoder.py
================================================
from difflib import SequenceMatcher
from typing import List, Optional, Union

import numpy as np
import pandas as pd
from sklearn.utils.validation import check_is_fitted

from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _variables_categorical_docstring,
)
from feature_engine._docstrings.init_parameters.encoders import _ignore_format_docstring
from feature_engine._docstrings.methods import _fit_transform_docstring
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import _check_optional_contains_na, check_X
from feature_engine.encoding.base_encoder import (
    CategoricalInitMixin,
    CategoricalMethodsMixin,
)

def _gpm_fast(x1: str, x2: str) -> float:
    return SequenceMatcher(None, str(x1), str(x2)).quick_ratio()

_gpm_fast_vec = np.vectorize(_gpm_fast)

@Substitution(
    ignore_format=_ignore_format_docstring,
    variables=_variables_categorical_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit_transform=_fit_transform_docstring,
)
class StringSimilarityEncoder(CategoricalMethodsMixin, CategoricalInitMixin):

    def __init__(
        self,
        top_categories: Optional[int] = None,
        keywords: Optional[dict] = None,
        missing_values: str = "impute",
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        ignore_format: bool = False,
    ):
        if top_categories and not isinstance(top_categories, int):
            raise ValueError(
                f"top_categories takes only integers. Got {top_categories!r} instead."
            )
        if missing_values not in ("raise", "impute", "ignore"):
            raise ValueError(
                "missing_values should be one of 'raise', 'impute' or 'ignore'."
                f" Got {missing_values!r} instead."
            )
        if keywords and not isinstance(keywords, dict):
            raise ValueError(
                f"keywords should be a dictionary or None. Got {keywords!r} instead."
            )
        if keywords and not all(isinstance(item, list) for item in keywords.values()):
            raise ValueError(
                "The items in keywords should be lists."
                f" Got {keywords.values()!r} instead."
            )
        super().__init__(variables, ignore_format)
        self.top_categories = top_categories
        self.missing_values = missing_values
        self.keywords = keywords

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):

        X = check_X(X)
        variables_ = self._check_or_select_variables(X)

        if self.keywords:
            if not all(item in variables_ for item in self.keywords.keys()):
                raise ValueError(
                    "There are variables in keywords that are not present "
                    "in the dataset."
                )

        if self.missing_values == "raise":
            _check_optional_contains_na(X, variables_)

        self.encoder_dict_ = {}

        if self.keywords:
            self.encoder_dict_.update(self.keywords)
            cols_to_iterate = [x for x in variables_ if x not in self.keywords]
        else:
            cols_to_iterate = variables_

        if self.missing_values == "raise":
            for var in cols_to_iterate:
                self.encoder_dict_[var] = (
                    X[var]
                    .astype(str)
                    .value_counts()
                    .head(self.top_categories)
                    .index.tolist()
                )
        elif self.missing_values == "impute":
            for var in cols_to_iterate:
                self.encoder_dict_[var] = (
                    X[var]
                    .astype(str)
                    .replace("nan", "")
                    .value_counts()
                    .head(self.top_categories)
                    .index.tolist()
                )
        elif self.missing_values == "ignore":
            for var in cols_to_iterate:
                self.encoder_dict_[var] = (
                    X[var]
                    .astype(str)
                    .value_counts(dropna=True)
                    .drop("nan", errors="ignore")
                    .head(self.top_categories)
                    .index.tolist()
                )
        else:
            raise ValueError(
                "Unrecognized value for missing_values. It should be 'raise', 'ignore' "
                f"or 'impute'. Got {self.missing_values} instead."
            )

        self.variables_ = variables_
        self._get_feature_names_in(X)
        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:

        check_is_fitted(self)
        X = self._check_transform_input_and_state(X)
        if self.missing_values == "raise":
            _check_optional_contains_na(X, self.variables_)

        new_values = []
        for var in self.variables_:
            if self.missing_values == "impute":
                X[var] = X[var].astype(str).replace("nan", "")
            categories = X[var].dropna().astype(str).unique()
            column_encoder_dict = {
                x: _gpm_fast_vec(x, self.encoder_dict_[var]) for x in categories
            }
            column_encoder_dict["nan"] = [np.nan] * len(self.encoder_dict_[var])
            encoded = np.vstack(X[var].astype(str).map(column_encoder_dict).values)
            if self.missing_values == "ignore":
                encoded[X[var].isna(), :] = np.nan
            new_values.append(encoded)

        new_features = self._get_new_features_name()
        X.loc[:, new_features] = np.hstack(new_values)

        return X.drop(self.variables_, axis=1)

    def _get_new_features_name(self) -> List[str]:
        feature_names = []
        for feature in self.variables_:
            for category in self.encoder_dict_[feature]:
                if category == "":
                    feature_names.append(f"{feature}_nan")
                else:
                    feature_names.append(f"{feature}_{category}")

        return feature_names

    def _add_new_feature_names(self, feature_names: List[str]) -> List[str]:
        feature_names = feature_names + self._get_new_features_name()
        feature_names = [f for f in feature_names if f not in self.variables_]

        return feature_names

    def inverse_transform(self, X: pd.DataFrame):
        raise NotImplementedError(
            "inverse_transform is not implemented for this transformer."
        )

================================================
FILE: feature_engine/encoding/woe.py
================================================

from typing import List, Union

import numpy as np
import pandas as pd

from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _variables_categorical_docstring,
)
from feature_engine._docstrings.init_parameters.encoders import (
    _ignore_format_docstring,
    _unseen_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_transform_docstring,
    _inverse_transform_docstring,
    _transform_encoders_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import _check_contains_na, check_X_y
from feature_engine.encoding._helper_functions import check_parameter_unseen
from feature_engine.encoding.base_encoder import (
    CategoricalInitMixin,
    CategoricalMethodsMixin,
)
from feature_engine.tags import _return_tags

class WoE:
    def _check_fit_input(self, X: pd.DataFrame, y: pd.Series):
        X, y = check_X_y(X, y)

        if y.nunique() != 2:
            raise ValueError(
                "This encoder is designed for binary classification. The target "
                "used has more than 2 unique values."
            )

        if y.min() != 0 or y.max() != 1:
            y = pd.Series(np.where(y == y.min(), 0, 1))
        return X, y

    def _calculate_woe(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        variable: Union[str, int],
        fill_value: Union[float, None] = None,
    ):
        total_pos = y.sum()
        inverse_y = y.ne(1).copy()
        total_neg = inverse_y.sum()

        pos = y.groupby(X[variable], observed=False).sum() / total_pos
        neg = inverse_y.groupby(X[variable], observed=False).sum() / total_neg

        if not (pos[:] == 0).sum() == 0 or not (neg[:] == 0).sum() == 0:
            if fill_value is None:
                raise ValueError(
                    "The proportion of one of the classes for a category in "
                    "variable {} is zero, and log of zero is not defined".format(
                        variable
                    )
                )
            else:
                pos[pos[:] == 0] = fill_value
                neg[neg[:] == 0] = fill_value

        woe = np.log(pos / neg)
        return pos, neg, woe

@Substitution(
    ignore_format=_ignore_format_docstring,
    variables=_variables_categorical_docstring,
    unseen=_unseen_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit_transform=_fit_transform_docstring,
    transform=_transform_encoders_docstring,
    inverse_transform=_inverse_transform_docstring,
)
class WoEEncoder(CategoricalMethodsMixin, CategoricalInitMixin, WoE):

    def __init__(
        self,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        ignore_format: bool = False,
        unseen: str = "ignore",
        fill_value: Union[int, float, None] = None,
    ) -> None:

        super().__init__(variables, ignore_format)
        check_parameter_unseen(unseen, ["ignore", "raise"])
        if fill_value is not None and not isinstance(fill_value, (int, float)):
            raise ValueError(
                f"fill_value takes None, integer or float. Got {fill_value} instead."
            )
        self.unseen = unseen
        self.fill_value = fill_value

    def fit(self, X: pd.DataFrame, y: pd.Series):
        X, y = self._check_fit_input(X, y)
        variables_ = self._check_or_select_variables(X)
        _check_contains_na(X, variables_)

        encoder_dict_ = {}
        vars_that_fail = []

        for var in variables_:
            try:
                _, _, woe = self._calculate_woe(X, y, var, self.fill_value)
                encoder_dict_[var] = woe.to_dict()
            except ValueError:
                vars_that_fail.append(var)

        if len(vars_that_fail) > 0:
            vars_that_fail_str = (
                ", ".join(vars_that_fail)
                if len(vars_that_fail) > 1
                else vars_that_fail[0]
            )

            raise ValueError(
                "During the WoE calculation, some of the categories in the "
                "following features contained 0 in the denominator or numerator, "
                f"and hence the WoE can't be calculated: {vars_that_fail_str}."
            )

        self.encoder_dict_ = encoder_dict_
        self.variables_ = variables_
        self._get_feature_names_in(X)
        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:

        X = self._check_transform_input_and_state(X)
        _check_contains_na(X, self.variables_)
        X = self._encode(X)
        return X

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["variables"] = "categorical"
        tags_dict["requires_y"] = True
        tags_dict["_skip_test"] = True
        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        return tags

================================================
FILE: feature_engine/imputation/__init__.py
================================================

from .arbitrary_number import ArbitraryNumberImputer
from .categorical import CategoricalImputer
from .drop_missing_data import DropMissingData
from .end_tail import EndTailImputer
from .mean_median import MeanMedianImputer
from .missing_indicator import AddMissingIndicator
from .random_sample import RandomSampleImputer

__all__ = [
    "MeanMedianImputer",
    "ArbitraryNumberImputer",
    "CategoricalImputer",
    "EndTailImputer",
    "AddMissingIndicator",
    "RandomSampleImputer",
    "DropMissingData",
]

================================================
FILE: feature_engine/imputation/arbitrary_number.py
================================================

from typing import List, Optional, Union

import pandas as pd

from feature_engine._check_init_parameters.check_input_dictionary import (
    _check_numerical_dict,
)
from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _imputer_dict_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_not_learn_docstring,
    _fit_transform_docstring,
    _transform_imputers_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import check_X
from feature_engine.imputation.base_imputer import BaseImputer
from feature_engine.variable_handling import (
    check_numerical_variables,
    find_numerical_variables,
)

@Substitution(
    imputer_dict_=_imputer_dict_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit=_fit_not_learn_docstring,
    transform=_transform_imputers_docstring,
    fit_transform=_fit_transform_docstring,
)
class ArbitraryNumberImputer(BaseImputer):

    def __init__(
        self,
        arbitrary_number: Union[int, float] = 999,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        imputer_dict: Optional[dict] = None,
    ) -> None:

        if isinstance(arbitrary_number, int) or isinstance(arbitrary_number, float):
            self.arbitrary_number = arbitrary_number
        else:
            raise ValueError("arbitrary_number must be numeric of type int or float")

        _check_numerical_dict(imputer_dict)

        self.variables = _check_variables_input_value(variables)

        self.imputer_dict = imputer_dict

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):

        X = check_X(X)

        if self.imputer_dict:
            self.variables_ = check_numerical_variables(
                X, list(self.imputer_dict.keys())
            )
            self.imputer_dict_ = self.imputer_dict
        else:
            if self.variables is None:
                self.variables_ = find_numerical_variables(X)
            else:
                self.variables_ = check_numerical_variables(X, self.variables)
            self.imputer_dict_ = {var: self.arbitrary_number for var in self.variables_}

        self._get_feature_names_in(X)

        return self

================================================
FILE: feature_engine/imputation/base_imputer.py
================================================
import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.utils.validation import check_is_fitted

from feature_engine._base_transformers.mixins import GetFeatureNamesOutMixin
from feature_engine.dataframe_checks import _check_X_matches_training_df, check_X
from feature_engine.tags import _return_tags

class BaseImputer(TransformerMixin, BaseEstimator, GetFeatureNamesOutMixin):

    def _transform(self, X: pd.DataFrame) -> pd.DataFrame:
        check_is_fitted(self)

        X = check_X(X)

        _check_X_matches_training_df(X, self.n_features_in_)

        X = X[self.feature_names_in_]

        return X

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:

        X = self._transform(X)

        with pd.option_context("future.no_silent_downcasting", True):
            X = X.fillna(value=self.imputer_dict_).infer_objects(copy=False)
        return X

    def _get_feature_names_in(self, X):

        self.feature_names_in_ = X.columns.to_list()
        self.n_features_in_ = X.shape[1]

        return self

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["allow_nan"] = True
        tags_dict["variables"] = "numerical"
        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        tags.input_tags.allow_nan = True
        return tags

================================================
FILE: feature_engine/imputation/categorical.py
================================================

from typing import List, Optional, Union

import pandas as pd

from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _imputer_dict_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_transform_docstring,
    _transform_imputers_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import check_X
from feature_engine.imputation.base_imputer import BaseImputer
from feature_engine.tags import _return_tags
from feature_engine.variable_handling import (
    check_all_variables,
    check_categorical_variables,
    find_all_variables,
    find_categorical_variables,
)

@Substitution(
    imputer_dict_=_imputer_dict_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    transform=_transform_imputers_docstring,
    fit_transform=_fit_transform_docstring,
)
class CategoricalImputer(BaseImputer):

    def __init__(
        self,
        imputation_method: str = "missing",
        fill_value: Union[str, int, float] = "Missing",
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        return_object: bool = False,
        ignore_format: bool = False,
    ) -> None:
        if imputation_method not in ["missing", "frequent"]:
            raise ValueError(
                "imputation_method takes only values 'missing' or 'frequent'"
            )

        if not isinstance(ignore_format, bool):
            raise ValueError("ignore_format takes only booleans True and False")

        self.imputation_method = imputation_method
        self.fill_value = fill_value
        self.variables = _check_variables_input_value(variables)
        self.return_object = return_object
        self.ignore_format = ignore_format

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):

        X = check_X(X)

        if self.ignore_format is True:
            if self.variables is None:
                self.variables_ = find_all_variables(X)
            else:
                self.variables_ = check_all_variables(X, self.variables)
        else:
            if self.variables is None:
                self.variables_ = find_categorical_variables(X)
            else:
                self.variables_ = check_categorical_variables(X, self.variables)

        if self.imputation_method == "missing":
            self.imputer_dict_ = {var: self.fill_value for var in self.variables_}

        elif self.imputation_method == "frequent":
            if len(self.variables_) == 1:
                var = self.variables_[0]
                mode_vals = X[var].mode()

                if len(mode_vals) > 1:
                    raise ValueError(
                        f"The variable {var} contains multiple frequent categories."
                    )

                self.imputer_dict_ = {var: mode_vals[0]}

            else:
                mode_vals = X[self.variables_].mode()

                if len(mode_vals) > 1:
                    varnames = mode_vals.dropna(axis=1).columns.to_list()
                    if len(varnames) > 1:
                        varnames_str = ", ".join(varnames)
                    else:
                        varnames_str = varnames[0]
                    raise ValueError(
                        f"The variable(s) {varnames_str} contain(s) multiple frequent "
                        f"categories."
                    )

                self.imputer_dict_ = mode_vals.iloc[0].to_dict()

        self._get_feature_names_in(X)

        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        if self.imputation_method == "frequent":
            X = super().transform(X)

        else:
            X = self._transform(X)

            add_cats = {}
            for variable in self.variables_:
                if X[variable].dtype.name == "category":
                    add_cats.update(
                        {
                            variable: X[variable].cat.add_categories(
                                self.imputer_dict_[variable]
                            )
                        }
                    )

            X = X.assign(**add_cats).fillna(self.imputer_dict_)

        if self.return_object:
            X[self.variables_] = X[self.variables_].astype("O")

        return X

    transform.__doc__ = BaseImputer.transform.__doc__

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["allow_nan"] = True
        tags_dict["variables"] = "categorical"
        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        tags.input_tags.allow_nan = True
        return tags

================================================
FILE: feature_engine/imputation/drop_missing_data.py
================================================

from typing import List, Optional, Union

import pandas as pd

from feature_engine._base_transformers.mixins import TransformXyMixin
from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
)
from feature_engine._docstrings.methods import _fit_transform_docstring
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import check_X
from feature_engine.imputation.base_imputer import BaseImputer
from feature_engine.tags import _return_tags
from feature_engine.variable_handling import check_all_variables, find_all_variables

@Substitution(
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit_transform=_fit_transform_docstring,
)
class DropMissingData(BaseImputer, TransformXyMixin):

    def __init__(
        self,
        missing_only: bool = True,
        threshold: Union[None, int, float] = None,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
    ) -> None:

        if not isinstance(missing_only, bool):
            raise ValueError(
                "missing_only takes values True or False. "
                f"Got {missing_only} instead."
            )

        if threshold is not None:
            if not isinstance(threshold, (int, float)) or not (0 < threshold <= 1):
                raise ValueError(
                    "threshold must be a value between 0 < x <= 1. "
                    f"Got {threshold} instead."
                )

        self.variables = _check_variables_input_value(variables)
        self.missing_only = missing_only
        self.threshold = threshold

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):

        X = check_X(X)

        if self.variables is None:
            self.variables_ = find_all_variables(X)
        else:
            self.variables_ = check_all_variables(X, self.variables)

        if self.threshold is None and self.missing_only is True:
            self.variables_ = [
                var for var in self.variables_ if X[var].isnull().sum() > 0
            ]

        self._get_feature_names_in(X)

        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:

        X = self._transform(X)

        if self.threshold:
            X.dropna(
                thresh=len(self.variables_) * self.threshold,
                subset=self.variables_,
                axis=0,
                inplace=True,
            )
        else:
            X.dropna(axis=0, how="any", subset=self.variables_, inplace=True)

        return X

    def return_na_data(self, X: pd.DataFrame) -> pd.DataFrame:

        X = self._transform(X)

        if self.threshold:
            idx = pd.isnull(X[self.variables_]).mean(axis=1) >= self.threshold
            idx = idx[idx]
        else:
            idx = pd.isnull(X[self.variables_]).any(axis=1)
            idx = idx[idx]

        return X.loc[idx.index, :]

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["allow_nan"] = True
        tags_dict["variables"] = "all"
        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        tags.input_tags.allow_nan = True
        return tags

================================================
FILE: feature_engine/imputation/end_tail.py
================================================

from typing import List, Optional, Union

import pandas as pd

from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _imputer_dict_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _variables_numerical_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_transform_docstring,
    _transform_imputers_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import check_X
from feature_engine.imputation.base_imputer import BaseImputer
from feature_engine.variable_handling import (
    check_numerical_variables,
    find_numerical_variables,
)

@Substitution(
    variables=_variables_numerical_docstring,
    imputer_dict_=_imputer_dict_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    transform=_transform_imputers_docstring,
    fit_transform=_fit_transform_docstring,
)
class EndTailImputer(BaseImputer):

    def __init__(
        self,
        imputation_method: str = "gaussian",
        tail: str = "right",
        fold: int = 3,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
    ) -> None:

        if imputation_method not in ["gaussian", "iqr", "max"]:
            raise ValueError(
                "imputation_method takes only values 'gaussian', 'iqr' or 'max'"
            )

        if tail not in ["right", "left"]:
            raise ValueError("tail takes only values 'right' or 'left'")

        if fold <= 0:
            raise ValueError("fold takes only positive numbers")

        self.imputation_method = imputation_method
        self.tail = tail
        self.fold = fold
        self.variables = _check_variables_input_value(variables)

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):
        X = check_X(X)

        if self.variables is None:
            self.variables_ = find_numerical_variables(X)
        else:
            self.variables_ = check_numerical_variables(X, self.variables)

        if self.imputation_method == "max":
            self.imputer_dict_ = (X[self.variables_].max() * self.fold).to_dict()

        elif self.imputation_method == "gaussian":
            if self.tail == "right":
                self.imputer_dict_ = (
                    X[self.variables_].mean() + self.fold * X[self.variables_].std()
                ).to_dict()
            elif self.tail == "left":
                self.imputer_dict_ = (
                    X[self.variables_].mean() - self.fold * X[self.variables_].std()
                ).to_dict()

        elif self.imputation_method == "iqr":
            IQR = X[self.variables_].quantile(0.75) - X[self.variables_].quantile(0.25)
            if self.tail == "right":
                self.imputer_dict_ = (
                    X[self.variables_].quantile(0.75) + (IQR * self.fold)
                ).to_dict()
            elif self.tail == "left":
                self.imputer_dict_ = (
                    X[self.variables_].quantile(0.25) - (IQR * self.fold)
                ).to_dict()

        self._get_feature_names_in(X)

        return self

================================================
FILE: feature_engine/imputation/mean_median.py
================================================

from typing import List, Optional, Union

import pandas as pd

from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _imputer_dict_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _variables_numerical_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_transform_docstring,
    _transform_imputers_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import check_X
from feature_engine.imputation.base_imputer import BaseImputer
from feature_engine.variable_handling import (
    check_numerical_variables,
    find_numerical_variables,
)

@Substitution(
    variables=_variables_numerical_docstring,
    imputer_dict_=_imputer_dict_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    transform=_transform_imputers_docstring,
    fit_transform=_fit_transform_docstring,
)
class MeanMedianImputer(BaseImputer):

    def __init__(
        self,
        imputation_method: str = "median",
        variables: Union[None, int, str, List[Union[str, int]]] = None,
    ) -> None:

        if imputation_method not in ["median", "mean"]:
            raise ValueError("imputation_method takes only values 'median' or 'mean'")

        self.imputation_method = imputation_method
        self.variables = _check_variables_input_value(variables)

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):

        X = check_X(X)

        if self.variables is None:
            self.variables_ = find_numerical_variables(X)
        else:
            self.variables_ = check_numerical_variables(X, self.variables)

        if self.imputation_method == "mean":
            self.imputer_dict_ = X[self.variables_].mean().to_dict()

        elif self.imputation_method == "median":
            self.imputer_dict_ = X[self.variables_].median().to_dict()

        self._get_feature_names_in(X)

        return self

================================================
FILE: feature_engine/imputation/missing_indicator.py
================================================

from typing import List, Optional, Union

import pandas as pd

from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
)
from feature_engine._docstrings.methods import _fit_transform_docstring
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import check_X
from feature_engine.imputation.base_imputer import BaseImputer
from feature_engine.tags import _return_tags
from feature_engine.variable_handling import check_all_variables, find_all_variables

@Substitution(
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit_transform=_fit_transform_docstring,
)
class AddMissingIndicator(BaseImputer):

    def __init__(
        self,
        missing_only: bool = True,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
    ) -> None:

        if not isinstance(missing_only, bool):
            raise ValueError("missing_only takes values True or False")

        self.variables = _check_variables_input_value(variables)
        self.missing_only = missing_only

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):

        X = check_X(X)

        if self.variables is None:
            self.variables_ = find_all_variables(X)
        else:
            self.variables_ = check_all_variables(X, self.variables)

        if self.missing_only is True:
            self.variables_ = [
                var for var in self.variables_ if X[var].isnull().sum() > 0
            ]

        self._get_feature_names_in(X)

        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:

        X = self._transform(X)

        indicator_names = [f"{feature}_na" for feature in self.variables_]
        X[indicator_names] = X[self.variables_].isna().astype(int)

        return X

    def _get_new_features_name(self) -> List:
        return [f"{feat}_na" for feat in self.variables_]

    def _add_new_feature_names(self, feature_names) -> List:
        return feature_names + self._get_new_features_name()

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["allow_nan"] = True
        tags_dict["variables"] = "all"
        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        tags.input_tags.allow_nan = True
        return tags

================================================
FILE: feature_engine/imputation/random_sample.py
================================================

from typing import List, Optional, Union

import numpy as np
import pandas as pd

from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_transform_docstring,
    _transform_imputers_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import check_X
from feature_engine.imputation.base_imputer import BaseImputer
from feature_engine.tags import _return_tags
from feature_engine.variable_handling import check_all_variables, find_all_variables

def _define_seed(
    X: pd.DataFrame,
    index: int,
    seed_variables: Union[str, int, List[Union[str, int]]],
    how: str = "add",
) -> int:
    if how == "add":
        internal_seed = int(np.round(X.loc[index, seed_variables].sum(), 0))
    elif how == "multiply":
        internal_seed = int(np.round(X.loc[index, seed_variables].product(), 0))
    return internal_seed

@Substitution(
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    transform=_transform_imputers_docstring,
    fit_transform=_fit_transform_docstring,
)
class RandomSampleImputer(BaseImputer):

    def __init__(
        self,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        random_state: Union[None, int, str, List[Union[str, int]]] = None,
        seed: str = "general",
        seeding_method: str = "add",
    ) -> None:

        if seed not in ["general", "observation"]:
            raise ValueError("seed takes only values 'general' or 'observation'")

        if seeding_method not in ["add", "multiply"]:
            raise ValueError("seeding_method takes only values 'add' or 'multiply'")

        if seed == "general" and random_state:
            if not isinstance(random_state, int):
                raise ValueError(
                    "if seed == 'general' then random_state must take an integer"
                )

        if seed == "observation" and not random_state:
            raise ValueError(
                "if seed == 'observation' the random state must take the name of one "
                "or more variables which will be used to seed the imputer"
            )

        self.variables = _check_variables_input_value(variables)
        self.random_state = random_state
        self.seed = seed
        self.seeding_method = seeding_method

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):

        X = check_X(X)

        if self.variables is None:
            self.variables_ = find_all_variables(X)
        else:
            self.variables_ = check_all_variables(X, self.variables)

        self.X_ = X[self.variables_].copy()

        if self.seed == "observation":
            self.random_state = _check_variables_input_value(self.random_state)
            if isinstance(self.random_state, (int, str)):
                self.random_state = [self.random_state]
            if self.random_state and any(
                var for var in self.random_state if var not in X.columns
            ):
                raise ValueError(
                    "There are variables assigned as random state which are not part "
                    "of the training dataframe."
                )

        self._get_feature_names_in(X)

        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:

        X = self._transform(X)

        if self.seed == "general":
            for feature in self.variables_:
                if X[feature].isnull().sum() > 0:
                    n_samples = X[feature].isnull().sum()

                    random_sample = (
                        self.X_[feature]
                        .dropna()
                        .sample(n_samples, replace=True, random_state=self.random_state)
                    )
                    random_sample.index = X[X[feature].isnull()].index

                    X.loc[X[feature].isnull(), feature] = random_sample

        elif self.seed == "observation" and self.random_state:
            for feature in self.variables_:
                if X[feature].isnull().sum() > 0:

                    for i in X[X[feature].isnull()].index:
                        internal_seed = _define_seed(
                            X, i, self.random_state, how=self.seeding_method
                        )

                        random_sample = (
                            self.X_[feature]
                            .dropna()
                            .sample(1, replace=True, random_state=internal_seed)
                        )
                        random_sample = random_sample.values[0]

                        X.loc[i, feature] = random_sample
        return X

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["allow_nan"] = True
        tags_dict["variables"] = "all"
        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        tags.input_tags.allow_nan = True
        return tags

================================================
FILE: feature_engine/outliers/__init__.py
================================================

from .artbitrary import ArbitraryOutlierCapper
from .trimmer import OutlierTrimmer
from .winsorizer import Winsorizer

__all__ = ["Winsorizer", "ArbitraryOutlierCapper", "OutlierTrimmer"]

================================================
FILE: feature_engine/outliers/artbitrary.py
================================================


from typing import Optional

import pandas as pd

from feature_engine._check_init_parameters.check_input_dictionary import (
    _check_numerical_dict,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _left_tail_caps_docstring,
    _n_features_in_docstring,
    _right_tail_caps_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _missing_values_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_not_learn_docstring,
    _fit_transform_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import (
    _check_contains_inf,
    _check_contains_na,
    check_X,
)
from feature_engine.outliers.base_outlier import BaseOutlier
from feature_engine.tags import _return_tags
from feature_engine.variable_handling import check_numerical_variables

@Substitution(
    missing_values=_missing_values_docstring,
    right_tail_caps_=_right_tail_caps_docstring,
    left_tail_caps_=_left_tail_caps_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit=_fit_not_learn_docstring,
    fit_transform=_fit_transform_docstring,
)
class ArbitraryOutlierCapper(BaseOutlier):

    def __init__(
        self,
        max_capping_dict: Optional[dict] = None,
        min_capping_dict: Optional[dict] = None,
        missing_values: str = "raise",
    ) -> None:

        if not max_capping_dict and not min_capping_dict:
            raise ValueError(
                "Please provide at least 1 dictionary with the capping values."
            )

        if missing_values not in ["raise", "ignore"]:
            raise ValueError("missing_values takes only values 'raise' or 'ignore'")

        _check_numerical_dict(max_capping_dict)
        _check_numerical_dict(min_capping_dict)

        self.max_capping_dict = max_capping_dict
        self.min_capping_dict = min_capping_dict
        self.missing_values = missing_values

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):
        X = check_X(X)

        if self.min_capping_dict is None and self.max_capping_dict:
            self.variables_ = [x for x in self.max_capping_dict.keys()]
        elif self.max_capping_dict is None and self.min_capping_dict:
            self.variables_ = [x for x in self.min_capping_dict.keys()]
        elif self.min_capping_dict and self.max_capping_dict:
            tmp = self.min_capping_dict.copy()
            tmp.update(self.max_capping_dict)
            self.variables_ = [x for x in tmp.keys()]

        if self.missing_values == "raise":
            _check_contains_na(X, self.variables_)
            _check_contains_inf(X, self.variables_)

        self.variables_ = check_numerical_variables(X, self.variables_)

        if self.max_capping_dict is not None:
            self.right_tail_caps_ = self.max_capping_dict
        else:
            self.right_tail_caps_ = {}

        if self.min_capping_dict is not None:
            self.left_tail_caps_ = self.min_capping_dict
        else:
            self.left_tail_caps_ = {}

        self.feature_names_in_ = X.columns.to_list()
        self.n_features_in_ = X.shape[1]

        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        return super()._transform(X)

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["_xfail_checks"][
            "check_parameters_default_constructible"
        ] = "transformer has 1 mandatory parameter"
        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        return tags

================================================
FILE: feature_engine/outliers/base_outlier.py
================================================
from typing import List, Literal, Optional, Union

import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.utils.validation import check_is_fitted

from feature_engine._base_transformers.mixins import GetFeatureNamesOutMixin
from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine.dataframe_checks import (
    _check_contains_inf,
    _check_contains_na,
    _check_X_matches_training_df,
    check_X,
)
from feature_engine.tags import _return_tags
from feature_engine.variable_handling import (
    check_numerical_variables,
    find_numerical_variables,
)

class BaseOutlier(TransformerMixin, BaseEstimator, GetFeatureNamesOutMixin):

    def _check_transform_input_and_state(self, X: pd.DataFrame) -> pd.DataFrame:
        check_is_fitted(self)

        X = check_X(X)

        _check_X_matches_training_df(X, self.n_features_in_)

        if self.missing_values == "raise":
            _check_contains_na(X, self.variables_)
            _check_contains_inf(X, self.variables_)

        X = X[self.feature_names_in_]

        return X

    def _transform(self, X: pd.DataFrame) -> pd.DataFrame:

        X = self._check_transform_input_and_state(X)

        for feature in self.right_tail_caps_.keys():
            X[feature] = X[feature].clip(upper=self.right_tail_caps_[feature])

        for feature in self.left_tail_caps_.keys():
            X[feature] = X[feature].clip(lower=self.left_tail_caps_[feature])

        return X

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["variables"] = "numerical"
        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        return tags

class WinsorizerBase(BaseOutlier):

    _intro_docstring = """The extreme values beyond which an observation is considered
    an outlier are determined using:

    - a Gaussian approximation
    - the inter-quantile range proximity rule (IQR)
    - MAD-median rule (MAD)
    - percentiles

    **Gaussian limits:**

    - right tail: mean + 3* std
    - left tail: mean - 3* std

    **IQR limits:**

    - right tail: 75th quantile + 1.5* IQR
    - left tail:  25th quantile - 1.5* IQR

    where IQR is the inter-quartile range: 75th quantile - 25th quantile.

    **MAD limits:**

    - right tail: median + 3.29* MAD
    - left tail:  median - 3.29* MAD

    where MAD is the median absoulte deviation from the median.

    **percentiles:**

    - right tail: 95th percentile
    - left tail:  5th percentile

    You can select how far out to cap the maximum or minimum values with the
    parameter `'fold'`.

    If `capping_method='gaussian'` fold gives the value to multiply the std.

    If `capping_method='iqr'` fold is the value to multiply the IQR.

    If `capping_method='mad'` fold is the value to multiply the MAD.

    If `capping_method='quantiles'`, fold is the percentile on each tail that should
    be censored. For example, if fold=0.05, the limits will be the 5th and 95th
    percentiles. If fold=0.1, the limits will be the 10th and 90th percentiles.
        Learn the values that should be used to replace outliers.

        Parameters
        ----------
        X : pandas dataframe of shape = [n_samples, n_features]
            The training input samples.

        y : pandas Series, default=None
            y is not needed in this transformer. You can pass y or None.================================================
FILE: feature_engine/outliers/trimmer.py
================================================

import pandas as pd

from feature_engine._base_transformers.mixins import TransformXyMixin
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _left_tail_caps_docstring,
    _n_features_in_docstring,
    _right_tail_caps_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _missing_values_docstring,
    _variables_numerical_docstring,
)
from feature_engine._docstrings.init_parameters.outliers import (
    _capping_method_docstring,
    _fold_docstring,
    _tail_docstring,
)
from feature_engine._docstrings.methods import _fit_transform_docstring
from feature_engine._docstrings.substitute import Substitution
from feature_engine.outliers.base_outlier import WinsorizerBase

@Substitution(
    intro_docstring=WinsorizerBase._intro_docstring,
    capping_method=_capping_method_docstring,
    tail=_tail_docstring,
    fold=_fold_docstring,
    variables=_variables_numerical_docstring,
    missing_values=_missing_values_docstring,
    right_tail_caps_=_right_tail_caps_docstring,
    left_tail_caps_=_left_tail_caps_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit_transform=_fit_transform_docstring,
)
class OutlierTrimmer(WinsorizerBase, TransformXyMixin):

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:

        X = self._check_transform_input_and_state(X)

        for feature in self.right_tail_caps_.keys():
            inliers = X[feature].le(self.right_tail_caps_[feature])
            X = X.loc[inliers]

        for feature in self.left_tail_caps_.keys():
            inliers = X[feature].ge(self.left_tail_caps_[feature])
            X = X.loc[inliers]

        return X

================================================
FILE: feature_engine/outliers/winsorizer.py
================================================

from typing import List, Literal, Union

import numpy as np
import pandas as pd

from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _left_tail_caps_docstring,
    _n_features_in_docstring,
    _right_tail_caps_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _missing_values_docstring,
    _variables_numerical_docstring,
)
from feature_engine._docstrings.init_parameters.outliers import (
    _capping_method_docstring,
    _fold_docstring,
    _tail_docstring,
)
from feature_engine._docstrings.methods import _fit_transform_docstring
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import check_X
from feature_engine.outliers.base_outlier import WinsorizerBase

@Substitution(
    intro_docstring=WinsorizerBase._intro_docstring,
    capping_method=_capping_method_docstring,
    tail=_tail_docstring,
    fold=_fold_docstring,
    variables=_variables_numerical_docstring,
    missing_values=_missing_values_docstring,
    right_tail_caps_=_right_tail_caps_docstring,
    left_tail_caps_=_left_tail_caps_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit_transform=_fit_transform_docstring,
)
class Winsorizer(WinsorizerBase):

    def __init__(
        self,
        capping_method: str = "gaussian",
        tail: str = "right",
        fold: Union[int, float, Literal["auto"]] = "auto",
        add_indicators: bool = False,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        missing_values: str = "raise",
    ) -> None:
        if not isinstance(add_indicators, bool):
            raise ValueError(
                "add_indicators takes only booleans True and False"
                f"Got {add_indicators} instead."
            )
        super().__init__(capping_method, tail, fold, variables, missing_values)
        self.add_indicators = add_indicators

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        if not self.add_indicators:
            X_out = super()._transform(X)

        else:
            X_orig = check_X(X)
            X_out = super()._transform(X_orig)
            X_orig = X_orig[self.variables_]
            X_out_filtered = X_out[self.variables_]

            if self.tail in ["left", "both"]:
                X_left = X_out_filtered > X_orig
                X_left.columns = [str(cl) + "_left" for cl in self.variables_]
            if self.tail in ["right", "both"]:
                X_right = X_out_filtered < X_orig
                X_right.columns = [str(cl) + "_right" for cl in self.variables_]
            if self.tail == "left":
                X_out = pd.concat([X_out, X_left.astype(np.float64)], axis=1)
            elif self.tail == "right":
                X_out = pd.concat([X_out, X_right.astype(np.float64)], axis=1)
            else:
                X_both = pd.concat([X_left, X_right], axis=1).astype(np.float64)
                X_both = X_both[
                    [
                        cl1
                        for cl2 in zip(X_left.columns.values, X_right.columns.values)
                        for cl1 in cl2
                    ]
                ]
                X_out = pd.concat([X_out, X_both], axis=1)

        return X_out

    def _get_new_features_name(self) -> List:
        if self.tail == "left":
            indicators = [str(cl) + "_left" for cl in self.variables_]
        elif self.tail == "right":
            indicators = [str(cl) + "_right" for cl in self.variables_]
        else:
            indicators = []
            for cl in self.variables_:
                indicators.append(str(cl) + "_left")
                indicators.append(str(cl) + "_right")
        return indicators

    def _add_new_feature_names(self, feature_names) -> List:
        if self.add_indicators is True:
            feature_names = feature_names + self._get_new_features_name()
        return feature_names

================================================
FILE: feature_engine/pipeline/__init__.py
================================================
from .pipeline import Pipeline, make_pipeline

__all__ = ["Pipeline", "make_pipeline"]

================================================
FILE: feature_engine/pipeline/pipeline.py
================================================


from sklearn import pipeline
from sklearn.base import _fit_context, clone
from sklearn.pipeline import _final_estimator_has, _fit_transform_one
try:
    from sklearn.utils import _print_elapsed_time
except ImportError:
    from sklearn.utils._user_interface import _print_elapsed_time
from sklearn.utils._metadata_requests import METHODS
from sklearn.utils._param_validation import HasMethods, Hidden
from sklearn.utils.metadata_routing import _routing_enabled, process_routing
from sklearn.utils.metaestimators import available_if
from sklearn.utils.validation import check_memory

METHODS.append("transform_x_y")

def _fit_transform_x_y_one(
    transformer, X, y, message_clsname="", message=None, params=None
):
    with _print_elapsed_time(message_clsname, message):
        transformer.fit(X, y)
        Xt, yt = transformer.transform_x_y(X, y, **params.get("transform_x_y", {}))
        return Xt, yt, transformer

class Pipeline(pipeline.Pipeline):

    _required_parameters = ["steps"]

    _parameter_constraints: dict = {
        "steps": [list, Hidden(tuple)],
        "memory": [None, str, HasMethods(["cache"])],
        "verbose": ["boolean"],
    }

    def __init__(self, steps, *, memory=None, verbose=False):
        self.steps = steps
        self.memory = memory
        self.verbose = verbose

    def _fit(self, X, y=None, routed_params=None):
        self.steps = list(self.steps)
        self._validate_steps()
        memory = check_memory(self.memory)

        fit_transform_one_cached = memory.cache(_fit_transform_one)
        fit_transform_x_y_one_cached = memory.cache(_fit_transform_x_y_one)

        for step_idx, name, transformer in self._iter(
            with_final=False, filter_passthrough=False
        ):
            if transformer is None or transformer == "passthrough":
                with _print_elapsed_time("Pipeline", self._log_message(step_idx)):
                    continue

            if hasattr(memory, "location") and memory.location is None:
                cloned_transformer = transformer
            else:
                cloned_transformer = clone(transformer)

            if hasattr(cloned_transformer, "transform_x_y"):
                X, y, fitted_transformer = fit_transform_x_y_one_cached(
                    cloned_transformer,
                    X,
                    y,
                    message_clsname="Pipeline",
                    message=self._log_message(step_idx),
                    params=routed_params[name],
                )
            elif hasattr(cloned_transformer, "transform") or hasattr(
                cloned_transformer, "fit_transform"
            ):
                X, fitted_transformer = fit_transform_one_cached(
                    cloned_transformer,
                    X,
                    y,
                    None,
                    message_clsname="Pipeline",
                    message=self._log_message(step_idx),
                    params=routed_params[name],
                )

            self.steps[step_idx] = (name, fitted_transformer)
        return X, y

    @_fit_context(
        prefer_skip_nested_validation=False
    )
    def fit(self, X, y=None, **params):
        routed_params = self._check_method_params(method="fit", props=params)
        Xt, yt = self._fit(X, y, routed_params)
        with _print_elapsed_time("Pipeline", self._log_message(len(self.steps) - 1)):
            if self._final_estimator != "passthrough":
                last_step_params = routed_params[self.steps[-1][0]]
                self._final_estimator.fit(Xt, yt, **last_step_params["fit"])
        return self

    def _can_fit_transform(self):
        return (
            self._final_estimator == "passthrough"
            or hasattr(self._final_estimator, "transform")
            or hasattr(self._final_estimator, "fit_transform")
        )

    @available_if(_can_fit_transform)
    @_fit_context(
        prefer_skip_nested_validation=False
    )
    def fit_transform(self, X, y=None, **params):
        routed_params = self._check_method_params(method="fit_transform", props=params)
        Xt, yt = self._fit(X, y, routed_params)

        last_step = self._final_estimator
        with _print_elapsed_time("Pipeline", self._log_message(len(self.steps) - 1)):
            if last_step == "passthrough":
                return Xt
            last_step_params = routed_params[self.steps[-1][0]]
            if hasattr(last_step, "fit_transform"):
                return last_step.fit_transform(
                    Xt, yt, **last_step_params["fit_transform"]
                )
            else:
                return last_step.fit(Xt, y, **last_step_params["fit"]).transform(
                    Xt, **last_step_params["transform"]
                )

    @available_if(_final_estimator_has("fit_predict"))
    @_fit_context(
        prefer_skip_nested_validation=False
    )
    def fit_predict(self, X, y=None, **params):
        routed_params = self._check_method_params(method="fit_predict", props=params)
        Xt, yt = self._fit(X, y, routed_params)

        params_last_step = routed_params[self.steps[-1][0]]
        with _print_elapsed_time("Pipeline", self._log_message(len(self.steps) - 1)):
            y_pred = self.steps[-1][1].fit_predict(
                Xt, yt, **params_last_step.get("fit_predict", {})
            )
        return y_pred

    def _can_transform_x_y(self):
        can_transform_x_y = any(
            [
                transformer
                for _, _, transformer in self._iter(
                    with_final=True, filter_passthrough=False
                )
                if hasattr(transformer, "transform_x_y")
            ]
        )
        last_step_is_transform = self._final_estimator == "passthrough" or hasattr(
            self._final_estimator, "transform"
        )
        return can_transform_x_y and last_step_is_transform

    @available_if(_can_transform_x_y)
    def transform_x_y(self, X, y, **params):
        routed_params = super()._check_method_params(method="transform", props=params)

        Xt = X
        yt = y
        for _, name, transform in self._iter():
            if hasattr(transform, "transform_x_y"):
                Xt, yt = transform.transform_x_y(
                    Xt, yt, **routed_params[name].transform
                )
            else:
                Xt = transform.transform(Xt, **routed_params[name].transform)
        return Xt, yt

        Xt, yt = self._fit(X, y, routed_params)

    @available_if(pipeline._final_estimator_has("score"))
    def score(self, X, y=None, sample_weight=None, **params):
        Xt = X
        yt = y
        if not _routing_enabled():
            for _, name, transform in self._iter(with_final=False):
                if hasattr(transform, "transform_x_y"):
                    Xt, yt = transform.transform_x_y(Xt, yt)
                else:
                    Xt = transform.transform(Xt)
            score_params = {}
            if sample_weight is not None:
                score_params["sample_weight"] = sample_weight
            return self.steps[-1][1].score(Xt, yt, **score_params)

        routed_params = process_routing(
            self, "score", sample_weight=sample_weight, **params
        )
        for _, name, transform in self._iter(with_final=False):
            if hasattr(transform, "transform_x_y"):
                Xt, yt = transform.transform_x_y(
                    Xt, yt, **routed_params[name].transform
                )
            else:
                Xt = transform.transform(Xt, **routed_params[name].transform)
        return self.steps[-1][1].score(Xt, yt, **routed_params[self.steps[-1][0]].score)

def make_pipeline(*steps, memory=None, verbose=False):
    return Pipeline(pipeline._name_estimators(steps), memory=memory, verbose=verbose)

================================================
FILE: feature_engine/preprocessing/__init__.py
================================================

from .match_categories import MatchCategories
from .match_columns import MatchVariables

__all__ = [
    "MatchCategories",
    "MatchVariables",
]

================================================
FILE: feature_engine/preprocessing/match_categories.py
================================================
import warnings
from typing import List, Optional, Union

import pandas as pd

from feature_engine._base_transformers.mixins import GetFeatureNamesOutMixin
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _missing_values_docstring,
    _variables_categorical_docstring,
)
from feature_engine._docstrings.init_parameters.encoders import _ignore_format_docstring
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import _check_optional_contains_na, check_X
from feature_engine.encoding.base_encoder import (
    CategoricalInitMixinNA,
    CategoricalMethodsMixin,
)

@Substitution(
    ignore_format=_ignore_format_docstring,
    missing_values=_missing_values_docstring,
    variables=_variables_categorical_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
)
class MatchCategories(
    CategoricalMethodsMixin, CategoricalInitMixinNA, GetFeatureNamesOutMixin
):

    def __init__(
        self,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        ignore_format: bool = False,
        missing_values: str = "raise",
    ) -> None:

        super().__init__(variables, missing_values, ignore_format)

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):
        X = check_X(X)
        variables_ = self._check_or_select_variables(X)

        if self.missing_values == "raise":
            _check_optional_contains_na(X, variables_)

        self.category_dict_ = dict()
        for var in variables_:
            self.category_dict_[var] = pd.Categorical(X[var]).categories

        self.variables_ = variables_
        self._get_feature_names_in(X)
        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X = self._check_transform_input_and_state(X)

        if self.missing_values == "raise":
            _check_optional_contains_na(X, self.variables_)

        for feature, levels in self.category_dict_.items():
            X[feature] = pd.Categorical(X[feature], levels)

        self._check_nas_in_result(X)
        return X

    def _check_nas_in_result(self, X: pd.DataFrame):
        if X[self.category_dict_.keys()].isnull().sum().sum() > 0:

            nan_columns = (
                X[self.category_dict_.keys()]
                .columns[X[self.category_dict_.keys()].isnull().any()]
                .tolist()
            )

            if len(nan_columns) > 1:
                nan_columns_str = ", ".join(nan_columns)
            else:
                nan_columns_str = nan_columns[0]

            if self.missing_values == "ignore":
                warnings.warn(
                    "During the encoding, NaN values were introduced in the feature(s) "
                    f"{nan_columns_str}."
                )
            elif self.missing_values == "raise":
                raise ValueError(
                    "During the encoding, NaN values were introduced in the feature(s) "
                    f"{nan_columns_str}."
                )

================================================
FILE: feature_engine/preprocessing/match_columns.py
================================================
from typing import Dict, List, Union

import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.utils.validation import check_is_fitted

from feature_engine._base_transformers.mixins import GetFeatureNamesOutMixin
from feature_engine.dataframe_checks import _check_contains_na, check_X
from feature_engine.tags import _return_tags

class MatchVariables(TransformerMixin, BaseEstimator, GetFeatureNamesOutMixin):

    def __init__(
        self,
        fill_value: Union[str, int, float] = np.nan,
        missing_values: str = "raise",
        match_dtypes: bool = False,
        verbose: bool = True,
    ):
        if missing_values not in ["raise", "ignore"]:
            raise ValueError(
                "missing_values takes only values 'raise' or 'ignore'."
                f"Got '{missing_values} instead."
            )

        if not isinstance(match_dtypes, bool):
            raise ValueError(
                "match_dtypes takes only booleans True and False. "
                f"Got '{match_dtypes} instead."
            )

        if not isinstance(verbose, bool):
            raise ValueError(
                "verbose takes only booleans True and False." f"Got '{verbose} instead."
            )

        if not isinstance(fill_value, (str, int, float)):
            raise ValueError(
                "fill_value takes integers, floats or strings."
                f"Got '{fill_value} instead."
            )

        self.fill_value = fill_value
        self.missing_values = missing_values
        self.match_dtypes = match_dtypes
        self.verbose = verbose

    def fit(self, X: pd.DataFrame, y: pd.Series = None):
        X = check_X(X)

        if self.missing_values == "raise":
            _check_contains_na(X, X.columns)

        self.feature_names_in_: List[Union[str, int]] = X.columns.tolist()

        self.n_features_in_ = X.shape[1]

        if self.match_dtypes:
            self.dtype_dict_: Dict = X.dtypes.to_dict()

        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        check_is_fitted(self)

        X = check_X(X)

        if self.missing_values == "raise":
            vars = [var for var in self.feature_names_in_ if var in X.columns]
            _check_contains_na(X, vars)

        _columns_to_drop = list(set(X.columns) - set(self.feature_names_in_))
        _columns_to_add = list(set(self.feature_names_in_) - set(X.columns))

        if self.verbose:
            if len(_columns_to_add) > 0:
                print(
                    "The following variables are added to the DataFrame: "
                    f"{_columns_to_add}"
                )
            if len(_columns_to_drop) > 0:
                print(
                    "The following variables are dropped from the DataFrame: "
                    f"{_columns_to_drop}"
                )

        X = X.drop(_columns_to_drop, axis=1)

        X = X.reindex(columns=self.feature_names_in_, fill_value=self.fill_value)

        if self.match_dtypes:
            _current_dtypes = X.dtypes.to_dict()
            _columns_to_update = {
                column: new_dtype
                for column, new_dtype in self.dtype_dict_.items()
                if new_dtype != _current_dtypes[column]
            }

            if self.verbose:
                for column, new_dtype in _columns_to_update.items():
                    print(
                        f"The {column} dtype is changing from ",
                        f"{_current_dtypes[column]} to {new_dtype}",
                    )

            X = X.astype(_columns_to_update)

        return X

    def _more_tags(self):
        tags_dict = _return_tags()

        msg = "input shape of dataframes in fit and transform can differ"
        tags_dict["_xfail_checks"]["check_transformer_general"] = msg

        msg = (
            "transformer takes categorical variables, and inf cannot be determined"
            "on these variables. Thus, check is not implemented"
        )
        tags_dict["_xfail_checks"]["check_estimators_nan_inf"] = msg

        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        return tags

================================================
FILE: feature_engine/scaling/__init__.py
================================================

from .mean_normalization import MeanNormalizationScaler

__all__ = [
    "MeanNormalizationScaler",
]

================================================
FILE: feature_engine/scaling/mean_normalization.py
================================================

from typing import List, Optional, Union

import pandas as pd

from feature_engine._base_transformers.base_numerical import BaseNumericalTransformer
from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _variables_numerical_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_transform_docstring,
    _inverse_transform_docstring,
)
from feature_engine._docstrings.substitute import Substitution

@Substitution(
    variables=_variables_numerical_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit_transform=_fit_transform_docstring,
    inverse_transform=_inverse_transform_docstring,
)
class MeanNormalizationScaler(BaseNumericalTransformer):

    def __init__(
        self,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
    ) -> None:

        self.variables = _check_variables_input_value(variables)

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):

        X = super().fit(X)
        self.mean_ = X[self.variables_].mean().to_dict()
        self.range_ = (X[self.variables_].max() - X[self.variables_].min()).to_dict()

        constant_columns = [col for col, value in self.range_.items() if value == 0]
        if constant_columns:
            raise ValueError(
                f"The following variable(s) are constant: {constant_columns}. "
                "Division by zero is not allowed. Please remove constant columns."
            )

        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:

        X = self._check_transform_input_and_state(X)

        X[self.variables_] = (X[self.variables_] - self.mean_) / self.range_

        return X

    def inverse_transform(self, X: pd.DataFrame) -> pd.DataFrame:

        X = self._check_transform_input_and_state(X)

        X[self.variables_] = X[self.variables_] * self.range_ + self.mean_

        return X

================================================
FILE: feature_engine/selection/__init__.py
================================================
from .drop_constant_features import DropConstantFeatures
from .drop_correlated_features import DropCorrelatedFeatures
from .drop_duplicate_features import DropDuplicateFeatures
from .drop_features import DropFeatures
from .drop_psi_features import DropHighPSIFeatures
from .information_value import SelectByInformationValue
from .probe_feature_selection import ProbeFeatureSelection
from .recursive_feature_addition import RecursiveFeatureAddition
from .recursive_feature_elimination import RecursiveFeatureElimination
from .shuffle_features import SelectByShuffling
from .single_feature_performance import SelectBySingleFeaturePerformance
from .smart_correlation_selection import SmartCorrelatedSelection
from .target_mean_selection import SelectByTargetMeanPerformance
from .mrmr import MRMR

__all__ = [
    "DropFeatures",
    "DropConstantFeatures",
    "DropDuplicateFeatures",
    "DropCorrelatedFeatures",
    "DropHighPSIFeatures",
    "SmartCorrelatedSelection",
    "SelectByShuffling",
    "SelectBySingleFeaturePerformance",
    "RecursiveFeatureAddition",
    "RecursiveFeatureElimination",
    "SelectByTargetMeanPerformance",
    "SelectByInformationValue",
    "ProbeFeatureSelection",
    "MRMR",
]

================================================
FILE: feature_engine/selection/_selection_constants.py
================================================
_CLASSIFICATION_METRICS = [
    "accuracy",
    "balanced_accuracy",
    "top_k_accuracy",
    "average_precision",
    "neg_brier_score",
    "f1",
    "f1_micro",
    "f1_macro",
    "f1_weighted",
    "f1_samples",
    "neg_log_loss",
    "precision",
    "precision_micro",
    "precision_macro",
    "precision_weighted",
    "precision_samples",
    "recall",
    "recall_micro",
    "recall_macro",
    "recall_weighted",
    "recall_samples",
    "jaccard",
    "jaccard_micro",
    "jaccard_macro",
    "jaccard_weighted",
    "jaccard_samples",
    "roc_auc",
    "roc_auc_ovr",
    "roc_auc_ovo",
    "roc_auc_ovr_weighted",
    "roc_auc_ovo_weighted",
]

_REGRESSION_METRICS = [
    "explained_variance",
    "r2",
    "max_error",
    "neg_median_absolute_error",
    "neg_mean_absolute_error",
    "neg_mean_absolute_percentage_error",
    "neg_mean_squared_error",
    "neg_mean_squared_log_error",
    "neg_root_mean_squared_error",
    "neg_mean_poisson_deviance",
    "neg_mean_gamma_deviance",
]

================================================
FILE: feature_engine/selection/base_recursive_selector.py
================================================
from types import GeneratorType
from typing import List, Union

import pandas as pd
from sklearn.inspection import permutation_importance
from sklearn.model_selection import cross_validate

from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine.dataframe_checks import check_X_y
from feature_engine.selection.base_selection_functions import get_feature_importances
from feature_engine.selection.base_selector import BaseSelector
from feature_engine.tags import _return_tags
from feature_engine.variable_handling import (
    check_numerical_variables,
    find_numerical_variables,
    retain_variables_if_in_df,
)

Variables = Union[None, int, str, List[Union[str, int]]]

class BaseRecursiveSelector(BaseSelector):

    def __init__(
        self,
        estimator,
        scoring: str = "roc_auc",
        cv=3,
        groups=None,
        threshold: Union[int, float] = 0.01,
        variables: Variables = None,
        confirm_variables: bool = False,
    ):

        if not isinstance(threshold, (int, float)):
            raise ValueError("threshold can only be integer or float")

        super().__init__(confirm_variables)
        self.variables = _check_variables_input_value(variables)
        self.estimator = estimator
        self.scoring = scoring
        self.threshold = threshold
        self.cv = cv
        self.groups = groups

    def fit(self, X: pd.DataFrame, y: pd.Series):

        X, y = check_X_y(X, y)

        if self.variables is None:
            self.variables_ = find_numerical_variables(X)
        else:
            if self.confirm_variables is True:
                variables_ = retain_variables_if_in_df(X, self.variables)
                self.variables_ = check_numerical_variables(X, variables_)
            else:
                self.variables_ = check_numerical_variables(X, self.variables)

        self._cv = list(self.cv) if isinstance(self.cv, GeneratorType) else self.cv

        self._check_variable_number()

        self._get_feature_names_in(X)

        model = cross_validate(
            estimator=self.estimator,
            X=X[self.variables_],
            y=y,
            cv=self._cv,
            groups=self.groups,
            scoring=self.scoring,
            return_estimator=True,
        )

        self.initial_model_performance_ = model["test_score"].mean()

        feature_importances_cv = pd.DataFrame()

        for i in range(len(model["estimator"])):
            m = model["estimator"][i]

            if hasattr(m, "feature_importances_") or hasattr(m, "coef_"):
                feature_importances_cv[i] = get_feature_importances(m)
            else:
                r = permutation_importance(
                    m,
                    X[self.variables_],
                    y,
                    n_repeats=1,
                    random_state=10,
                )
                feature_importances_cv[i] = r.importances_mean

        feature_importances_cv.index = self.variables_

        self.feature_importances_ = feature_importances_cv.mean(axis=1)
        self.feature_importances_std_ = feature_importances_cv.std(axis=1)

        return X, y

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["variables"] = "numerical"
        tags_dict["requires_y"] = True
        tags_dict["_xfail_checks"][
            "check_parameters_default_constructible"
        ] = "transformer has 1 mandatory parameter"
        tags_dict["_xfail_checks"]["check_estimators_nan_inf"] = "transformer allows NA"

        msg = "transformers need more than 1 feature to work"
        tags_dict["_xfail_checks"]["check_fit2d_1feature"] = msg

        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        return tags

================================================
FILE: feature_engine/selection/base_selection_functions.py
================================================
from typing import List, Union
from types import GeneratorType

import numpy as np
import pandas as pd
from sklearn.model_selection import cross_validate

from feature_engine.variable_handling import (
    check_all_variables,
    check_numerical_variables,
    find_all_variables,
    find_numerical_variables,
    retain_variables_if_in_df,
)

Variables = Union[int, str, List[Union[str, int]], None]

def get_feature_importances(estimator):

    importances = getattr(estimator, "feature_importances_", None)

    coef_ = getattr(estimator, "coef_", None)

    if coef_ is not None:

        if estimator.coef_.ndim == 1:
            importances = np.abs(coef_)

        else:
            importances = np.linalg.norm(coef_, axis=0, ord=len(estimator.coef_))

        importances = list(importances)

    return importances

def _select_all_variables(
    X: pd.DataFrame,
    variables: Variables,
    confirm_variables: bool,
    exclude_datetime: bool = False,
):
    if variables is None:
        variables_ = find_all_variables(X, exclude_datetime)
    else:
        if confirm_variables is True:
            variables_ = retain_variables_if_in_df(X, variables)
            variables_ = check_all_variables(X, variables_)
        else:
            variables_ = check_all_variables(X, variables)
    return variables_

def _select_numerical_variables(
    X: pd.DataFrame,
    variables: Variables,
    confirm_variables: bool,
):
    if variables is None:
        variables_ = find_numerical_variables(X)
    else:
        if confirm_variables is True:
            variables_ = retain_variables_if_in_df(X, variables)
            variables_ = check_numerical_variables(X, variables_)
        else:
            variables_ = check_numerical_variables(X, variables)
    return variables_

def find_correlated_features(
    X: pd.DataFrame,
    variables: list[Union[str, int]],
    method: str,
    threshold: float,
):
    correlated_matrix = X[variables].corr(method=method).to_numpy()

    correlated_mask = np.triu(np.abs(correlated_matrix), 1) > threshold

    examined = set()
    correlated_groups = list()
    features_to_drop = list()
    correlated_dict = {}
    for i, f_i in enumerate(variables):
        if f_i not in examined:
            examined.add(f_i)
            temp_set = set([f_i])
            for j, f_j in enumerate(variables):
                if f_j not in examined:
                    if correlated_mask[i, j] == 1:
                        examined.add(f_j)
                        features_to_drop.append(f_j)
                        temp_set.add(f_j)
            if len(temp_set) > 1:
                correlated_groups.append(temp_set)
                correlated_dict[f_i] = temp_set.difference({f_i})

    return correlated_groups, features_to_drop, correlated_dict

def single_feature_performance(
    X: pd.DataFrame,
    y: pd.Series,
    variables: List[Union[str, int]],
    estimator,
    cv,
    scoring,
    groups=None,
):
    feature_performance = {}
    feature_performance_std = {}

    cv = list(cv) if isinstance(cv, GeneratorType) else cv

    for feature in variables:
        model = cross_validate(
            estimator,
            X[feature].to_frame(),
            y,
            cv=cv,
            groups=groups,
            return_estimator=False,
            scoring=scoring,
        )

        feature_performance[feature] = model["test_score"].mean()
        feature_performance_std[feature] = model["test_score"].std()
    return feature_performance, feature_performance_std

def find_feature_importance(
    X: pd.DataFrame,
    y: pd.Series,
    estimator,
    cv,
    scoring,
    groups=None,
):
    cv = list(cv) if isinstance(cv, GeneratorType) else cv

    model = cross_validate(
        estimator,
        X,
        y,
        cv=cv,
        groups=groups,
        scoring=scoring,
        return_estimator=True,
    )

    feature_importances_cv = pd.DataFrame()

    for i in range(len(model["estimator"])):
        m = model["estimator"][i]
        feature_importances_cv[i] = get_feature_importances(m)

    feature_importances_cv.index = X.columns

    feature_importances_ = feature_importances_cv.mean(axis=1)
    feature_importances_std_ = feature_importances_cv.std(axis=1)
    return feature_importances_, feature_importances_std_

================================================
FILE: feature_engine/selection/base_selector.py
================================================
import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.utils.validation import check_is_fitted

from feature_engine._base_transformers.mixins import GetFeatureNamesOutMixin
from feature_engine.dataframe_checks import _check_X_matches_training_df, check_X
from feature_engine.tags import _return_tags

class BaseSelector(TransformerMixin, BaseEstimator, GetFeatureNamesOutMixin):

    def __init__(
        self,
        confirm_variables: bool = False,
    ) -> None:

        if not isinstance(confirm_variables, bool):
            raise ValueError(
                "confirm_variables takes only values True and False. "
                f"Got {confirm_variables} instead."
            )

        self.confirm_variables = confirm_variables

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:

        check_is_fitted(self)

        X = check_X(X)

        _check_X_matches_training_df(X, self.n_features_in_)

        X = X[self.feature_names_in_]

        return X.drop(columns=self.features_to_drop_)

    def _get_feature_names_in(self, X):

        self.feature_names_in_ = X.columns.to_list()
        self.n_features_in_ = X.shape[1]

        return self

    def _check_variable_number(self) -> None:
        if len(self.variables_) < 2:
            raise ValueError(
                "The selector needs at least 2 or more variables to select from. "
                f"Got only 1 variable: {self.variables_}."
            )

    def get_support(self, indices=False):
        mask = [
            True if f not in self.features_to_drop_ else False
            for f in self.feature_names_in_
        ]
        return mask if not indices else np.where(mask)[0]

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["variables"] = "numerical"
        tags_dict["_xfail_checks"]["check_estimators_nan_inf"] = "transformer allows NA"

        msg = "transformers need more than 1 feature to work"
        tags_dict["_xfail_checks"]["check_fit2d_1feature"] = msg

        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        return tags

================================================
FILE: feature_engine/selection/drop_constant_features.py
================================================
from typing import List, Union

import pandas as pd

from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
)
from feature_engine._docstrings.init_parameters.selection import (
    _confirm_variables_docstring,
)
from feature_engine._docstrings.methods import _fit_transform_docstring
from feature_engine._docstrings.selection._docstring import (
    _get_support_docstring,
    _variables_all_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import _check_contains_na, check_X
from feature_engine.selection.base_selector import BaseSelector
from feature_engine.tags import _return_tags

from .base_selection_functions import _select_all_variables

Variables = Union[None, int, str, List[Union[str, int]]]

@Substitution(
    confirm_variables=_confirm_variables_docstring,
    variables=_variables_all_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit_transform=_fit_transform_docstring,
    get_support=_get_support_docstring,
)
class DropConstantFeatures(BaseSelector):

    def __init__(
        self,
        variables: Variables = None,
        tol: float = 1,
        missing_values: str = "raise",
        confirm_variables: bool = False,
    ):

        if (
            not isinstance(tol, (float, int))
            or isinstance(tol, bool)
            or tol < 0
            or tol > 1
        ):
            raise ValueError("tol must be a float or integer between 0 and 1")

        if missing_values not in ["raise", "ignore", "include"]:
            raise ValueError(
                "missing_values takes only values 'raise', 'ignore' or " "'include'."
            )

        super().__init__(confirm_variables)

        self.tol = tol
        self.variables = _check_variables_input_value(variables)
        self.missing_values = missing_values

    def fit(self, X: pd.DataFrame, y: pd.Series = None):

        X = check_X(X)

        self.variables_ = _select_all_variables(
            X, self.variables, self.confirm_variables
        )

        if self.missing_values == "raise":
            _check_contains_na(X, self.variables_)

        if self.missing_values == "include":
            X[self.variables_] = X[self.variables_].fillna("missing_values")

        if self.tol == 1:
            self.features_to_drop_ = [
                feature for feature in self.variables_ if X[feature].nunique() == 1
            ]

        else:
            self.features_to_drop_ = []

            for feature in self.variables_:
                predominant = (
                    (X[feature].value_counts() / float(len(X)))
                    .sort_values(ascending=False)
                    .values[0]
                )

                if predominant >= self.tol:
                    self.features_to_drop_.append(feature)

        if len(self.features_to_drop_) == len(X.columns):
            raise ValueError(
                "The resulting dataframe will have no columns after dropping all "
                "constant or quasi-constant features. Try changing the tol value."
            )

        self._get_feature_names_in(X)

        return self

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["allow_nan"] = True
        tags_dict["variables"] = "all"
        tags_dict["_xfail_checks"][
            "check_fit2d_1sample"
        ] = "the transformer raises an error when dropping all columns, ok to fail"
        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        tags.input_tags.allow_nan = True
        return tags

================================================
FILE: feature_engine/selection/drop_correlated_features.py
================================================
from typing import List, Union

import pandas as pd

from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
)
from feature_engine._docstrings.init_parameters.selection import (
    _confirm_variables_docstring,
)
from feature_engine._docstrings.methods import _fit_transform_docstring
from feature_engine._docstrings.selection._docstring import (
    _get_support_docstring,
    _missing_values_docstring,
    _variables_attribute_docstring,
    _variables_numerical_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import (
    _check_contains_inf,
    _check_contains_na,
    check_X,
)
from feature_engine.selection.base_selector import BaseSelector

from .base_selection_functions import (
    _select_numerical_variables,
    find_correlated_features,
)

Variables = Union[None, int, str, List[Union[str, int]]]

@Substitution(
    confirm_variables=_confirm_variables_docstring,
    variables=_variables_numerical_docstring,
    missing_values=_missing_values_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit_transform=_fit_transform_docstring,
    get_support=_get_support_docstring,
)
class DropCorrelatedFeatures(BaseSelector):

    def __init__(
        self,
        variables: Variables = None,
        method: str = "pearson",
        threshold: float = 0.8,
        missing_values: str = "ignore",
        confirm_variables: bool = False,
    ):

        if not isinstance(threshold, float) or threshold < 0 or threshold > 1:
            raise ValueError(
                "`threshold` must be a float between 0 and 1. "
                f"Got {threshold} instead."
            )

        if missing_values not in ["raise", "ignore"]:
            raise ValueError(
                "`missing_values` takes only values 'raise' or 'ignore'. "
                f"Got {missing_values} instead."
            )

        super().__init__(confirm_variables)

        self.variables = _check_variables_input_value(variables)
        self.method = method
        self.threshold = threshold
        self.missing_values = missing_values

    def fit(self, X: pd.DataFrame, y: pd.Series = None):

        X = check_X(X)

        self.variables_ = _select_numerical_variables(
            X, self.variables, self.confirm_variables
        )

        self._check_variable_number()

        if self.missing_values == "raise":
            _check_contains_na(X, self.variables_)
            _check_contains_inf(X, self.variables_)

        features = sorted(self.variables_)

        correlated_groups, features_to_drop, correlated_dict = find_correlated_features(
            X, features, self.method, self.threshold
        )

        self.features_to_drop_ = features_to_drop
        self.correlated_feature_sets_ = correlated_groups
        self.correlated_feature_dict_ = correlated_dict

        self._get_feature_names_in(X)

        return self

================================================
FILE: feature_engine/selection/drop_duplicate_features.py
================================================
from collections import defaultdict
from typing import List, Union

import pandas as pd

from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
)
from feature_engine._docstrings.init_parameters.selection import (
    _confirm_variables_docstring,
)
from feature_engine._docstrings.methods import _fit_transform_docstring
from feature_engine._docstrings.selection._docstring import (
    _get_support_docstring,
    _missing_values_docstring,
    _variables_all_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import _check_contains_na, check_X
from feature_engine.selection.base_selector import BaseSelector
from feature_engine.tags import _return_tags

from .base_selection_functions import _select_all_variables

Variables = Union[None, int, str, List[Union[str, int]]]

@Substitution(
    confirm_variables=_confirm_variables_docstring,
    variables=_variables_all_docstring,
    missing_values=_missing_values_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit_transform=_fit_transform_docstring,
    get_support=_get_support_docstring,
)
class DropDuplicateFeatures(BaseSelector):

    def __init__(
        self,
        variables: Variables = None,
        missing_values: str = "ignore",
        confirm_variables: bool = False,
    ):
        if missing_values not in ["raise", "ignore"]:
            raise ValueError("missing_values takes only values 'raise' or 'ignore'.")

        super().__init__(confirm_variables)

        self.variables = _check_variables_input_value(variables)
        self.missing_values = missing_values

    def fit(self, X: pd.DataFrame, y: pd.Series = None):

        X = check_X(X)

        self.variables_ = _select_all_variables(
            X, self.variables, self.confirm_variables
        )

        self._check_variable_number()

        if self.missing_values == "raise":
            _check_contains_na(X, self.variables_)

        _features_hashmap = defaultdict(list)

        _X_hash = pd.util.hash_pandas_object(X[self.variables_].T, index=False)

        for feature, feature_hash in _X_hash.items():
            _features_hashmap[feature_hash].append(feature)

        self.duplicated_feature_sets_ = [
            set(duplicate)
            for duplicate in _features_hashmap.values()
            if len(duplicate) > 1
        ]

        self.features_to_drop_ = {
            item
            for duplicates in _features_hashmap.values()
            for item in duplicates[1:]
            if duplicates and len(duplicates) > 1
        }

        self._get_feature_names_in(X)

        return self

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["allow_nan"] = True
        tags_dict["variables"] = "all"

        msg = "transformers need more than 1 feature to work"
        tags_dict["_xfail_checks"]["check_fit2d_1feature"] = msg

        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        tags.input_tags.allow_nan = True
        return tags

================================================
FILE: feature_engine/selection/drop_features.py
================================================
from typing import List, Union

import pandas as pd

from feature_engine.dataframe_checks import check_X
from feature_engine.selection.base_selector import BaseSelector
from feature_engine.tags import _return_tags
from feature_engine.variable_handling import check_all_variables

class DropFeatures(BaseSelector):

    def __init__(self, features_to_drop: List[Union[str, int]]):
        if not isinstance(features_to_drop, (str, list)) or len(features_to_drop) == 0:
            raise ValueError(
                f"features_to_drop should be a list with the name of the variables "
                f"you wish to drop from the dataframe. Got {features_to_drop} instead."
            )

        self.features_to_drop = features_to_drop

    def fit(self, X: pd.DataFrame, y: pd.Series = None):
        X = check_X(X)

        self.features_to_drop_ = check_all_variables(X, variables=self.features_to_drop)

        if len(self.features_to_drop_) == len(X.columns):
            raise ValueError(
                "The resulting dataframe will have no columns after dropping all "
                "existing variables"
            )

        self._get_feature_names_in(X)

        return self

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["allow_nan"] = True
        tags_dict["_xfail_checks"][
            "check_parameters_default_constructible"
        ] = "transformer has 1 mandatory parameter"
        tags_dict["_xfail_checks"][
            "check_fit2d_1feature"
        ] = "the transformer raises an error when removing the only column, ok to fail"
        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        tags.input_tags.allow_nan = True
        return tags

================================================
FILE: feature_engine/selection/drop_psi_features.py
================================================
import datetime
from typing import Dict, List, Union

import numpy as np
import pandas as pd
import scipy.stats as stats
from pandas.api.types import is_numeric_dtype

from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
)
from feature_engine._docstrings.init_parameters.selection import (
    _confirm_variables_docstring,
)
from feature_engine._docstrings.methods import _fit_transform_docstring
from feature_engine._docstrings.selection._docstring import (
    _get_support_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import (
    _check_contains_inf,
    _check_contains_na,
    check_X,
)
from feature_engine.discretisation import (
    EqualFrequencyDiscretiser,
    EqualWidthDiscretiser,
)
from feature_engine.selection.base_selector import BaseSelector
from feature_engine.tags import _return_tags
from feature_engine.variable_handling import (
    find_categorical_and_numerical_variables,
    find_numerical_variables,
    retain_variables_if_in_df,
)

Variables = Union[None, int, str, List[Union[str, int]]]

PSI = """PSI = sum ( (test_i - basis_i) x ln(test_i/basis_i) )""".rstrip()

@Substitution(
    confirm_variables=_confirm_variables_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit_transform=_fit_transform_docstring,
    get_support=_get_support_docstring,
    psi=PSI,
)
class DropHighPSIFeatures(BaseSelector):
    r"""
    DropHighPSIFeatures() drops features which Population Stability Index (PSI) is
    above a given threshold.

    The PSI is used to compare distributions. Higher PSI values mean greater changes in
    a feature's distribution. Therefore, a feature with high PSI can be considered
    unstable.

    To compute the PSI, DropHighPSIFeatures() splits the dataset in two: a basis and
    a test set. Then, it compares the distribution of each feature between those sets.

    To determine the PSI, continuous features are sorted into discrete intervals, and
    then, the number of observations per interval are compared between the 2
    distributions.

    The PSI is calculated as:

    {psi}

    where `basis` and `test` are the 2 datasets, `i` refers to each interval, and then,
    `test_i` and `basis_i` are the number of observations in interval i in each data
    set.

    The PSI has traditionally been used to assess changes in distributions of
    continuous variables.

    In version 1.7, we extended the functionality of DropHighPSIFeatures() to
    calculate the PSI for categorical features as well. In this case, `i` is each
    unique category, and `test_i` and `basis_i` are the number of observations in
    category i.

    **Threshold**

    Different thresholds can be used to assess the magnitude of the distribution shift
    according to the PSI value. The most commonly used thresholds are:

    - Below 10%, the variable has not experienced a significant shift.
    - Above 25%, the variable has experienced a major shift.
    - Between those two values, the shift is intermediate.

    **Data split**

    To compute the PSI, DropHighPSIFeatures() splits the dataset in two: a basis and
    a test set. Then, it compares the distribution of each feature between those sets.

    There are various options to split a dataset:

    First, you can indicate which variable should be used to guide the data split. This
    variable can be of any data type. If you do not enter a variable name,
    DropHighPSIFeatures() will use the dataframe index.

    Next, you need to specify how that variable (or the index) should be used to split
    the data. You can specify a proportion of observations to be put in each data set,
    or alternatively, provide a cut-off value.

    If you specify a proportion through the `split_frac` parameter, the data will
    be sorted to accommodate that proportion. If `split_frac` is 0.5, 50% of the
    observations will go to either basis or test sets. If `split_frac` is 0.6, 60% of
    the samples will go to the basis data set and the remaining 40% to the test set.

    If `split_distinct` is True, the data will be sorted considering unique values in
    the selected variables. Check the parameter below for more details.

    If you define a numeric cut-off value or a specific date using the `cut_off`
    parameter, the observations with value <= cut-off will go to the basis data set and
    the remaining ones to the test set. If the variable used to guide the split is
    categorical, its values are sorted alphabetically and cut accordingly.

    If you pass a list of values in the `cut-off`, the observations with the values in
    the list, will go to the basis set, and the remaining ones to the test set.

    More details in the :ref:`User Guide <psi_selection>`.

    Parameters
    ----------
    split_col: string or int, default=None.
        The variable that will be used to split the dataset into the basis and test
        sets. If None, the dataframe index will be used. `split_col` can be a numerical,
        categorical or datetime variable. If `split_col` is a categorical variable, and
        the splitting criteria is given by `split_frac`, it will be assumed that the
        labels of the variable are sorted alphabetically.

    split_frac: float, default=0.5.
        The proportion of observations in each of the basis and test dataframes. If
        `split_frac` is 0.6, 60% of the observations will be put in the basis data set.

        If `split_distinct` is True, the indicated fraction may not be achieved exactly.
        See parameter `split_distinct` for more details.

        If `cut_off` is not None, `split_frac` will be ignored and the data split based
        on the `cut_off` value.

    split_distinct: boolean, default=False.
        If True, `split_frac` is applied to the vector of unique values in `split_col`
        instead of being applied to the whole vector of values. For example, if the
        values in `split_col` are [1, 1, 1, 1, 2, 2, 3, 4] and `split_frac` is
        0.5, we have the following:

            - `split_distinct=False` splits the vector in two equally sized parts:
                [1, 1, 1, 1] and [2, 2, 3, 4]. This involves that 2 dataframes with 4
                observations each are used for the PSI calculations.
            - `split_distinct=True` computes the vector of unique values in `split_col`
                ([1, 2, 3, 4]) and splits that vector in two equal parts: [1, 2] and
                [3, 4]. The number of observations in the two dataframes used for the
                PSI calculations is respectively 6 ([1, 1, 1, 1, 2, 2]) and 2 ([3, 4]).

    cut_off: int, float, date or list, default=None
        Threshold to split the dataset based on the `split_col` variable. If int, float
        or date, observations where the `split_col` values are <= threshold will
        go to the basis data set and the rest to the test set. If `cut_off` is a list,
        the observations where the `split_col` values are within the list will go to the
        basis data set and the remaining observations to the test set. If `cut_off` is
        not None, this parameter will be used to split the data and `split_frac` will be
        ignored.

    switch: boolean, default=False.
        If True, the order of the 2 dataframes used to determine the PSI (basis and
        test) will be switched. This is important because the interval limits used to
        calculate the PSI are inferred from the basis dataframe. Hence, changing the
        order of the dataframes may lead to different PSI values.

    threshold: float, str, default = 0.25.
        The threshold to drop a feature. If the PSI for a feature is >= threshold, the
        feature will be dropped. The most common threshold values are 0.25 (large shift)
        and 0.10 (medium shift).
        If 'auto', the threshold will be calculated based on the size of the basis and
        test dataset and the number of bins as:

                threshold = χ2(q, B−1) × (1/N + 1/M)

        where:

            - q = quantile of the distribution (or 1 - p-value),
            - B = number of bins/categories,
            - N = size of basis dataset,
            - M = size of test dataset.

        See formula (5.2) from reference [1].

    bins: int, default = 10
        Number of bins or intervals. For continuous features with good value spread, 10
        bins is commonly used. For features with lower cardinality or highly skewed
        distributions, lower values may be required.

    strategy: string, default='equal_frequency'
        If the intervals into which the features should be discretized are of equal
        size or equal number of observations. Takes values "equal_width" for equally
        spaced bins or "equal_frequency" for bins based on quantiles, that is, bins
        with similar number of observations.

    min_pct_empty_bins: float, default = 0.0001
        Value to add to empty bins or intervals. If after sorting the variable
        values into bins, a bin is empty, the PSI cannot be determined. By adding a
        small number to empty bins, we can avoid this issue. Note, that if the value
        added is too large, it may disturb the PSI calculation.

    missing_values: str, default='raise'
        Whether to perform the PSI feature selection on a dataframe with missing values.
        Takes values 'raise' or 'ignore'. If 'ignore', missing values will be dropped
        when determining the PSI for that particular feature. If 'raise' the transformer
        will raise an error and features will not be selected.

    p_value: float, default = 0.001
        The p-value to test the null hypothesis that there is no feature drift. In that
        case, the PSI-value approximates a random variable that follows a chi-square
        distribution. See [1] for details. This parameter is used only if `threshold`
        is set to 'auto'.

    variables: int, str, list, default = None
        The list of variables to evaluate. If `None`, the transformer will evaluate all
        numerical variables in the dataset. If `"all"` the transformer will evaluate all
        categorical and numerical variables in the dataset. Alternatively, the
        transformer will evaluate the variables indicated in the list or string.

    {confirm_variables}

    Attributes
    ----------
    features_to_drop_:
        List with the features that will be dropped.

    {variables_}

    psi_values_:
        Dictionary containing the PSI value per feature.

    cut_off_:
        Value used to split the dataframe into basis and test.
        This value is computed when not given as parameter.

    {feature_names_in_}

    {n_features_in_}

    Methods
    -------
    fit:
        Find features with high PSI values.

    {fit_transform}

    {get_support}

    transform:
        Remove features with high PSI values.

    See Also
    --------
    feature_engine.discretisation.EqualFrequencyDiscretiser
    feature_engine.discretisation.EqualWidthDiscretiser

    References
    ----------
    .. [1] Yurdakul B. "Statistical properties of population stability index".
       Western Michigan University, 2018.
       https://scholarworks.wmich.edu/dissertations/3208/

    Examples
    --------

    >>> import pandas as pd
    >>> from feature_engine.selection import DropHighPSIFeatures
    >>> X = pd.DataFrame(dict(
    >>>         x1 = [1,1,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0],
    >>>         x2 = [32,87,6,32,11,44,8,7,9,0,32,87,6,32,11,44,8,7,9,0],
    >>>         ))
    >>> psi = DropHighPSIFeatures()
    >>> psi.fit_transform(X)
        x2
    0   32
    1   87
    2    6
    3   32
    4   11
    5   44
    6    8
    7    7
    8    9
    9    0
    10  32
        Find features with high PSI values.

        Parameters
        ----------
        X : pandas dataframe of shape = [n_samples, n_features]
            The training dataset.

        y : pandas series. Default = None
            y is not needed in this transformer. You can pass y or None.
        numerical and categorical variables for which the PSI should be calculated.

        If `None`, select all numerical variables.
        If `"all", select all numerical and categorical variables.
        If string, int, or list, split into lists of numerical or categorical variables.
        categorical variable lists if necessary.

        It will get added if the variables are selected automatically.
        Obtain the fraction of observations per interval.

        Parameters
        ----------
        basis : pd.DataFrame.
            The basis Pandas DataFrame with discretised (i.e., binned) values.

        test: pd.DataFrame.
            The test Pandas DataFrame with discretised (i.e., binned) values.

        Returns
        -------
        distribution.basis: pd.Series.
            Basis Pandas Series with percentage of observations per bin.

        distribution.meas: pd.Series.
            Test Pandas Series with percentage of observations per bin.
        Split dataframe according to a cut-off value and return two dataframes: the
        basis dataframe contains all observations <= cut_off and the test dataframe the
        observations > cut_off.

        If cut-off is a list, then the basis dataframe will contain all observations
        which values are within the list, and the test dataframe all remaining
        observations.

        The cut-off value is associated to a specific column.

        Parameters
        ----------
        X : pandas dataframe

        Returns
        -------
        basis_df: pd.DataFrame
            pandas dataframe with observations which value <= cut_off

        test_df: pd.DataFrame
            pandas dataframe with observations which value > cut_off
        Find the cut-off value to split the dataframe. It is implemented when the user
        does not enter a cut_off value as a parameter. It is calculated based on
        split_frac.

        Finds the value in a pandas series at which we find the split_frac percentage
        of observations.

        If the reference column is numerical, the cut-off value is determined using
        np.quantile. Otherwise, the cut-off value is based on the value_counts:

            - The distinct values are sorted and the cumulative sum is
            used to compute the quantile. The value with the quantile that
            is the closest to the chosen split fraction is used as cut-off.

            - The procedure assumes that categorical values are sorted alphabetically
            and cut accordingly.

        Parameters
        ----------
        split_column: pd.Series.
            Series for which the nth quantile will be computed.

        Returns
        -------
        cut_off: (float, int, str, object).
            value for the cut-off.

        The threshold is given by:

            threshold = χ2(q,B−1) × (1/N + 1/M)

        where:

        q = quantile of the distribution (or 1 - p-value),
        B = number of bins/categories,
        N = size of basis dataset,
        M = size of test dataset.
        See formula (5.2) from reference [1] in the class docstring.

        Parameters
        ----------
        N: float or int
        M: float or int
        bins: int

        Returns
        -------
        float================================================
FILE: feature_engine/selection/information_value.py
================================================
from typing import List, Union

import numpy as np
import pandas as pd

from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.encoders import _ignore_format_docstring
from feature_engine._docstrings.init_parameters.selection import (
    _confirm_variables_docstring,
)
from feature_engine._docstrings.methods import _fit_transform_docstring
from feature_engine._docstrings.selection._docstring import (
    _features_to_drop_docstring,
    _get_support_docstring,
    _threshold_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import _check_contains_inf, _check_contains_na
from feature_engine.discretisation import (
    EqualFrequencyDiscretiser,
    EqualWidthDiscretiser,
)
from feature_engine.encoding.woe import WoE
from feature_engine.selection.base_selector import BaseSelector
from feature_engine.tags import _return_tags
from feature_engine.variable_handling import find_categorical_and_numerical_variables

from .base_selection_functions import _select_all_variables

Variables = Union[None, int, str, List[Union[str, int]]]

@Substitution(
    threshold=_threshold_docstring,
    ignore_format=_ignore_format_docstring,
    variables_=_variables_attribute_docstring,
    features_to_drop=_features_to_drop_docstring,
    feature_names_in=_feature_names_in_docstring,
    n_features_in=_n_features_in_docstring,
    fit_transform=_fit_transform_docstring,
    confirm_variables=_confirm_variables_docstring,
    get_support=_get_support_docstring,
)
class SelectByInformationValue(BaseSelector, WoE):

    def __init__(
        self,
        variables: Variables = None,
        bins: int = 5,
        strategy: str = "equal_width",
        threshold: Union[float, int] = 0.2,
        confirm_variables: bool = False,
    ) -> None:

        if not isinstance(bins, int) or isinstance(bins, int) and bins <= 0:
            raise ValueError(f"bins must be an integer. Got {bins} instead.")

        if strategy not in ["equal_width", "equal_frequency"]:
            raise ValueError(
                "strategy takes only values 'equal_width' or 'equal_frequency'. "
                f"Got {strategy} instead."
            )

        if not isinstance(threshold, (int, float)):
            raise ValueError(
                f"threshold must be a an integer or a float. Got {threshold} "
                "instead."
            )

        self.variables = _check_variables_input_value(variables)
        self.bins = bins
        self.strategy = strategy
        self.threshold = threshold
        self.confirm_variables = confirm_variables

    def fit(self, X: pd.DataFrame, y: pd.Series):
        X, y = self._check_fit_input(X, y)

        self.variables_ = _select_all_variables(
            X, self.variables, self.confirm_variables, exclude_datetime=True
        )

        _, variables_numerical = find_categorical_and_numerical_variables(
            X, self.variables_
        )

        _check_contains_na(X, self.variables_)
        _check_contains_inf(X, variables_numerical)

        self._get_feature_names_in(X)

        if len(variables_numerical) > 0:
            discretiser = self._make_discretiser(variables_numerical)
            X = discretiser.fit_transform(X)

        self.information_values_ = {}
        for var in self.variables_:
            total_pos, total_neg, woe = self._calculate_woe(X, y, var)
            iv = self._calculate_iv(total_pos, total_neg, woe)
            self.information_values_[var] = iv

        self.features_to_drop_ = [
            f
            for f in self.information_values_.keys()
            if self.information_values_[f] < self.threshold
        ]

        return self

    def _calculate_iv(self, pos, neg, woe):
        return np.sum((pos - neg) * woe)

    def _make_discretiser(self, variables):
        if self.strategy == "equal_width":
            discretiser = EqualWidthDiscretiser(
                bins=self.bins,
                variables=variables,
                return_boundaries=True,
            )
        else:
            discretiser = EqualFrequencyDiscretiser(
                q=self.bins,
                variables=variables,
                return_boundaries=True,
            )

        return discretiser

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["variables"] = "all"
        tags_dict["requires_y"] = True
        tags_dict["binary_only"] = True
        tags_dict["_skip_test"] = True
        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        return tags

================================================
FILE: feature_engine/selection/mrmr.py
================================================
import copy

from types import GeneratorType
from typing import List, Optional, Union

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.feature_selection import (
    f_classif,
    f_regression,
    mutual_info_classif,
    mutual_info_regression,
)
from sklearn.model_selection import GridSearchCV

from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
)
from feature_engine._docstrings.init_parameters.selection import (
    _confirm_variables_docstring,
)
from feature_engine._docstrings.methods import _fit_transform_docstring
from feature_engine._docstrings.selection._docstring import (
    _cv_docstring,
    _features_to_drop_docstring,
    _fit_docstring,
    _get_support_docstring,
    _scoring_docstring,
    _transform_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import check_X_y
from feature_engine.selection._selection_constants import (
    _CLASSIFICATION_METRICS,
    _REGRESSION_METRICS,
)
from feature_engine.selection.base_selector import BaseSelector
from feature_engine.tags import _return_tags
from feature_engine.variable_handling import (
    check_numerical_variables,
    find_numerical_variables,
    retain_variables_if_in_df,
)

_cv_docstring = _cv_docstring + """ Only used when `method = 'RFCQ'`."""

_scoring_docstring = _scoring_docstring + """. Only used when `method = 'RFCQ'`."""

Variables = Union[None, int, str, List[Union[str, int]]]

@Substitution(
    scoring=_scoring_docstring,
    cv=_cv_docstring,
    confirm_variables=_confirm_variables_docstring,
    features_to_drop_=_features_to_drop_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit=_fit_docstring,
    transform=_transform_docstring,
    fit_transform=_fit_transform_docstring,
    get_support=_get_support_docstring,
)
class MRMR(BaseSelector):

    def __init__(
        self,
        variables: Variables = None,
        method: str = "MIQ",
        max_features: Optional[int] = None,
        discrete_features="auto",
        n_neighbors=3,
        scoring: str = "roc_auc",
        cv=3,
        param_grid: Optional[dict] = None,
        regression: bool = False,
        confirm_variables: bool = False,
        random_state: Optional[int] = None,
        n_jobs: Optional[int] = None,
    ):

        if not isinstance(method, str) or method not in [
            "MIQ",
            "MID",
            "FCQ",
            "FCD",
            "RFCQ",
        ]:
            raise ValueError(
                "method must be one of 'MIQ', 'MID', 'FCQ', 'FCD', 'RFCQ'. "
                f"Got {method} instead."
            )

        if max_features is not None and not (
            isinstance(max_features, int) and max_features > 0
        ):
            raise ValueError(
                "max_features must be an integer with the number of features to "
                f"select. Got {max_features} instead."
            )

        if (
            variables is not None
            and max_features is not None
            and not isinstance(variables, int)
        ):
            if max_features >= len(variables):
                raise ValueError(
                    f"The number of variables to examine is {len(variables)}, which is "
                    "less than or equal to the number of features to select indicated "
                    f"in `max_features`, which is {max_features}. Please check the "
                    "values entered in the parameters `variables` and `max_features`."
                )

        if (
            regression is True
            and method == "RFCQ"
            and scoring not in _REGRESSION_METRICS
        ):
            raise ValueError(
                f"The metric {scoring} is not suitable for regression. Set the "
                "parameter regression to False or choose a different performance "
                "metric."
            )

        if (
            regression is False
            and method == "RFCQ"
            and scoring not in _CLASSIFICATION_METRICS
        ):
            raise ValueError(
                f"The metric {scoring} is not suitable for classification. Set the"
                "parameter regression to True or choose a different performance "
                "metric."
            )

        super().__init__(confirm_variables)
        self.variables = _check_variables_input_value(variables)
        self.method = method
        self.max_features = max_features
        self.discrete_features = discrete_features
        self.n_neighbors = n_neighbors
        self.scoring = scoring
        self.cv = cv
        self.param_grid = param_grid
        self.regression = regression
        self.random_state = random_state
        self.n_jobs = n_jobs

    def fit(self, X: pd.DataFrame, y: pd.Series):
        X, y = check_X_y(X, y)

        if self.variables is None:
            self.variables_ = find_numerical_variables(X)
        else:
            if self.confirm_variables is True:
                variables_ = retain_variables_if_in_df(X, self.variables)
                self.variables_ = check_numerical_variables(X, variables_)
            else:
                self.variables_ = check_numerical_variables(X, self.variables)

        self._check_variable_number()

        self._get_feature_names_in(X)

        self.relevance_ = self._calculate_relevance(X[self.variables_], y)

        relevance = self.relevance_
        n = relevance.argmax()
        relevance = np.delete(relevance, n)

        remaining = copy.deepcopy(self.variables_)
        feature = remaining[n]
        selected = [feature]
        remaining.remove(feature)

        redundance = self._calculate_redundance(X[remaining], X[feature])

        mrmr = self._calculate_mrmr(relevance, redundance)

        if self.max_features is None:
            iter = int(0.2 * len(self.variables_)) - 2
        else:
            iter = self.max_features - 2

        for i in range(iter):
            n = mrmr.argmax()

            feature = remaining[n]
            selected.append(feature)
            remaining.remove(feature)

            relevance = np.delete(relevance, n)
            if i == 0:
                redundance = np.delete(redundance, n)
            else:
                redundance = np.delete(redundance, n, axis=1)

            new_redundance = self._calculate_redundance(X[remaining], X[feature])
            redundance = np.vstack([redundance, new_redundance])
            mean_redundance = redundance.mean(axis=0)

            mrmr = self._calculate_mrmr(relevance, mean_redundance)

        n = mrmr.argmax()
        selected.append(remaining[n])

        self.features_to_drop_ = [f for f in self.variables_ if f not in selected]

        return self

    def _calculate_relevance(self, X, y):

        if self.method in ["MIQ", "MID"]:
            if self.regression is True:
                relevance = mutual_info_regression(
                    X=X,
                    y=y,
                    discrete_features=self.discrete_features,
                    n_neighbors=self.n_neighbors,
                    random_state=self.random_state,
                    n_jobs=self.n_jobs,
                )
            else:
                relevance = mutual_info_classif(
                    X=X,
                    y=y,
                    discrete_features=self.discrete_features,
                    n_neighbors=self.n_neighbors,
                    random_state=self.random_state,
                    n_jobs=self.n_jobs,
                )

        elif self.method in ["FCQ", "FCD"]:
            if self.regression is True:
                relevance = f_regression(X, y)[0]
            else:
                relevance = f_classif(X, y)[0]

        else:
            if self.regression is True:
                model = RandomForestRegressor(
                    random_state=self.random_state,
                    n_jobs=self.n_jobs,
                )
            else:
                model = RandomForestClassifier(
                    random_state=self.random_state,
                    n_jobs=self.n_jobs,
                )

            if self.param_grid:
                param_grid = self.param_grid
            else:
                param_grid = {"max_depth": [1, 2, 3, 4]}

            cv = list(self.cv) if isinstance(self.cv, GeneratorType) else self.cv

            model = GridSearchCV(
                model, cv=cv, scoring=self.scoring, param_grid=param_grid
            )

            model.fit(X, y)

            relevance = model.best_estimator_.feature_importances_

        return relevance

    def _calculate_redundance(self, X, y):

        if self.method in ["FCD", "FCQ", "RFCQ"]:
            redundance = X.corrwith(y).values
            redundance = np.absolute(redundance)

        else:
            redundance = mutual_info_regression(
                X=X,
                y=y,
                n_neighbors=self.n_neighbors,
                random_state=self.random_state,
                n_jobs=self.n_jobs,
            )

        return redundance

    def _calculate_mrmr(self, relevance, redundance):
        if self.method in ["MID", "FCD"]:
            mrmr = relevance - redundance
        else:
            mrmr = relevance / redundance
        return mrmr

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["variables"] = "numerical"
        tags_dict["requires_y"] = True
        tags_dict["_xfail_checks"]["check_estimators_nan_inf"] = "transformer allows NA"

        msg = "transformers need more than 1 feature to work"
        tags_dict["_xfail_checks"]["check_fit2d_1feature"] = msg
        tags_dict["_xfail_checks"]["check_fit2d_1sample"] = msg

        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        return tags

================================================
FILE: feature_engine/selection/probe_feature_selection.py
================================================
from typing import List, Union

import numpy as np
import pandas as pd

from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
)
from feature_engine._docstrings.init_parameters.selection import (
    _confirm_variables_docstring,
    _estimator_docstring,
)
from feature_engine._docstrings.methods import _fit_transform_docstring
from feature_engine._docstrings.selection._docstring import (
    _cv_docstring,
    _groups_docstring,
    _features_to_drop_docstring,
    _fit_docstring,
    _get_support_docstring,
    _scoring_docstring,
    _transform_docstring,
    _variables_attribute_docstring,
    _variables_numerical_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import check_X_y
from feature_engine.selection.base_selector import BaseSelector
from feature_engine.tags import _return_tags

from .base_selection_functions import (
    _select_numerical_variables,
    find_feature_importance,
    single_feature_performance,
)

Variables = Union[None, int, str, List[Union[str, int]]]

@Substitution(
    estimator=_estimator_docstring,
    scoring=_scoring_docstring,
    cv=_cv_docstring,
    groups=_groups_docstring,
    confirm_variables=_confirm_variables_docstring,
    variables=_variables_numerical_docstring,
    feature_names_in_=_feature_names_in_docstring,
    features_to_drop_=_features_to_drop_docstring,
    variables_=_variables_attribute_docstring,
    n_features_in_=_n_features_in_docstring,
    fit=_fit_docstring,
    transform=_transform_docstring,
    fit_transform=_fit_transform_docstring,
    get_support=_get_support_docstring,
)
class ProbeFeatureSelection(BaseSelector):

    def __init__(
        self,
        estimator,
        variables: Variables = None,
        collective: bool = True,
        scoring: str = "roc_auc",
        n_probes: int = 1,
        distribution: Union[str, list] = "normal",
        n_categories: int = 10,
        threshold: str = "mean",
        cv=5,
        groups=None,
        random_state: int = 0,
        confirm_variables: bool = False,
    ):
        if not isinstance(collective, bool):
            raise ValueError(
                f"collective takes values True or False. Got {collective} instead."
            )

        error_msg = (
            "distribution takes values 'normal', 'binary', 'uniform', "
            "'discrete_uniform', 'poisson', or 'all'. "
            f"Got {distribution} instead."
        )

        allowed_distributions = [
            "normal",
            "binary",
            "uniform",
            "discrete_uniform",
            "poisson",
            "all",
        ]

        if not isinstance(distribution, (str, list)):
            raise ValueError(error_msg)
        if isinstance(distribution, str) and distribution not in allowed_distributions:
            raise ValueError(error_msg)
        if isinstance(distribution, list) and not all(
            dist in allowed_distributions for dist in distribution
        ):
            raise ValueError(error_msg)

        if not isinstance(n_probes, int):
            raise ValueError(f"n_probes must be an integer. Got {n_probes} instead.")

        if not isinstance(n_categories, int) or n_categories < 1:
            raise ValueError(
                f"n_categories must be a positive integer. Got {n_categories} instead."
            )

        if not isinstance(threshold, str) or threshold not in [
            "mean",
            "max",
            "mean_plus_std",
        ]:
            raise ValueError(
                "threshold takes values 'mean', 'max' or 'mean_plus_std'. "
                f"Got {threshold} instead."
            )

        super().__init__(confirm_variables)
        self.estimator = estimator
        self.variables = variables
        self.collective = collective
        self.scoring = scoring
        self.distribution = distribution
        self.n_categories = n_categories
        self.cv = cv
        self.groups = groups
        self.n_probes = n_probes
        self.threshold = threshold
        self.random_state = random_state

    def fit(self, X: pd.DataFrame, y: pd.Series):
        X, y = check_X_y(X, y)

        self.variables_ = _select_numerical_variables(
            X, self.variables, self.confirm_variables
        )

        self._get_feature_names_in(X)

        self.probe_features_ = self._generate_probe_features(X.shape[0])

        X.reset_index(drop=True, inplace=True)

        X_new = pd.concat([X[self.variables_], self.probe_features_], axis=1)

        if self.collective is True:
            f_importance_mean, f_importance_std = find_feature_importance(
                X=X_new,
                y=y,
                estimator=self.estimator,
                cv=self.cv,
                groups=self.groups,
                scoring=self.scoring,
            )
            self.feature_importances_ = f_importance_mean
            self.feature_importances_std_ = f_importance_std

        else:
            f_importance_mean, f_importance_std = single_feature_performance(
                X=X_new,
                y=y,
                variables=X_new.columns,
                estimator=self.estimator,
                cv=self.cv,
                groups=self.groups,
                scoring=self.scoring,
            )
            self.feature_importances_ = pd.Series(f_importance_mean)
            self.feature_importances_std_ = pd.Series(f_importance_std)

        self.features_to_drop_ = self._get_features_to_drop()

        return self

    def _generate_probe_features(self, n_obs: int) -> pd.DataFrame:
        df = pd.DataFrame()

        np.random.seed(self.random_state)

        if isinstance(self.distribution, str):
            distribution = set([self.distribution])
        else:
            distribution = set(self.distribution)

        if {"normal", "all"} & distribution:
            for i in range(self.n_probes):
                df[f"gaussian_probe_{i}"] = np.random.normal(0, 3, n_obs)

        if {"binary", "all"} & distribution:
            for i in range(self.n_probes):
                df[f"binary_probe_{i}"] = np.random.randint(0, 2, n_obs)

        if {"uniform", "all"} & distribution:
            for i in range(self.n_probes):
                df[f"uniform_probe_{i}"] = np.random.uniform(0, 1, n_obs)

        if {"discrete_uniform", "all"} & distribution:
            for i in range(self.n_probes):
                df[f"discrete_uniform_probe_{i}"] = np.random.randint(
                    0, self.n_categories, n_obs
                )

        if {"poisson", "all"} & distribution:
            for i in range(self.n_probes):
                df[f"poisson_probe_{i}"] = np.random.poisson(self.n_categories, n_obs)

        return df

    def _get_features_to_drop(self):

        if self.probe_features_.shape[1] > 1:
            if self.threshold == "mean":
                threshold = self.feature_importances_[
                    self.probe_features_.columns
                ].values.mean()
            elif self.threshold == "max":
                threshold = self.feature_importances_[
                    self.probe_features_.columns
                ].values.max()
            else:
                threshold = (
                    self.feature_importances_[
                        self.probe_features_.columns
                    ].values.mean()
                    + 3
                    * self.feature_importances_[
                        self.probe_features_.columns
                    ].values.std()
                )

        else:
            threshold = self.feature_importances_[self.probe_features_.columns].values

        features_to_drop = []

        for var in self.variables_:
            if self.feature_importances_[var] < threshold:
                features_to_drop.append(var)

        return features_to_drop

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["variables"] = "numerical"
        tags_dict["requires_y"] = True
        tags_dict["_xfail_checks"]["check_estimators_nan_inf"] = "transformer allows NA"
        tags_dict["_xfail_checks"][
            "check_parameters_default_constructible"
        ] = "transformer has 1 mandatory parameter"

        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        return tags

================================================
FILE: feature_engine/selection/recursive_feature_addition.py
================================================
import pandas as pd
from sklearn.model_selection import cross_validate

from feature_engine._docstrings.fit_attributes import (
    _feature_importances_docstring,
    _feature_importances_std_docstring,
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _performance_drifts_docstring,
    _performance_drifts_std_docstring,
)
from feature_engine._docstrings.init_parameters.selection import (
    _confirm_variables_docstring,
)
from feature_engine._docstrings.methods import _fit_transform_docstring
from feature_engine._docstrings.selection._docstring import (
    _cv_docstring,
    _features_to_drop_docstring,
    _fit_docstring,
    _get_support_docstring,
    _groups_docstring,
    _initial_model_performance_docstring,
    _scoring_docstring,
    _threshold_docstring,
    _transform_docstring,
    _variables_attribute_docstring,
    _variables_numerical_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.selection.base_recursive_selector import BaseRecursiveSelector

@Substitution(
    scoring=_scoring_docstring,
    threshold=_threshold_docstring,
    cv=_cv_docstring,
    groups=_groups_docstring,
    variables=_variables_numerical_docstring,
    confirm_variables=_confirm_variables_docstring,
    initial_model_performance_=_initial_model_performance_docstring,
    feature_importances_=_feature_importances_docstring,
    feature_importances_std_=_feature_importances_std_docstring,
    performance_drifts_=_performance_drifts_docstring,
    performance_drifts_std_=_performance_drifts_std_docstring,
    features_to_drop_=_features_to_drop_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit=_fit_docstring,
    transform=_transform_docstring,
    fit_transform=_fit_transform_docstring,
    get_support=_get_support_docstring,
)
class RecursiveFeatureAddition(BaseRecursiveSelector):

    def fit(self, X: pd.DataFrame, y: pd.Series):

        X, y = super().fit(X, y)

        self.feature_importances_.sort_values(ascending=False, inplace=True)

        first_most_important_feature = list(self.feature_importances_.index)[0]

        baseline_model = cross_validate(
            estimator=self.estimator,
            X=X[first_most_important_feature].to_frame(),
            y=y,
            cv=self._cv,
            groups=self.groups,
            scoring=self.scoring,
            return_estimator=True,
        )

        baseline_model_performance = baseline_model["test_score"].mean()

        _selected_features = [first_most_important_feature]

        self.performance_drifts_ = {first_most_important_feature: 0}
        self.performance_drifts_std_ = {first_most_important_feature: 0}

        for feature in list(self.feature_importances_.index)[1:]:

            model_tmp = cross_validate(
                estimator=self.estimator,
                X=X[_selected_features + [feature]],
                y=y,
                cv=self._cv,
                groups=self.groups,
                scoring=self.scoring,
                return_estimator=True,
            )

            model_tmp_performance = model_tmp["test_score"].mean()

            performance_drift = model_tmp_performance - baseline_model_performance

            self.performance_drifts_[feature] = performance_drift
            self.performance_drifts_std_[feature] = model_tmp["test_score"].std()

            if performance_drift > self.threshold:
                _selected_features.append(feature)

                baseline_model_performance = model_tmp_performance

        self.features_to_drop_ = [
            f for f in self.variables_ if f not in _selected_features
        ]

        return self

================================================
FILE: feature_engine/selection/recursive_feature_elimination.py
================================================
import pandas as pd
from sklearn.model_selection import cross_validate

from feature_engine._docstrings.fit_attributes import (
    _feature_importances_docstring,
    _feature_importances_std_docstring,
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _performance_drifts_docstring,
    _performance_drifts_std_docstring,
)
from feature_engine._docstrings.init_parameters.selection import (
    _confirm_variables_docstring,
)
from feature_engine._docstrings.methods import _fit_transform_docstring
from feature_engine._docstrings.selection._docstring import (
    _cv_docstring,
    _features_to_drop_docstring,
    _fit_docstring,
    _get_support_docstring,
    _groups_docstring,
    _initial_model_performance_docstring,
    _scoring_docstring,
    _threshold_docstring,
    _transform_docstring,
    _variables_attribute_docstring,
    _variables_numerical_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.selection.base_recursive_selector import BaseRecursiveSelector

@Substitution(
    scoring=_scoring_docstring,
    threshold=_threshold_docstring,
    cv=_cv_docstring,
    groups=_groups_docstring,
    variables=_variables_numerical_docstring,
    confirm_variables=_confirm_variables_docstring,
    initial_model_performance_=_initial_model_performance_docstring,
    feature_importances_=_feature_importances_docstring,
    feature_importances_std_=_feature_importances_std_docstring,
    performance_drifts_=_performance_drifts_docstring,
    performance_drifts_std_=_performance_drifts_std_docstring,
    features_to_drop_=_features_to_drop_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit=_fit_docstring,
    transform=_transform_docstring,
    fit_transform=_fit_transform_docstring,
    get_support=_get_support_docstring,
)
class RecursiveFeatureElimination(BaseRecursiveSelector):

    def fit(self, X: pd.DataFrame, y: pd.Series):

        X, y = super().fit(X, y)

        self.feature_importances_.sort_values(ascending=True, inplace=True)

        _selected_features = []

        X_tmp = X[self.variables_].copy()

        baseline_model_performance = self.initial_model_performance_

        self.performance_drifts_ = {}
        self.performance_drifts_std_ = {}

        for feature in list(self.feature_importances_.index):

            if X_tmp.shape[1] == 1:
                self.performance_drifts_[feature] = 0
                _selected_features.append(feature)
                break

            model_tmp = cross_validate(
                estimator=self.estimator,
                X=X_tmp.drop(columns=feature),
                y=y,
                cv=self._cv,
                groups=self.groups,
                scoring=self.scoring,
                return_estimator=False,
            )

            model_tmp_performance = model_tmp["test_score"].mean()

            performance_drift = baseline_model_performance - model_tmp_performance

            self.performance_drifts_[feature] = performance_drift
            self.performance_drifts_std_[feature] = model_tmp["test_score"].std()

            if performance_drift > self.threshold:

                _selected_features.append(feature)

            else:
                X_tmp = X_tmp.drop(columns=feature)

                baseline_model = cross_validate(
                    estimator=self.estimator,
                    X=X_tmp,
                    y=y,
                    cv=self._cv,
                    groups=self.groups,
                    return_estimator=False,
                    scoring=self.scoring,
                )

                baseline_model_performance = baseline_model["test_score"].mean()

        self.features_to_drop_ = [
            f for f in self.variables_ if f not in _selected_features
        ]

        return self

================================================
FILE: feature_engine/selection/shuffle_features.py
================================================
from types import GeneratorType
from typing import List, MutableSequence, Union

import numpy as np
import pandas as pd
from sklearn.base import is_classifier
from sklearn.metrics import get_scorer
from sklearn.model_selection import check_cv, cross_validate
from sklearn.utils.validation import _check_sample_weight, check_random_state

from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
)
from feature_engine._docstrings.init_parameters.selection import (
    _confirm_variables_docstring,
)
from feature_engine._docstrings.methods import _fit_transform_docstring
from feature_engine._docstrings.selection._docstring import (
    _cv_docstring,
    _estimator_docstring,
    _features_to_drop_docstring,
    _fit_docstring,
    _get_support_docstring,
    _initial_model_performance_docstring,
    _scoring_docstring,
    _threshold_docstring,
    _transform_docstring,
    _variables_attribute_docstring,
    _variables_numerical_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import check_X_y
from feature_engine.selection.base_selector import BaseSelector
from feature_engine.tags import _return_tags

from .base_selection_functions import _select_numerical_variables

Variables = Union[None, int, str, List[Union[str, int]]]

@Substitution(
    estimator=_estimator_docstring,
    scoring=_scoring_docstring,
    threshold=_threshold_docstring,
    cv=_cv_docstring,
    variables=_variables_numerical_docstring,
    confirm_variables=_confirm_variables_docstring,
    initial_model_performance_=_initial_model_performance_docstring,
    features_to_drop_=_features_to_drop_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit=_fit_docstring,
    transform=_transform_docstring,
    fit_transform=_fit_transform_docstring,
    get_support=_get_support_docstring,
)
class SelectByShuffling(BaseSelector):

    def __init__(
        self,
        estimator,
        scoring: str = "roc_auc",
        cv=3,
        threshold: Union[float, int, None] = None,
        variables: Variables = None,
        random_state: Union[int, None] = None,
        confirm_variables: bool = False,
    ):

        if threshold and not isinstance(threshold, (int, float)):
            raise ValueError("threshold can only be integer or float or None")

        super().__init__(confirm_variables)

        self.variables = _check_variables_input_value(variables)
        self.estimator = estimator
        self.scoring = scoring
        self.threshold = threshold
        self.cv = cv
        self.random_state = random_state

    def fit(
        self,
        X: pd.DataFrame,
        y: pd.Series,
        sample_weight: Union[MutableSequence, None] = None,
    ):

        X, y = check_X_y(X, y)

        X = X.reset_index(drop=True)
        y = y.reset_index(drop=True)

        if sample_weight is not None:
            sample_weight = _check_sample_weight(sample_weight, X)

        self.variables_ = _select_numerical_variables(
            X, self.variables, self.confirm_variables
        )

        self._check_variable_number()

        cv = list(self.cv) if isinstance(self.cv, GeneratorType) else self.cv

        model = cross_validate(
            estimator=self.estimator,
            X=X[self.variables_],
            y=y,
            cv=cv,
            return_estimator=True,
            scoring=self.scoring,
            params={"sample_weight": sample_weight},
        )

        self.initial_model_performance_ = model["test_score"].mean()

        cv_ = check_cv(cv, y=y, classifier=is_classifier(self.estimator))
        validation_indices = [val_index for _, val_index in cv_.split(X, y)]

        scorer = get_scorer(self.scoring)

        random_state = check_random_state(self.random_state)

        self.performance_drifts_ = {}
        self.performance_drifts_std_ = {}

        for feature in self.variables_:

            X_shuffled = X[self.variables_].copy()

            X_shuffled[feature] = (
                X_shuffled[feature]
                .sample(frac=1, random_state=random_state)
                .reset_index(drop=True)
            )

            performance = [
                scorer(m, X_shuffled.iloc[idx], y.iloc[idx])
                for m, idx in zip(model["estimator"], validation_indices)
            ]

            performance_std = np.std(performance)
            performance = np.mean(performance)

            performance_drift = self.initial_model_performance_ - performance

            self.performance_drifts_[feature] = performance_drift
            self.performance_drifts_std_[feature] = performance_std

        if not self.threshold:
            threshold = pd.Series(self.performance_drifts_).mean()
        else:
            threshold = self.threshold

        self.features_to_drop_ = [
            f
            for f in self.performance_drifts_.keys()
            if self.performance_drifts_[f] < threshold
        ]

        self._get_feature_names_in(X)

        return self

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["variables"] = "numerical"
        tags_dict["requires_y"] = True
        tags_dict["_xfail_checks"]["check_estimators_nan_inf"] = "transformer allows NA"
        tags_dict["_xfail_checks"][
            "check_parameters_default_constructible"
        ] = "transformer has 1 mandatory parameter"

        msg = "transformers need more than 1 feature to work"
        tags_dict["_xfail_checks"]["check_fit2d_1feature"] = msg

        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        return tags

================================================
FILE: feature_engine/selection/single_feature_performance.py
================================================
import warnings
from typing import List, Union

import pandas as pd

from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
)
from feature_engine._docstrings.init_parameters.selection import (
    _confirm_variables_docstring,
)
from feature_engine._docstrings.methods import _fit_transform_docstring
from feature_engine._docstrings.selection._docstring import (
    _cv_docstring,
    _groups_docstring,
    _estimator_docstring,
    _features_to_drop_docstring,
    _fit_docstring,
    _get_support_docstring,
    _initial_model_performance_docstring,
    _scoring_docstring,
    _threshold_docstring,
    _transform_docstring,
    _variables_attribute_docstring,
    _variables_numerical_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import check_X_y
from feature_engine.selection.base_selector import BaseSelector
from feature_engine.tags import _return_tags

from .base_selection_functions import (
    _select_numerical_variables,
    single_feature_performance,
)

Variables = Union[None, int, str, List[Union[str, int]]]

@Substitution(
    estimator=_estimator_docstring,
    scoring=_scoring_docstring,
    threshold=_threshold_docstring,
    cv=_cv_docstring,
    groups=_groups_docstring,
    variables=_variables_numerical_docstring,
    confirm_variables=_confirm_variables_docstring,
    initial_model_performance_=_initial_model_performance_docstring,
    features_to_drop_=_features_to_drop_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit=_fit_docstring,
    transform=_transform_docstring,
    fit_transform=_fit_transform_docstring,
    get_support=_get_support_docstring,
)
class SelectBySingleFeaturePerformance(BaseSelector):

    def __init__(
        self,
        estimator,
        scoring: str = "roc_auc",
        cv=3,
        groups=None,
        threshold: Union[int, float, None] = None,
        variables: Variables = None,
        confirm_variables: bool = False,
    ):

        if threshold:
            if not isinstance(threshold, (int, float)):
                raise ValueError(
                    "`threshold` can only be integer, float or None. "
                    f"Got {threshold} instead."
                )

            if scoring == "roc_auc" and (threshold < 0.5 or threshold > 1):
                raise ValueError(
                    "`threshold` for roc-auc score should be between 0.5 and 1. "
                    f"Got {threshold} instead."
                )

            if scoring == "r2" and (threshold < 0 or threshold > 1):
                raise ValueError(
                    "`threshold` for r2 score should be between 0 and 1. "
                    f"Got {threshold} instead."
                )

        super().__init__(confirm_variables)
        self.variables = _check_variables_input_value(variables)
        self.estimator = estimator
        self.scoring = scoring
        self.threshold = threshold
        self.cv = cv
        self.groups = groups

    def fit(self, X: pd.DataFrame, y: pd.Series):

        X, y = check_X_y(X, y)

        self.variables_ = _select_numerical_variables(
            X, self.variables, self.confirm_variables
        )

        if len(self.variables_) == 1 and self.threshold is None:
            raise ValueError(
                "When evaluating a single feature you need to manually set a value "
                "for the threshold. "
                f"The transformer is evaluating the performance of {self.variables_} "
                f"and the threshold was left to {self.threshold} when initializing "
                f"the transformer."
            )

        self.feature_performance_, self.feature_performance_std_ = (
            single_feature_performance(
                X=X,
                y=y,
                variables=self.variables_,
                estimator=self.estimator,
                cv=self.cv,
                groups=self.groups,
                scoring=self.scoring,
            )
        )

        if not self.threshold:
            threshold = pd.Series(self.feature_performance_).mean()
        else:
            threshold = self.threshold

        self.features_to_drop_ = [
            f
            for f in self.feature_performance_.keys()
            if self.feature_performance_[f] < threshold
        ]

        if len(self.features_to_drop_) == len(X.columns):
            warnings.warn("All features will be dropped, try changing the threshold.")

        self._get_feature_names_in(X)

        return self

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["variables"] = "numerical"
        tags_dict["requires_y"] = True
        tags_dict["_xfail_checks"][
            "check_parameters_default_constructible"
        ] = "transformer has 1 mandatory parameter"
        tags_dict["_xfail_checks"]["check_estimators_nan_inf"] = "transformer allows NA"

        msg = "transformers need more than 1 feature to work"
        tags_dict["_xfail_checks"]["check_fit2d_1feature"] = msg

        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        return tags

================================================
FILE: feature_engine/selection/smart_correlation_selection.py
================================================
from types import GeneratorType
from typing import List, Union

import pandas as pd

from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
)
from feature_engine._docstrings.init_parameters.selection import (
    _confirm_variables_docstring,
)
from feature_engine._docstrings.methods import _fit_transform_docstring
from feature_engine._docstrings.selection._docstring import (
    _cv_docstring,
    _groups_docstring,
    _estimator_docstring,
    _get_support_docstring,
    _missing_values_docstring,
    _scoring_docstring,
    _variables_attribute_docstring,
    _variables_numerical_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import (
    _check_contains_inf,
    _check_contains_na,
    check_X,
    check_y,
)
from feature_engine.selection.base_selector import BaseSelector

from .base_selection_functions import (
    _select_numerical_variables,
    find_correlated_features,
    single_feature_performance,
)

Variables = Union[None, int, str, List[Union[str, int]]]

@Substitution(
    estimator=_estimator_docstring,
    scoring=_scoring_docstring,
    cv=_cv_docstring,
    groups=_groups_docstring,
    confirm_variables=_confirm_variables_docstring,
    variables=_variables_numerical_docstring,
    missing_values=_missing_values_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit_transform=_fit_transform_docstring,
    get_support=_get_support_docstring,
)
class SmartCorrelatedSelection(BaseSelector):

    def __init__(
        self,
        variables: Variables = None,
        method: str = "pearson",
        threshold: float = 0.8,
        missing_values: str = "ignore",
        selection_method: str = "missing_values",
        estimator=None,
        scoring: str = "roc_auc",
        cv=3,
        groups=None,
        confirm_variables: bool = False,
    ):
        if not isinstance(threshold, float) or threshold < 0 or threshold > 1:
            raise ValueError(
                f"`threshold` must be a float between 0 and 1. Got {threshold} instead."
            )

        if missing_values not in ["raise", "ignore"]:
            raise ValueError(
                "missing_values takes only values 'raise' or 'ignore'. "
                f"Got {missing_values} instead."
            )

        if selection_method not in [
            "missing_values",
            "cardinality",
            "variance",
            "model_performance",
            "corr_with_target",
        ]:
            raise ValueError(
                "selection_method takes only values 'missing_values', 'cardinality', "
                "'variance', 'model_performance' or 'corr_with_target'. "
                f"Got {selection_method} instead."
            )

        if selection_method == "model_performance" and estimator is None:
            raise ValueError(
                "Please provide an estimator, e.g., "
                "RandomForestClassifier or select another "
                "selection_method."
            )

        if selection_method == "missing_values" and missing_values == "raise":
            raise ValueError(
                "When `selection_method = 'missing_values'`, you need to set "
                f"`missing_values` to `'ignore'`. Got {missing_values} instead."
            )

        super().__init__(confirm_variables)

        self.variables = _check_variables_input_value(variables)
        self.method = method
        self.threshold = threshold
        self.missing_values = missing_values
        self.selection_method = selection_method
        self.estimator = estimator
        self.scoring = scoring
        self.cv = cv
        self.groups = groups

    def fit(self, X: pd.DataFrame, y: pd.Series = None):

        X = check_X(X)

        self.variables_ = _select_numerical_variables(
            X, self.variables, self.confirm_variables
        )

        self._check_variable_number()

        if self.missing_values == "raise":
            _check_contains_na(X, self.variables_)
            _check_contains_inf(X, self.variables_)

        if (
            self.selection_method in ["model_performance", "corr_with_target"]
        ) and y is None:
            raise ValueError(
                f"When `selection_method = '{self.selection_method}'` y is needed to "
                "fit the transformer."
            )

        if self.selection_method == "missing_values":
            features = (
                X[self.variables_]
                .isnull()
                .sum()
                .sort_values(ascending=True)
                .index.to_list()
            )
        elif self.selection_method == "variance":
            features = (
                X[self.variables_].std().sort_values(ascending=False).index.to_list()
            )
        elif self.selection_method == "cardinality":
            features = (
                X[self.variables_]
                .nunique()
                .sort_values(ascending=False)
                .index.to_list()
            )
        elif self.selection_method == "corr_with_target":
            y = check_y(y)
            features = (
                X[self.variables_]
                .corrwith(y, method=self.method)
                .abs()
                .sort_values(ascending=False)
                .index.to_list()
            )
        else:
            features = sorted(self.variables_)

        correlated_groups, features_to_drop, correlated_dict = find_correlated_features(
            X, features, self.method, self.threshold
        )

        if self.selection_method == "model_performance":
            correlated_dict = dict()
            cv = list(self.cv) if isinstance(self.cv, GeneratorType) else self.cv
            for feature_group in correlated_groups:
                feature_performance, _ = single_feature_performance(
                    X=X,
                    y=y,
                    variables=feature_group,
                    estimator=self.estimator,
                    cv=cv,
                    groups=self.groups,
                    scoring=self.scoring,
                )
                f_i = (
                    pd.Series(feature_performance).sort_values(ascending=False).index[0]
                )
                correlated_dict[f_i] = feature_group.difference({f_i})

            features_to_drop = [
                variable
                for set_ in correlated_dict.values()
                for variable in sorted(set_)
            ]

        self.features_to_drop_ = features_to_drop
        self.correlated_feature_sets_ = correlated_groups
        self.correlated_feature_dict_ = correlated_dict

        self._get_feature_names_in(X)

        return self

================================================
FILE: feature_engine/selection/target_mean_selection.py
================================================
from types import GeneratorType
from typing import List, Union

import pandas as pd
from sklearn.base import clone
from sklearn.model_selection import cross_validate

from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
)
from feature_engine._docstrings.init_parameters.selection import (
    _confirm_variables_docstring,
)
from feature_engine._docstrings.methods import _fit_transform_docstring
from feature_engine._docstrings.selection._docstring import (
    _cv_docstring,
    _features_to_drop_docstring,
    _fit_docstring,
    _get_support_docstring,
    _groups_docstring,
    _scoring_docstring,
    _threshold_docstring,
    _transform_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine._prediction.target_mean_classifier import TargetMeanClassifier
from feature_engine._prediction.target_mean_regressor import TargetMeanRegressor
from feature_engine.dataframe_checks import check_X_y
from feature_engine.selection._selection_constants import (
    _CLASSIFICATION_METRICS,
    _REGRESSION_METRICS,
)
from feature_engine.selection.base_selector import BaseSelector
from feature_engine.tags import _return_tags

from .base_selection_functions import _select_all_variables

Variables = Union[None, int, str, List[Union[str, int]]]

@Substitution(
    scoring=_scoring_docstring,
    threshold=_threshold_docstring,
    cv=_cv_docstring,
    groups=_groups_docstring,
    confirm_variables=_confirm_variables_docstring,
    features_to_drop_=_features_to_drop_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit=_fit_docstring,
    transform=_transform_docstring,
    fit_transform=_fit_transform_docstring,
    get_support=_get_support_docstring,
)
class SelectByTargetMeanPerformance(BaseSelector):

    def __init__(
        self,
        variables: Variables = None,
        bins: int = 5,
        strategy: str = "equal_width",
        scoring: str = "roc_auc",
        cv=3,
        groups=None,
        threshold: Union[int, float, None] = None,
        regression: bool = False,
        confirm_variables: bool = False,
    ):

        if not isinstance(bins, int):
            raise ValueError(f"bins must be an integer. Got {bins} instead.")

        if strategy not in ["equal_width", "equal_frequency"]:
            raise ValueError(
                "strategy takes only values 'equal_width' or 'equal_frequency'. "
                f"Got {strategy} instead."
            )

        if threshold is not None and not isinstance(threshold, (int, float)):
            raise ValueError(
                "threshold can only take integer or float. " f"Got {threshold} instead."
            )

        if regression is True and scoring not in _REGRESSION_METRICS:
            raise ValueError(
                f"The metric {scoring} is not suitable for regression. Set the "
                "parameter regression to False or choose a different performance "
                "metric."
            )

        if regression is False and scoring not in _CLASSIFICATION_METRICS:
            raise ValueError(
                f"The metric {scoring} is not suitable for classification. Set the"
                "parameter regression to True or choose a different performance "
                "metric."
            )

        super().__init__(confirm_variables)
        self.variables = _check_variables_input_value(variables)
        self.bins = bins
        self.strategy = strategy
        self.scoring = scoring
        self.cv = cv
        self.groups = groups
        self.threshold = threshold
        self.regression = regression

    def fit(self, X: pd.DataFrame, y: pd.Series):
        X, y = check_X_y(X, y)

        self.variables_ = _select_all_variables(
            X, self.variables, self.confirm_variables, exclude_datetime=True
        )

        if len(self.variables_) == 1 and self.threshold is None:
            raise ValueError(
                "When evaluating a single feature you need to manually set a value "
                "for the threshold. "
                f"The transformer is evaluating the performance of {self.variables_} "
                f"and the threshold was left to {self.threshold} when initializing "
                f"the transformer."
            )

        self._get_feature_names_in(X)

        if self.regression is True:
            est = TargetMeanRegressor(
                bins=self.bins,
                strategy=self.strategy,
            )
        else:
            est = TargetMeanClassifier(
                bins=self.bins,
                strategy=self.strategy,
            )

        self.feature_performance_ = {}
        self.feature_performance_std_ = {}

        cv = list(self.cv) if isinstance(self.cv, GeneratorType) else self.cv

        for variable in self.variables_:
            estimator = clone(est)

            estimator.set_params(variables=variable)

            model = cross_validate(
                estimator=estimator,
                X=X,
                y=y,
                cv=cv,
                groups=self.groups,
                scoring=self.scoring,
            )

            self.feature_performance_[variable] = model["test_score"].mean()
            self.feature_performance_std_[variable] = model["test_score"].std()

        if not self.threshold:
            threshold = pd.Series(self.feature_performance_).mean()
        else:
            threshold = self.threshold

        self.features_to_drop_ = [
            f for f in self.variables_ if self.feature_performance_[f] < threshold
        ]

        return self

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["variables"] = "all"
        tags_dict["requires_y"] = True
        tags_dict["binary_only"] = True
        tags_dict["_xfail_checks"]["check_estimators_nan_inf"] = "transformer allows NA"
        msg = "transformers need more than 1 feature to work"
        tags_dict["_xfail_checks"]["check_fit2d_1feature"] = msg
        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        return tags

================================================
FILE: feature_engine/timeseries/__init__.py
================================================
[Empty file]

================================================
FILE: feature_engine/timeseries/forecasting/__init__.py
================================================

from .expanding_window_features import ExpandingWindowFeatures
from .lag_features import LagFeatures
from .window_features import WindowFeatures

__all__ = ["LagFeatures", "WindowFeatures", "ExpandingWindowFeatures"]

================================================
FILE: feature_engine/timeseries/forecasting/base_forecast_transformers.py
================================================
from typing import List, Optional, Union

import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.utils.validation import check_is_fitted

from feature_engine._base_transformers.mixins import (
    GetFeatureNamesOutMixin,
    TransformXyMixin,
)
from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _drop_original_docstring,
    _missing_values_docstring,
)
from feature_engine._docstrings.methods import _fit_not_learn_docstring
from feature_engine._docstrings.substitute import Substitution
from feature_engine.dataframe_checks import (
    _check_contains_inf,
    _check_contains_na,
    _check_X_matches_training_df,
    check_X,
)
from feature_engine.tags import _return_tags
from feature_engine.variable_handling import (
    check_numerical_variables,
    find_numerical_variables,
)

@Substitution(
    missing_values=_missing_values_docstring,
    drop_original=_drop_original_docstring,
    feature_names_in_=_feature_names_in_docstring,
    fit=_fit_not_learn_docstring,
    n_features_in_=_n_features_in_docstring,
)
class BaseForecastTransformer(
    TransformerMixin, BaseEstimator, GetFeatureNamesOutMixin, TransformXyMixin
):

    def __init__(
        self,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        missing_values: str = "raise",
        drop_original: bool = False,
        drop_na: bool = False,
    ) -> None:

        if missing_values not in ["raise", "ignore"]:
            raise ValueError(
                "missing_values takes only values 'raise' or 'ignore'. "
                f"Got {missing_values} instead."
            )

        if not isinstance(drop_original, bool):
            raise ValueError(
                "drop_original takes only boolean values True and False. "
                f"Got {drop_original} instead."
            )

        if not isinstance(drop_na, bool):
            raise ValueError(
                "drop_na takes only boolean values True and False. "
                f"Got {drop_na} instead."
            )

        self.variables = _check_variables_input_value(variables)
        self.missing_values = missing_values
        self.drop_original = drop_original
        self.drop_na = drop_na

    def _check_index(self, X: pd.DataFrame):
        if X.index.isnull().any():
            raise NotImplementedError(
                "The dataframe's index contains NaN values or missing data. "
                "Only dataframes with complete indexes are compatible with "
                "this transformer."
            )

        if X.index.is_unique is False:
            raise NotImplementedError(
                "The dataframe's index does not contain unique values. "
                "Only dataframes with unique values in the index are "
                "compatible with this transformer."
            )

        return self

    def _check_na_and_inf(self, X: pd.DataFrame):
        _check_contains_na(X, self.variables_)
        _check_contains_inf(X, self.variables_)

        return self

    def _get_feature_names_in(self, X: pd.DataFrame):

        self.feature_names_in_ = X.columns.tolist()
        self.n_features_in_ = X.shape[1]

        return self

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):
        X = check_X(X)

        self._check_index(X)

        if self.variables is None:
            self.variables_ = find_numerical_variables(X)
        else:
            self.variables_ = check_numerical_variables(X, self.variables)

        if self.missing_values == "raise":
            self._check_na_and_inf(X)

        self._get_feature_names_in(X)

        return self

    def _check_transform_input_and_state(self, X: pd.DataFrame) -> pd.DataFrame:
        check_is_fitted(self)

        X = check_X(X)

        _check_X_matches_training_df(X, self.n_features_in_)

        self._check_index(X)

        if self.missing_values == "raise":
            self._check_na_and_inf(X)

        X = X[self.feature_names_in_]

        if self.sort_index is True:
            X.sort_index(inplace=True)

        return X

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["allow_nan"] = True
        tags_dict["variables"] = "numerical"
        tags_dict["_xfail_checks"][
            "check_methods_subset_invariance"
        ] = "LagFeatures is not invariant when applied to a subset. Not sure why yet"
        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        return tags

================================================
FILE: feature_engine/timeseries/forecasting/expanding_window_features.py
================================================

from __future__ import annotations

from typing import List

import pandas as pd

from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _drop_original_docstring,
    _missing_values_docstring,
    _variables_numerical_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_not_learn_docstring,
    _fit_transform_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.timeseries.forecasting.base_forecast_transformers import (
    BaseForecastTransformer,
)

@Substitution(
    variables=_variables_numerical_docstring,
    missing_values=_missing_values_docstring,
    drop_original=_drop_original_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit=_fit_not_learn_docstring,
    fit_transform=_fit_transform_docstring,
)
class ExpandingWindowFeatures(BaseForecastTransformer):

    def __init__(
        self,
        variables: None | int | str | list[str | int] = None,
        min_periods: int | None = None,
        functions: str | list[str] = "mean",
        periods: int = 1,
        freq: str | None = None,
        sort_index: bool = True,
        missing_values: str = "raise",
        drop_original: bool = False,
        drop_na: bool = False,
    ) -> None:

        if not isinstance(functions, (str, list)) or not all(
            isinstance(val, str) for val in functions
        ):
            raise ValueError(
                f"functions must be a list of strings or a string."
                f"Got {functions} instead."
            )
        if isinstance(functions, list) and len(functions) != len(set(functions)):
            raise ValueError(f"There are duplicated functions in the list: {functions}")

        if not isinstance(periods, int) or periods < 0:
            raise ValueError(
                f"periods must be a non-negative integer. Got {periods} instead."
            )

        super().__init__(variables, missing_values, drop_original, drop_na)

        self.min_periods = min_periods
        self.functions = functions
        self.periods = periods
        self.freq = freq
        self.sort_index = sort_index

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X = self._check_transform_input_and_state(X)

        tmp = (
            X[self.variables_]
            .expanding(min_periods=self.min_periods)
            .agg(self.functions)
            .shift(periods=self.periods, freq=self.freq)
        )

        tmp.columns = self._get_new_features_name()

        X = X.merge(tmp, left_index=True, right_index=True, how="left")

        if self.drop_original:
            X = X.drop(self.variables_, axis=1)

        if self.drop_na:
            X = X.dropna(subset=tmp.columns, axis=0)

        return X

    def _get_new_features_name(self) -> List:

        if not isinstance(self.functions, list):
            functions_ = [self.functions]
        else:
            functions_ = self.functions

        feature_names = [
            f"{feature}_expanding_{agg}"
            for feature in self.variables_
            for agg in functions_
        ]

        return feature_names

================================================
FILE: feature_engine/timeseries/forecasting/lag_features.py
================================================

from collections.abc import Hashable
from typing import List, Union

import pandas as pd

from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _drop_original_docstring,
    _missing_values_docstring,
    _variables_numerical_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_not_learn_docstring,
    _fit_transform_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.timeseries.forecasting.base_forecast_transformers import (
    BaseForecastTransformer,
)

@Substitution(
    variables=_variables_numerical_docstring,
    missing_values=_missing_values_docstring,
    drop_original=_drop_original_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit=_fit_not_learn_docstring,
    fit_transform=_fit_transform_docstring,
)
class LagFeatures(BaseForecastTransformer):

    def __init__(
        self,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        periods: Union[int, List[int]] = 1,
        freq: Union[str, List[str], None] = None,
        fill_value: Hashable = None,
        sort_index: bool = True,
        missing_values: str = "raise",
        drop_original: bool = False,
        drop_na: bool = False,
    ) -> None:

        if not (
            isinstance(periods, int)
            and periods > 0
            or isinstance(periods, list)
            and all(isinstance(num, int) and num > 0 for num in periods)
        ):

            raise ValueError(
                "periods must be an integer or a list of positive integers. "
                f"Got {periods} instead."
            )
        if isinstance(periods, list) and len(periods) != len(set(periods)):
            raise ValueError(f"There are duplicated periods in the list: {periods}")

        if isinstance(freq, list) and len(freq) != len(set(freq)):
            raise ValueError(f"There are duplicated freq values in the list: {freq}")

        if not isinstance(sort_index, bool):
            raise ValueError(
                "sort_index takes values True and False." f"Got {sort_index} instead."
            )

        super().__init__(variables, missing_values, drop_original, drop_na)

        self.periods = periods
        self.freq = freq
        self.fill_value = fill_value
        self.sort_index = sort_index

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X = self._check_transform_input_and_state(X)

        if self.freq is not None:

            if isinstance(self.freq, list):
                df_ls = []
                for fr in self.freq:
                    tmp = X[self.variables_].shift(
                        freq=fr,
                        axis=0,
                    )
                    df_ls.append(tmp)
                tmp = pd.concat(df_ls, axis=1)

            else:
                tmp = X[self.variables_].shift(
                    freq=self.freq,
                    axis=0,
                )

        else:
            if isinstance(self.periods, list):
                df_ls = []
                for pr in self.periods:
                    tmp = X[self.variables_].shift(
                        periods=pr,
                        fill_value=self.fill_value,
                        axis=0,
                    )
                    df_ls.append(tmp)
                tmp = pd.concat(df_ls, axis=1)

            else:
                tmp = X[self.variables_].shift(
                    periods=self.periods,
                    fill_value=self.fill_value,
                    axis=0,
                )

        tmp.columns = self._get_new_features_name()

        X = X.merge(tmp, left_index=True, right_index=True, how="left")

        if self.freq is not None and self.fill_value is not None:
            lags = [x for x in tmp.columns if x not in self.feature_names_in_]
            X[lags] = X[lags].fillna(value=self.fill_value)

        if self.drop_original:
            X = X.drop(self.variables_, axis=1)

        if self.drop_na:
            X = X.dropna(subset=tmp.columns, axis=0)

        return X

    def _get_new_features_name(self) -> List:

        if isinstance(self.freq, list):
            feature_names = [
                f"{feature}_lag_{fr}" for fr in self.freq for feature in self.variables_
            ]
        elif self.freq is not None:
            feature_names = [
                f"{feature}_lag_{self.freq}" for feature in self.variables_
            ]
        elif isinstance(self.periods, list):
            feature_names = [
                f"{feature}_lag_{pr}"
                for pr in self.periods
                for feature in self.variables_
            ]
        else:
            feature_names = [
                f"{feature}_lag_{self.periods}" for feature in self.variables_
            ]

        return feature_names

================================================
FILE: feature_engine/timeseries/forecasting/window_features.py
================================================
from typing import Callable, List, Union

import pandas as pd

from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _drop_original_docstring,
    _missing_values_docstring,
    _variables_numerical_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_not_learn_docstring,
    _fit_transform_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.timeseries.forecasting.base_forecast_transformers import (
    BaseForecastTransformer,
)

@Substitution(
    variables=_variables_numerical_docstring,
    missing_values=_missing_values_docstring,
    drop_original=_drop_original_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit=_fit_not_learn_docstring,
    fit_transform=_fit_transform_docstring,
)
class WindowFeatures(BaseForecastTransformer):

    def __init__(
        self,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        window: Union[str, int, Callable, List[int], List[str]] = 3,
        min_periods: Union[int, None] = None,
        functions: Union[str, List[str]] = "mean",
        periods: int = 1,
        freq: Union[str, None] = None,
        sort_index: bool = True,
        missing_values: str = "raise",
        drop_original: bool = False,
        drop_na: bool = False,
    ) -> None:

        if isinstance(window, list) and len(window) != len(set(window)):
            raise ValueError(f"There are duplicated windows in the list: {window}")

        if not isinstance(functions, (str, list)) or not all(
            isinstance(val, str) for val in functions
        ):
            raise ValueError(
                f"functions must be a string or a list of strings. "
                f"Got {functions} instead."
            )
        if isinstance(functions, list) and len(functions) != len(set(functions)):
            raise ValueError(f"There are duplicated functions in the list: {functions}")

        if not isinstance(periods, int) or periods < 1:
            raise ValueError(
                f"periods must be a positive integer. Got {periods} instead."
            )

        super().__init__(variables, missing_values, drop_original, drop_na)

        self.window = window
        self.min_periods = min_periods
        self.functions = functions
        self.periods = periods
        self.freq = freq
        self.sort_index = sort_index

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X = self._check_transform_input_and_state(X)

        if isinstance(self.window, list):
            df_ls = []
            for win in self.window:
                tmp = (
                    X[self.variables_]
                    .rolling(window=win)
                    .agg(self.functions)
                    .shift(periods=self.periods, freq=self.freq)
                )
                df_ls.append(tmp)
            tmp = pd.concat(df_ls, axis=1)

        else:
            tmp = (
                X[self.variables_]
                .rolling(window=self.window)
                .agg(self.functions)
                .shift(periods=self.periods, freq=self.freq)
            )

        tmp.columns = self._get_new_features_name()

        X = X.merge(tmp, left_index=True, right_index=True, how="left")

        if self.drop_original:
            X = X.drop(self.variables_, axis=1)

        if self.drop_na:
            X = X.dropna(subset=tmp.columns, axis=0)

        return X

    def _get_new_features_name(self) -> List:

        if not isinstance(self.functions, list):
            functions_ = [self.functions]
        else:
            functions_ = self.functions

        if isinstance(self.window, list):
            feature_names = [
                f"{feature}_window_{win}_{agg}"
                for win in self.window
                for feature in self.variables_
                for agg in functions_
            ]
        else:
            feature_names = [
                f"{feature}_window_{self.window}_{agg}"
                for feature in self.variables_
                for agg in functions_
            ]

        return feature_names

================================================
FILE: feature_engine/transformation/__init__.py
================================================

from .arcsin import ArcsinTransformer
from .boxcox import BoxCoxTransformer
from .log import LogCpTransformer, LogTransformer
from .power import PowerTransformer
from .reciprocal import ReciprocalTransformer
from .yeojohnson import YeoJohnsonTransformer

__all__ = [
    "BoxCoxTransformer",
    "LogTransformer",
    "LogCpTransformer",
    "PowerTransformer",
    "ReciprocalTransformer",
    "YeoJohnsonTransformer",
    "ArcsinTransformer",
]

================================================
FILE: feature_engine/transformation/arcsin.py
================================================

from typing import List, Optional, Union

import numpy as np
import pandas as pd

from feature_engine._base_transformers.base_numerical import BaseNumericalTransformer
from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _variables_numerical_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_not_learn_docstring,
    _fit_transform_docstring,
    _inverse_transform_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.tags import _return_tags

@Substitution(
    variables=_variables_numerical_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit=_fit_not_learn_docstring,
    fit_transform=_fit_transform_docstring,
    inverse_transform=_inverse_transform_docstring,
)
class ArcsinTransformer(BaseNumericalTransformer):

    def __init__(
        self, variables: Union[None, int, str, List[Union[str, int]]] = None
    ) -> None:

        self.variables = _check_variables_input_value(variables)

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):

        X = super().fit(X)

        if ((X[self.variables_] < 0) | (X[self.variables_] > 1)).any().any():
            raise ValueError(
                "Some variables contain values outside the possible range 0-1. "
                "Can't apply the arcsin transformation. "
            )

        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:

        X = self._check_transform_input_and_state(X)

        if ((X[self.variables_] < 0) | (X[self.variables_] > 1)).any().any():
            raise ValueError(
                "Some variables contain values outside the possible range 0-1. "
                "Can't apply the arcsin transformation."
            )

        X.loc[:, self.variables_] = np.arcsin(np.sqrt(X.loc[:, self.variables_]))

        return X

    def inverse_transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X.loc[:, self.variables_] = (np.sin(X.loc[:, self.variables_])) ** 2

        return X

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["variables"] = "numerical"
        msg = (
            "transformers raise errors when data is outside [0, 1] range, thus this"
            "check fails"
        )
        tags_dict["_xfail_checks"]["check_estimators_dtypes"] = msg
        tags_dict["_xfail_checks"]["check_estimators_fit_returns_self"] = msg
        tags_dict["_xfail_checks"]["check_pipeline_consistency"] = msg
        tags_dict["_xfail_checks"]["check_estimators_overwrite_params"] = msg
        tags_dict["_xfail_checks"]["check_estimators_pickle"] = msg
        tags_dict["_xfail_checks"]["check_transformer_general"] = msg
        tags_dict["_xfail_checks"]["check_methods_subset_invariance"] = msg
        tags_dict["_xfail_checks"]["check_fit2d_1sample"] = msg
        tags_dict["_xfail_checks"]["check_fit2d_1feature"] = msg
        tags_dict["_xfail_checks"]["check_dict_unchanged"] = msg
        tags_dict["_xfail_checks"]["check_dont_overwrite_parameters"] = msg
        tags_dict["_xfail_checks"]["check_fit_check_is_fitted"] = msg
        tags_dict["_xfail_checks"]["check_n_features_in"] = msg

        return tags_dict

    def __sklearn_tags__(self):
        return super().__sklearn_tags__()

================================================
FILE: feature_engine/transformation/boxcox.py
================================================

from typing import List, Optional, Union

import pandas as pd
import scipy.stats as stats
from scipy.special import inv_boxcox

from feature_engine._base_transformers.base_numerical import BaseNumericalTransformer
from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _variables_numerical_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_transform_docstring,
    _inverse_transform_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.tags import _return_tags

@Substitution(
    variables=_variables_numerical_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit_transform=_fit_transform_docstring,
    inverse_transform=_inverse_transform_docstring,
)
class BoxCoxTransformer(BaseNumericalTransformer):

    def __init__(
        self, variables: Union[None, int, str, List[Union[str, int]]] = None
    ) -> None:

        self.variables = _check_variables_input_value(variables)

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):

        X = super().fit(X)

        self.lambda_dict_ = {}

        for var in self.variables_:
            _, self.lambda_dict_[var] = stats.boxcox(X[var])

        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:

        X = self._check_transform_input_and_state(X)

        if (X[self.variables_] <= 0).any().any():
            raise ValueError("Data must be positive.")

        for feature in self.variables_:
            X[feature] = stats.boxcox(X[feature], lmbda=self.lambda_dict_[feature])

        return X

    def inverse_transform(self, X: pd.DataFrame) -> pd.DataFrame:

        X = self._check_transform_input_and_state(X)

        for feature in self.variables_:
            X[feature] = inv_boxcox(X[feature], self.lambda_dict_[feature])

        return X

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["variables"] = "numerical"
        msg = (
            "transformers raise errors when data contains zeroes, thus this check fails"
        )
        tags_dict["_xfail_checks"]["check_estimators_dtypes"] = msg
        tags_dict["_xfail_checks"]["check_estimators_fit_returns_self"] = msg
        tags_dict["_xfail_checks"]["check_pipeline_consistency"] = msg
        tags_dict["_xfail_checks"]["check_estimators_overwrite_params"] = msg
        tags_dict["_xfail_checks"]["check_estimators_pickle"] = msg
        tags_dict["_xfail_checks"]["check_transformer_general"] = msg

        msg = "scipy.stats.boxcox does not like the input data"
        tags_dict["_xfail_checks"]["check_methods_subset_invariance"] = msg
        tags_dict["_xfail_checks"]["check_fit2d_1sample"] = msg

        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        return tags

================================================
FILE: feature_engine/transformation/log.py
================================================

from typing import Dict, List, Optional, Union

import numpy as np
import pandas as pd

from feature_engine._base_transformers.base_numerical import BaseNumericalTransformer
from feature_engine._base_transformers.mixins import FitFromDictMixin
from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _variables_numerical_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_not_learn_docstring,
    _fit_transform_docstring,
    _inverse_transform_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.tags import _return_tags

@Substitution(
    variables=_variables_numerical_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit=_fit_not_learn_docstring,
    fit_transform=_fit_transform_docstring,
    inverse_transform=_inverse_transform_docstring,
)
class LogTransformer(BaseNumericalTransformer):

    def __init__(
        self,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        base: str = "e",
    ) -> None:

        if base not in ["e", "10"]:
            raise ValueError("base can take only '10' or 'e' as values")

        self.variables = _check_variables_input_value(variables)
        self.base = base

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):

        X = super().fit(X)

        if (X[self.variables_] <= 0).any().any():
            raise ValueError(
                "Some variables contain zero or negative values, can't apply log"
            )

        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:

        X = self._check_transform_input_and_state(X)

        if (X[self.variables_] <= 0).any().any():
            raise ValueError(
                "Some variables contain zero or negative values, can't apply log"
            )

        X[self.variables_] = X[self.variables_].astype(float)

        if self.base == "e":
            X.loc[:, self.variables_] = np.log(X.loc[:, self.variables_])
        elif self.base == "10":
            X.loc[:, self.variables_] = np.log10(X.loc[:, self.variables_])

        return X

    def inverse_transform(self, X: pd.DataFrame) -> pd.DataFrame:

        X = self._check_transform_input_and_state(X)

        if self.base == "e":
            X.loc[:, self.variables_] = np.exp(X.loc[:, self.variables_])
        elif self.base == "10":
            X.loc[:, self.variables_] = np.array(10 ** X.loc[:, self.variables_])

        return X

    def _more_tags(self):
        tags_dict = _return_tags()
        msg = (
            "transformers raise errors when data contains zeroes, thus this check fails"
        )
        tags_dict["_xfail_checks"]["check_estimators_dtypes"] = msg
        tags_dict["_xfail_checks"]["check_estimators_fit_returns_self"] = msg
        tags_dict["_xfail_checks"]["check_pipeline_consistency"] = msg
        tags_dict["_xfail_checks"]["check_estimators_overwrite_params"] = msg
        tags_dict["_xfail_checks"]["check_estimators_pickle"] = msg
        tags_dict["_xfail_checks"]["check_transformer_general"] = msg

        return tags_dict

    def __sklearn_tags__(self):
        tags = super().__sklearn_tags__()
        return tags

@Substitution(
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit_transform=_fit_transform_docstring,
    inverse_transform=_inverse_transform_docstring,
)
class LogCpTransformer(BaseNumericalTransformer, FitFromDictMixin):

    def __init__(
        self,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        base: str = "e",
        C: Union[int, float, str, Dict[Union[str, int], Union[float, int]]] = "auto",
    ) -> None:

        if base not in ["e", "10"]:
            raise ValueError(
                f"base can take only '10' or 'e' as values. Got {base} instead."
            )

        if not isinstance(C, (int, float, dict)) and not C == "auto":
            raise ValueError(
                f"C can take only 'auto', integers or floats. Got {C} instead."
            )

        self.variables = _check_variables_input_value(variables)
        self.base = base
        self.C = C

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):

        if isinstance(self.C, dict):
            X = super()._fit_from_dict(X, self.C)
        else:
            X = super().fit(X)

        self.C_ = self.C

        if self.C == "auto":
            c_dict = {var: 0 for var in self.variables_ if X[var].min() > 0}

            non_positive_vars = [
                var for var in self.variables_ if var not in c_dict.keys()
            ]
            c_dict.update(dict(X[non_positive_vars].min(axis=0).abs() + 1))
            self.C_ = c_dict

        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:

        X = self._check_transform_input_and_state(X)

        error_msg = (
            "Some variables contain zero or negative values after adding"
            + " constant C, can't apply log."
        )

        if (X[self.variables_] + self.C_ <= 0).any().any():
            raise ValueError(error_msg)

        X[self.variables_] = X[self.variables_].astype(float)

        if self.base == "e":
            X.loc[:, self.variables_] = np.log(X.loc[:, self.variables_] + self.C_)
        else:
            X.loc[:, self.variables_] = np.log10(X.loc[:, self.variables_] + self.C_)

        return X

    def inverse_transform(self, X: pd.DataFrame) -> pd.DataFrame:

        X = self._check_transform_input_and_state(X)

        if self.base == "e":
            X.loc[:, self.variables_] = np.exp(X.loc[:, self.variables_]) - self.C_
        else:
            X.loc[:, self.variables_] = 10 ** X.loc[:, self.variables_] - self.C_

        return X

================================================
FILE: feature_engine/transformation/power.py
================================================

from typing import List, Optional, Union

import numpy as np
import pandas as pd

from feature_engine._base_transformers.base_numerical import BaseNumericalTransformer
from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _variables_numerical_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_not_learn_docstring,
    _fit_transform_docstring,
    _inverse_transform_docstring,
)
from feature_engine._docstrings.substitute import Substitution

@Substitution(
    variables=_variables_numerical_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit=_fit_not_learn_docstring,
    fit_transform=_fit_transform_docstring,
    inverse_transform=_inverse_transform_docstring,
)
class PowerTransformer(BaseNumericalTransformer):

    def __init__(
        self,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
        exp: Union[float, int] = 0.5,
    ):

        if not isinstance(exp, (float, int)):
            raise ValueError("exp must be a float or an int")

        self.exp = exp
        self.variables = _check_variables_input_value(variables)

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):

        super().fit(X)

        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:

        X = self._check_transform_input_and_state(X)

        X[self.variables_] = X[self.variables_].astype(float)
        X.loc[:, self.variables_] = np.power(X.loc[:, self.variables_], self.exp)

        return X

    def inverse_transform(self, X: pd.DataFrame) -> pd.DataFrame:

        X = self._check_transform_input_and_state(X)

        X.loc[:, self.variables_] = np.power(X.loc[:, self.variables_], 1 / self.exp)

        return X

================================================
FILE: feature_engine/transformation/reciprocal.py
================================================

from typing import List, Optional, Union

import pandas as pd

from feature_engine._base_transformers.base_numerical import BaseNumericalTransformer
from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _variables_numerical_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_not_learn_docstring,
    _fit_transform_docstring,
    _inverse_transform_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.tags import _return_tags

@Substitution(
    variables=_variables_numerical_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit=_fit_not_learn_docstring,
    fit_transform=_fit_transform_docstring,
    inverse_transform=_inverse_transform_docstring,
)
class ReciprocalTransformer(BaseNumericalTransformer):

    def __init__(
        self, variables: Union[None, int, str, List[Union[str, int]]] = None
    ) -> None:
        self.variables = _check_variables_input_value(variables)

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):

        X = super().fit(X)

        if (X[self.variables_] == 0).any().any():
            raise ValueError(
                "Some variables contain the value zero, can't apply reciprocal "
                "transformation."
            )

        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:

        X = self._check_transform_input_and_state(X)

        if (X[self.variables_] == 0).any().any():
            raise ValueError(
                "Some variables contain the value zero, can't apply reciprocal "
                "transformation."
            )

        X[self.variables_] = X[self.variables_].astype(float)
        X.loc[:, self.variables_] = 1 / X.loc[:, self.variables_]

        return X

    def inverse_transform(self, X: pd.DataFrame) -> pd.DataFrame:
        return self.transform(X)

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["variables"] = "numerical"
        msg = (
            "transformers raise errors when data contains zeroes, thus this check fails"
        )
        tags_dict["_xfail_checks"]["check_estimators_dtypes"] = msg
        tags_dict["_xfail_checks"]["check_estimators_fit_returns_self"] = msg
        tags_dict["_xfail_checks"]["check_pipeline_consistency"] = msg
        tags_dict["_xfail_checks"]["check_estimators_overwrite_params"] = msg
        tags_dict["_xfail_checks"]["check_estimators_pickle"] = msg
        tags_dict["_xfail_checks"]["check_transformer_general"] = msg

        return tags_dict

    def __sklearn_tags__(self):
        return super().__sklearn_tags__()

================================================
FILE: feature_engine/transformation/yeojohnson.py
================================================

from typing import List, Optional, Union

import numpy as np
import pandas as pd
import scipy.stats as stats

from feature_engine._base_transformers.base_numerical import BaseNumericalTransformer
from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine._docstrings.fit_attributes import (
    _feature_names_in_docstring,
    _n_features_in_docstring,
    _variables_attribute_docstring,
)
from feature_engine._docstrings.init_parameters.all_trasnformers import (
    _variables_numerical_docstring,
)
from feature_engine._docstrings.methods import (
    _fit_transform_docstring,
    _inverse_transform_docstring,
)
from feature_engine._docstrings.substitute import Substitution
from feature_engine.tags import _return_tags

@Substitution(
    variables=_variables_numerical_docstring,
    variables_=_variables_attribute_docstring,
    feature_names_in_=_feature_names_in_docstring,
    n_features_in_=_n_features_in_docstring,
    fit_transform=_fit_transform_docstring,
    inverse_transform=_inverse_transform_docstring,
)
class YeoJohnsonTransformer(BaseNumericalTransformer):

    def __init__(
        self, variables: Union[None, int, str, List[Union[str, int]]] = None
    ) -> None:
        self.variables = _check_variables_input_value(variables)

    def fit(self, X: pd.DataFrame, y: Optional[pd.Series] = None):

        X = super().fit(X)

        self.lambda_dict_ = {}

        for var in self.variables_:
            _, self.lambda_dict_[var] = stats.yeojohnson(X[var])

        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:

        X = self._check_transform_input_and_state(X)
        for feature in self.variables_:
            X[feature] = stats.yeojohnson(X[feature], lmbda=self.lambda_dict_[feature])

        return X

    def inverse_transform(self, X: pd.DataFrame) -> pd.DataFrame:
        X = self._check_transform_input_and_state(X)

        for feature in self.variables_:
            X[feature] = self._inverse_transform_series(
                X[feature], lmbda=self.lambda_dict_[feature]
            )

        return X

    def _inverse_transform_series(self, X: pd.Series, lmbda: float) -> pd.Series:
        x_inv = pd.Series(np.zeros_like(X), index=X.index)
        pos = X >= 0

        if lmbda == 0:
            x_inv[pos] = np.exp(X[pos]) - 1
        else:
            x_inv[pos] = np.power(X[pos] * lmbda + 1, 1 / lmbda) - 1

        if lmbda != 2:
            x_inv[~pos] = 1 - np.power(-(2 - lmbda) * X[~pos] + 1, 1 / (2 - lmbda))
        else:
            x_inv[~pos] = 1 - np.exp(-X[~pos])

        return x_inv

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["variables"] = "numerical"

        msg = (
            "Transformer raises error when it can't find the optimal lambda for "
            "the transformation, thus this check fails."
        )
        tags_dict["_xfail_checks"]["check_fit2d_1sample"] = msg

        return tags_dict

    def __sklearn_tags__(self):
        return super().__sklearn_tags__()

================================================
FILE: feature_engine/variable_handling/__init__.py
================================================

from .check_variables import (
    check_all_variables,
    check_categorical_variables,
    check_datetime_variables,
    check_numerical_variables,
)
from .find_variables import (
    find_all_variables,
    find_categorical_and_numerical_variables,
    find_categorical_variables,
    find_datetime_variables,
    find_numerical_variables,
)
from .retain_variables import retain_variables_if_in_df

__all__ = [
    "check_all_variables",
    "check_numerical_variables",
    "check_categorical_variables",
    "check_datetime_variables",
    "find_all_variables",
    "find_numerical_variables",
    "find_categorical_variables",
    "find_datetime_variables",
    "find_categorical_and_numerical_variables",
    "retain_variables_if_in_df",
]

================================================
FILE: feature_engine/variable_handling/_variable_type_checks.py
================================================
import warnings

import pandas as pd
from pandas.core.dtypes.common import is_datetime64_any_dtype as is_datetime
from pandas.core.dtypes.common import is_numeric_dtype as is_numeric
from pandas.core.dtypes.common import is_object_dtype as is_object

def _is_categorical_and_is_not_datetime(column: pd.Series) -> bool:
    if is_object(column):
        is_cat = _is_convertible_to_num(column) or not _is_convertible_to_dt(column)

    elif isinstance(column.dtype, pd.CategoricalDtype):
        is_cat = _is_categories_num(column) or not _is_convertible_to_dt(column)

    return is_cat

def _is_categories_num(column: pd.Series) -> bool:
    return is_numeric(column.dtype.categories)

def _is_convertible_to_dt(column: pd.Series) -> bool:
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        return is_datetime(pd.to_datetime(column, errors="ignore", utc=True))

def _is_convertible_to_num(column: pd.Series) -> bool:
    try:
        ser = pd.to_numeric(column)
    except (ValueError, TypeError):
        ser = column
    return is_numeric(ser)

def _is_categorical_and_is_datetime(column: pd.Series) -> bool:
    if is_object(column):
        is_dt = not _is_convertible_to_num(column) and _is_convertible_to_dt(column)

    elif isinstance(column.dtype, pd.CategoricalDtype):
        is_dt = not _is_categories_num(column) and _is_convertible_to_dt(column)

    return is_dt

================================================
FILE: feature_engine/variable_handling/check_variables.py
================================================

from typing import List, Union

import pandas as pd
from pandas.api.types import is_numeric_dtype as is_numeric

from feature_engine.variable_handling._variable_type_checks import (
    _is_categorical_and_is_datetime,
)
from feature_engine.variable_handling.dtypes import DATETIME_TYPES

Variables = Union[int, str, List[Union[str, int]]]

def check_numerical_variables(
    X: pd.DataFrame, variables: Variables
) -> List[Union[str, int]]:

    if isinstance(variables, (str, int)):
        variables = [variables]

    if len(X[variables].select_dtypes(exclude="number").columns) > 0:
        raise TypeError(
            "Some of the variables are not numerical. Please cast them as "
            "numerical before using this transformer."
        )

    return variables

def check_categorical_variables(
    X: pd.DataFrame, variables: Variables
) -> List[Union[str, int]]:

    if isinstance(variables, (str, int)):
        variables = [variables]

    if len(X[variables].select_dtypes(exclude=["O", "category"]).columns) > 0:
        raise TypeError(
            "Some of the variables are not categorical. Please cast them as "
            "object or categorical before using this transformer."
        )

    return variables

def check_datetime_variables(
    X: pd.DataFrame,
    variables: Variables,
) -> List[Union[str, int]]:

    if isinstance(variables, (str, int)):
        variables = [variables]

    non_datetime_vars = []
    for column in X[variables].select_dtypes(exclude=DATETIME_TYPES):
        if is_numeric(X[column]) or not _is_categorical_and_is_datetime(X[column]):
            non_datetime_vars.append(column)

    if len(non_datetime_vars) > 0:
        raise TypeError(
            "Some of the variables are not or cannot be parsed as datetime."
        )

    return variables

def check_all_variables(
    X: pd.DataFrame,
    variables: Variables,
) -> List[Union[str, int]]:
    if isinstance(variables, (str, int)):
        if variables not in X.columns.to_list():
            raise KeyError(f"The variable {variables} is not in the dataframe.")
        variables_ = [variables]

    else:
        if not set(variables).issubset(set(X.columns)):
            raise KeyError("Some of the variables are not in the dataframe.")

        variables_ = variables

    return variables_

================================================
FILE: feature_engine/variable_handling/dtypes.py
================================================
DATETIME_TYPES = ("datetimetz", "datetime")

================================================
FILE: feature_engine/variable_handling/find_variables.py
================================================

from typing import List, Tuple, Union

import pandas as pd
from pandas.api.types import is_datetime64_any_dtype as is_datetime
from pandas.core.dtypes.common import is_numeric_dtype as is_numeric
from pandas.core.dtypes.common import is_object_dtype as is_object

from feature_engine.variable_handling._variable_type_checks import (
    _is_categorical_and_is_datetime,
    _is_categorical_and_is_not_datetime,
)
from feature_engine.variable_handling.dtypes import DATETIME_TYPES

def find_numerical_variables(X: pd.DataFrame) -> List[Union[str, int]]:
    variables = list(X.select_dtypes(include="number").columns)
    if len(variables) == 0:
        raise TypeError(
            "No numerical variables found in this dataframe. Please check "
            "variable format with pandas dtypes."
        )
    return variables

def find_categorical_variables(X: pd.DataFrame) -> List[Union[str, int]]:
    variables = [
        column
        for column in X.select_dtypes(include=["O", "category"]).columns
        if _is_categorical_and_is_not_datetime(X[column])
    ]
    if len(variables) == 0:
        raise TypeError(
            "No categorical variables found in this dataframe. Please check "
            "variable format with pandas dtypes."
        )
    return variables

def find_datetime_variables(X: pd.DataFrame) -> List[Union[str, int]]:

    variables = [
        column
        for column in X.select_dtypes(exclude="number").columns
        if is_datetime(X[column]) or _is_categorical_and_is_datetime(X[column])
    ]

    if len(variables) == 0:
        raise ValueError("No datetime variables found in this dataframe.")

    return variables

def find_all_variables(
    X: pd.DataFrame,
    exclude_datetime: bool = False,
) -> List[Union[str, int]]:
    if exclude_datetime is True:
        variables = X.select_dtypes(exclude=DATETIME_TYPES).columns.to_list()
        variables = [
            var
            for var in variables
            if is_numeric(X[var]) or not _is_categorical_and_is_datetime(X[var])
        ]
    else:
        variables = X.columns.to_list()
    return variables

def find_categorical_and_numerical_variables(
    X: pd.DataFrame,
    variables: Union[None, int, str, List[Union[str, int]]] = None,
) -> Tuple[List[Union[str, int]], List[Union[str, int]]]:

    if isinstance(variables, (str, int)):
        if X[variables].dtype.name == "category" or is_object(X[variables]):
            variables_cat = [variables]
            variables_num = []
        elif is_numeric(X[variables]):
            variables_num = [variables]
            variables_cat = []
        else:
            raise TypeError(
                "The variable entered is neither numerical nor categorical."
            )

    elif variables is None:
        if variables is None:
            variables_cat = [
                column
                for column in X.select_dtypes(include=["O", "category"]).columns
                if _is_categorical_and_is_not_datetime(X[column])
            ]
        variables_num = list(X.select_dtypes(include="number").columns)

        if len(variables_num) == 0 and len(variables_cat) == 0:
            raise TypeError(
                "There are no numerical or categorical variables in the dataframe"
            )

    else:
        if len(variables) == 0:
            raise ValueError("The list of variables is empty.")

        variables_cat = [
            var for var in X[variables].select_dtypes(include=["O", "category"]).columns
        ]

        variables_num = list(X[variables].select_dtypes(include="number").columns)

        if any([v for v in variables if v not in variables_cat + variables_num]):
            raise TypeError(
                "Some of the variables are neither numerical nor categorical."
            )

    return variables_cat, variables_num

================================================
FILE: feature_engine/variable_handling/retain_variables.py
================================================

from typing import List, Union

Variables = Union[int, str, List[Union[str, int]]]

def retain_variables_if_in_df(X, variables):
    if isinstance(variables, (str, int)):
        variables = [variables]

    variables_in_df = [var for var in variables if var in X.columns]

    if len(variables_in_df) == 0:
        raise ValueError(
            "None of the variables in the list are present in the dataframe."
        )

    return variables_in_df

================================================
FILE: feature_engine/wrappers/__init__.py
================================================

from .wrappers import SklearnTransformerWrapper

__all__ = ["SklearnTransformerWrapper"]

================================================
FILE: feature_engine/wrappers/wrappers.py
================================================
from typing import List, Optional, Union

import pandas as pd
from sklearn.base import BaseEstimator, TransformerMixin, clone
from sklearn.utils.validation import check_is_fitted

from feature_engine._check_init_parameters.check_variables import (
    _check_variables_input_value,
)
from feature_engine.dataframe_checks import _check_X_matches_training_df, check_X
from feature_engine.tags import _return_tags
from feature_engine.variable_handling import (
    check_numerical_variables,
    find_numerical_variables,
)
from feature_engine.variable_handling.check_variables import check_all_variables
from feature_engine.variable_handling.find_variables import find_all_variables

_SELECTORS = [
    "GenericUnivariateSelect",
    "RFE",
    "RFECV",
    "SelectFdr",
    "SelectFpr",
    "SelectFromModel",
    "SelectFwe",
    "SelectKBest",
    "SelectPercentile",
    "SequentialFeatureSelector",
    "VarianceThreshold",
]

_CREATORS = [
    "OneHotEncoder",
    "PolynomialFeatures",
]

_TRANSFORMERS = [
    "Binarizer",
    "FunctionTransformer",
    "KBinsDiscretizer",
    "PowerTransformer",
    "QuantileTransformer",
    "SimpleImputer",
    "IterativeImputer",
    "KNNImputer",
    "OrdinalEncoder",
    "MaxAbsScaler",
    "MinMaxScaler",
    "StandardScaler",
    "RobustScaler",
    "Normalizer",
]

_ALL_TRANSFORMERS = _SELECTORS + _CREATORS + _TRANSFORMERS

_INVERSE_TRANSFORM = [
    "PowerTransformer",
    "QuantileTransformer",
    "OrdinalEncoder",
    "MaxAbsScaler",
    "MinMaxScaler",
    "StandardScaler",
    "RobustScaler",
]

class SklearnTransformerWrapper(TransformerMixin, BaseEstimator):

    def __init__(
        self,
        transformer,
        variables: Union[None, int, str, List[Union[str, int]]] = None,
    ) -> None:

        if not issubclass(transformer.__class__, TransformerMixin):
            raise TypeError(
                "transformer expected a Scikit-learn transformer. "
                f"got {transformer} instead. "
            )

        if transformer.__class__.__name__ not in _ALL_TRANSFORMERS:
            raise NotImplementedError(
                "This transformer is not compatible with the wrapper. "
                "Supported transformers are {}.".format(", ".join(_ALL_TRANSFORMERS))
            )

        if (
            transformer.__class__.__name__
            in ["SimpleImputer", "KNNImputer", "IterativeImputer"]
            and transformer.add_indicator is True
        ):
            raise NotImplementedError(
                "The imputer is only compatible with the wrapper when the "
                "parameter `add_indicator` is False. "
            )

        if (
            transformer.__class__.__name__ == "KBinsDiscretizer"
            and transformer.encode != "ordinal"
        ):
            raise NotImplementedError(
                "The KBinsDiscretizer is only compatible with the wrapper when the "
                "parameter `encode` is `ordinal`. "
            )

        if transformer.__class__.__name__ == "OneHotEncoder":
            msg = (
                "The SklearnTransformerWrapper can only wrap the OneHotEncoder if the "
                "sparse is set to False."
            )
            if getattr(transformer, "sparse", False) or getattr(
                transformer, "sparse_output", False
            ):
                raise NotImplementedError(msg)

        self.transformer = transformer
        self.variables = _check_variables_input_value(variables)

    def fit(self, X: pd.DataFrame, y: Optional[str] = None):

        X = check_X(X)

        self.transformer_ = clone(self.transformer)

        if self.transformer_.__class__.__name__ in [
            "OneHotEncoder",
            "OrdinalEncoder",
            "SimpleImputer",
            "FunctionTransformer",
        ]:
            if self.variables is None:
                self.variables_ = find_all_variables(X)
            else:
                self.variables_ = check_all_variables(X, self.variables)

        else:
            if self.variables is None:
                self.variables_ = find_numerical_variables(X)
            else:
                self.variables_ = check_numerical_variables(X, self.variables)

        self.transformer_.fit(X[self.variables_], y)

        if self.transformer_.__class__.__name__ in _SELECTORS:
            selected = X[self.variables_].columns[self.transformer_.get_support()]
            self.features_to_drop_ = [f for f in self.variables_ if f not in selected]

        self.feature_names_in_ = X.columns.tolist()

        self.n_features_in_ = X.shape[1]

        return self

    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        check_is_fitted(self)

        X = check_X(X)

        _check_X_matches_training_df(X, self.n_features_in_)

        X = X[self.feature_names_in_]

        if self.transformer_.__class__.__name__ in [
            "OneHotEncoder",
            "PolynomialFeatures",
        ]:
            new_features_df = pd.DataFrame(
                data=self.transformer_.transform(X[self.variables_]),
                columns=self.transformer_.get_feature_names_out(self.variables_),
                index=X.index,
            )
            X = pd.concat([X.drop(columns=self.variables_), new_features_df], axis=1)

        elif self.transformer_.__class__.__name__ in _SELECTORS:

            X.drop(columns=self.features_to_drop_, inplace=True)

        else:
            X[self.variables_] = self.transformer_.transform(X[self.variables_])

        return X

    def inverse_transform(self, X: pd.DataFrame) -> pd.DataFrame:
        check_is_fitted(self)

        X = check_X(X)

        if self.transformer_.__class__.__name__ not in _INVERSE_TRANSFORM:
            raise NotImplementedError(
                "The method `inverse_transform` is not implemented for this "
                "transformer. Supported transformers are {}.".format(
                    ", ".join(_INVERSE_TRANSFORM)
                )
            )
        if hasattr(self.transformer_, "inverse_transform") and callable(
            self.transformer_.inverse_transform
        ):
            X[self.variables_] = self.transformer_.inverse_transform(X[self.variables_])
        else:
            raise NotImplementedError(
                "This Scikit-learn transformer does not have the method "
                "`inverse_transform` implemented."
            )
        return X

    def get_feature_names_out(
        self, input_features: Optional[List[Union[str, int]]] = None
    ) -> List:
        check_is_fitted(self)

        if self.transformer_.__class__.__name__ in _TRANSFORMERS:
            feature_names = self.feature_names_in_

        if self.transformer_.__class__.__name__ in _CREATORS:
            if input_features is None:
                added_features = self.transformer_.get_feature_names_out(
                    self.variables_
                )
                original_features = [
                    feature
                    for feature in self.feature_names_in_
                    if feature not in self.variables_
                ]
                feature_names = original_features + list(added_features)
            else:
                feature_names = list(
                    self.transformer_.get_feature_names_out(input_features)
                )

        if self.transformer_.__class__.__name__ in _SELECTORS:
            feature_names = [
                f for f in self.feature_names_in_ if f not in self.features_to_drop_
            ]

        return feature_names

    def _more_tags(self):
        tags_dict = _return_tags()
        tags_dict["_xfail_checks"]["check_estimators_nan_inf"] = "transformer allows NA"
        tags_dict["_xfail_checks"][
            "check_parameters_default_constructible"
        ] = "transformer has 1 mandatory parameter"
        return tags_dict

    def __sklearn_tags__(self):
        return super().__sklearn_tags__()