Directory structure:
└── user_guide/
    ├── index.rst
    ├── creation/
    │   ├── CyclicalFeatures.rst
    │   ├── DecisionTreeFeatures.rst
    │   ├── index.rst
    │   ├── MathFeatures.rst
    │   └── RelativeFeatures.rst
    ├── datetime/
    │   ├── DatetimeFeatures.rst
    │   ├── DatetimeSubtraction.rst
    │   └── index.rst
    ├── discretisation/
    │   ├── ArbitraryDiscretiser.rst
    │   ├── DecisionTreeDiscretiser.rst
    │   ├── EqualFrequencyDiscretiser.rst
    │   ├── EqualWidthDiscretiser.rst
    │   ├── GeometricWidthDiscretiser.rst
    │   └── index.rst
    ├── encoding/
    │   ├── CountFrequencyEncoder.rst
    │   ├── DecisionTreeEncoder.rst
    │   ├── index.rst
    │   ├── MeanEncoder.rst
    │   ├── OneHotEncoder.rst
    │   ├── OrdinalEncoder.rst
    │   ├── RareLabelEncoder.rst
    │   ├── StringSimilarityEncoder.rst
    │   └── WoEEncoder.rst
    ├── imputation/
    │   ├── AddMissingIndicator.rst
    │   ├── ArbitraryNumberImputer.rst
    │   ├── CategoricalImputer.rst
    │   ├── DropMissingData.rst
    │   ├── EndTailImputer.rst
    │   ├── index.rst
    │   ├── MeanMedianImputer.rst
    │   └── RandomSampleImputer.rst
    ├── outliers/
    │   ├── ArbitraryOutlierCapper.rst
    │   ├── index.rst
    │   ├── OutlierTrimmer.rst
    │   └── Winsorizer.rst
    ├── pipeline/
    │   ├── index.rst
    │   ├── make_pipeline.rst
    │   └── Pipeline.rst
    ├── preprocessing/
    │   ├── index.rst
    │   ├── MatchCategories.rst
    │   └── MatchVariables.rst
    ├── scaling/
    │   ├── index.rst
    │   └── MeanNormalizationScaler.rst
    ├── selection/
    │   ├── DropConstantFeatures.rst
    │   ├── DropCorrelatedFeatures.rst
    │   ├── DropDuplicateFeatures.rst
    │   ├── DropFeatures.rst
    │   ├── DropHighPSIFeatures.rst
    │   ├── index.rst
    │   ├── MRMR.rst
    │   ├── ProbeFeatureSelection.rst
    │   ├── RecursiveFeatureAddition.rst
    │   ├── RecursiveFeatureElimination.rst
    │   ├── SelectByInformationValue.rst
    │   ├── SelectByShuffling.rst
    │   ├── SelectBySingleFeaturePerformance.rst
    │   ├── SelectByTargetMeanPerformance.rst
    │   └── SmartCorrelatedSelection.rst
    ├── timeseries/
    │   ├── index.rst
    │   └── forecasting/
    │       ├── ExpandingWindowFeatures.rst
    │       ├── index.rst
    │       ├── LagFeatures.rst
    │       └── WindowFeatures.rst
    ├── transformation/
    │   ├── ArcsinTransformer.rst
    │   ├── BoxCoxTransformer.rst
    │   ├── index.rst
    │   ├── LogCpTransformer.rst
    │   ├── LogTransformer.rst
    │   ├── PowerTransformer.rst
    │   ├── ReciprocalTransformer.rst
    │   └── YeoJohnsonTransformer.rst
    ├── variable_handling/
    │   ├── check_all_variables.rst
    │   ├── check_categorical_variables.rst
    │   ├── check_datetime_variables.rst
    │   ├── check_numerical_variables.rst
    │   ├── find_all_variables.rst
    │   ├── find_categorical_and_numerical_variables.rst
    │   ├── find_categorical_variables.rst
    │   ├── find_datetime_variables.rst
    │   ├── find_numerical_variables.rst
    │   ├── index.rst
    │   └── retain_variables_if_in_df.rst
    └── wrappers/
        ├── index.rst
        └── Wrapper.rst

================================================
FILE: docs/user_guide/index.rst
================================================
.. -*- mode: rst -*-
.. _user_guide:

User Guide
==========

In this section you will find additional information about Feature-engine's transformers
and feature engineering transformations in general, as well as additional examples.

Transformation
--------------

.. toctree::
   :maxdepth: 1

   imputation/index
   encoding/index
   discretisation/index
   outliers/index
   transformation/index
   scaling/index

Creation
--------

.. toctree::
   :maxdepth: 1

   creation/index
   datetime/index

Selection
---------
.. toctree::
   :maxdepth: 1

   selection/index

Time series
-----------

.. toctree::
   :maxdepth: 1

   timeseries/index

Other
-----
.. toctree::
   :maxdepth: 1

   preprocessing/index
   wrappers/index

Pipeline
--------
.. toctree::
   :maxdepth: 1

   pipeline/index

Tools
-----
.. toctree::
   :maxdepth: 1

   variable_handling/index

================================================
FILE: docs/user_guide/creation/CyclicalFeatures.rst
================================================
.. _cyclical_features:

.. currentmodule:: feature_engine.creation

CyclicalFeatures
================

Some features are inherently cyclical. Clear examples are **time features**, i.e., those features
derived from datetime variables like the *hours of the day*, the *days of the week*, or the
*months of the year*.

But that’s not the end of it. Many variables related to natural processes are also cyclical,
like, for example, *tides*, *moon cycles*, or *solar energy generation* (which coincides with
light periods, which are cyclical).

In cyclical features, higher values of the variable are closer to lower values.
For example, December (12) is closer to January (1) than to June (6).

How can we convey to machine learning models like linear regression the cyclical nature
of the features?

In the article "Advanced machine learning techniques for building performance simulation,"
the authors engineered cyclical variables by representing them as (x,y) coordinates on a
circle. The idea was that, after preprocessing the cyclical data, the lowest value of every
cyclical feature would appear right next to the largest value.

To represent cyclical features in (x, y) coordinates, the authors created two new features,
deriving the sine and cosine components of the cyclical variable. We call this procedure
**"cyclical encoding."**

Cyclical encoding
-----------------

The trigonometric functions sine and cosine are periodic and repeat their values every
2 pi radians. Thus, to transform cyclical variables into (x, y) coordinates using these
functions, first we need to normalize them to 2 pi radians.

We achieve this by dividing the variables' values by their maximum value. Thus, the two
new features are derived as follows:

- var_sin = sin(variable * (2. * pi / max_value))
- var_cos = cos(variable * (2. * pi / max_value))

In Python, we can encode cyclical features by using the Numpy functions `sin` and `cos`:

.. code:: python

    import numpy as np

    X[f"{variable}_sin"] = np.sin(X["variable"] * (2.0 * np.pi / X["variable"]).max())
    X[f"{variable}_cos"] = np.cos(X["variable"] * (2.0 * np.pi / X["variable"]).max())

We can also use Feature-engine to automate this process.

Cyclical encoding with Feature-engine
-------------------------------------

:class:`CyclicalFeatures()` creates two new features from numerical variables to better
capture the cyclical nature of the original variable. :class:`CyclicalFeatures()` returns
two new features per variable, according to:

- var_sin = sin(variable * (2. * pi / max_value))
- var_cos = cos(variable * (2. * pi / max_value))

where max_value is the maximum value in the variable, and pi is 3.14...

Finding the max_value
~~~~~~~~~~~~~~~~~~~~~

:class:`CyclicalFeatures()` attempts to automate the process of cyclical encoding by
automatically determining the value used to normalize the feature between
0 and 2 * pi radians, which coincides with the cycle of the periodic functions sine and
cosine.

Hence, the values of the variable matter. For example, if the variable `hour` shows values
ranging between 0 and 23, it will create the new features by dividing by 23. If instead,
the variable varies between 1 and 24, it will create the new features by dividing by 24.

If you want to use a specific period to rescale your variables, you can pass a dictionary
containing the variable names as keys, and the values for the scaling as values. In this
way, you can apply cyclical encoding while dividing by 24 to a variable that shows values
between 0 and 23.

Applying cyclical encoding
--------------------------

We'll start by applying cyclical encoding to a toy dataset to get familiar with how to
use Feature-engine for cyclical encoding.

In this example, we'll encode the cyclical features **days of the week** and **months**.
Let's create a toy dataframe with the variables "days" and "months":

.. code:: python

    import pandas as pd
    from feature_engine.creation import CyclicalFeatures

    df = pd.DataFrame({
        'day': [6, 7, 5, 3, 1, 2, 4],
        'months': [3, 7, 9, 12, 4, 6, 12],
        })

In the following output we see the toy dataframe:

.. code:: python

       day  months
    0    6       3
    1    7       7
    2    5       9
    3    3      12
    4    1       4
    5    2       6
    6    4      12

Now we set up :class:`CyclicalFeatures()` to find the maximum value of each variable
automatically:

.. code:: python

    cyclical = CyclicalFeatures(variables=None, drop_original=False)

    X = cyclical.fit_transform(df)

The maximum values used for the transformation are stored in the attribute
`max_values_`:

.. code:: python

    cyclical.max_values_

.. code:: python

    {'day': 7, 'months': 12}

Let's have a look at the transformed dataframe:

.. code:: python

    print(X.head())

We see that the new variables were added at the right of our dataframe.

.. code:: python

       day  months       day_sin   day_cos    months_sin    months_cos
    0    6       3 -7.818315e-01  0.623490  1.000000e+00  6.123234e-17
    1    7       7 -2.449294e-16  1.000000 -5.000000e-01 -8.660254e-01
    2    5       9 -9.749279e-01 -0.222521 -1.000000e+00 -1.836970e-16
    3    3      12  4.338837e-01 -0.900969 -2.449294e-16  1.000000e+00
    4    1       4  7.818315e-01  0.623490  8.660254e-01 -5.000000e-01

Dropping variables after encoding
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the previous section, we set the parameter `drop_original` to `False`, which means that
we keep the original variables after the cyclical encoding. If we want them dropped after
the feature creation, we can set the parameter to `True`:

.. code:: python

    cyclical = CyclicalFeatures(variables=None, drop_original=True)
    X = cyclical.fit_transform(df)
    print(X.head())

The resulting dataframe contains only the cyclical encoded features; the original variables
are removed:

.. code:: python

            day_sin   day_cos    months_sin    months_cos
    0 -7.818315e-01  0.623490  1.000000e+00  6.123234e-17
    1 -2.449294e-16  1.000000 -5.000000e-01 -8.660254e-01
    2 -9.749279e-01 -0.222521 -1.000000e+00 -1.836970e-16
    3  4.338837e-01 -0.900969 -2.449294e-16  1.000000e+00
    4  7.818315e-01  0.623490  8.660254e-01 -5.000000e-01

We can now use the new features, which convey the cyclical nature of the data, to train
machine learning algorithms, like linear or logistic regression.

Obtaining the name of the resulting features
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We can obtain the names of the variables in the transformed dataset as follows:

.. code:: python

    cyclical.get_feature_names_out()

This returns the name of all the variables in the final output:

.. code:: python

    ['day_sin', 'day_cos', 'months_sin', 'months_cos']

Understanding cyclical encoding
-------------------------------

We now know how to convert cyclical variables into (x, y) coordinates of a circle by using
the sine and cosine functions. Let’s now carry out some visualizations to better understand
the effect of this transformation.

Let's create a toy dataframe:

.. code:: python

    import pandas as pd
    import matplotlib.pyplot as plt

    df = pd.DataFrame([i for i in range(24)], columns=['hour'])

If we execute `print(df)`, we'll see the resulting dataframe:

.. code:: python

        hour
    0      0
    1      1
    2      2
    3      3
    4      4
    5      5
    6      6
    7      7
    8      8
    9      9
    10    10
    11    11
    12    12
    13    13
    14    14
    15    15
    16    16
    17    17
    18    18
    19    19
    20    20
    21    21
    22    22
    23    23

Let's now compute the sine and cosine features, and then display the resulting dataframe:

.. code:: python

    cyclical = CyclicalFeatures(variables=None)

    df = cyclical.fit_transform(df)

    print(df.head())

These are the sine and cosine features that represent the hour:

.. code:: python

       hour  hour_sin  hour_cos
    0     0  0.000000  1.000000
    1     1  0.269797  0.962917
    2     2  0.519584  0.854419
    3     3  0.730836  0.682553
    4     4  0.887885  0.460065

Let's now plot the hour variable against its sine transformation. We add perpendicular
lines to flag the hours 0 and 22.

.. code:: python

    plt.scatter(df["hour"], df["hour_sin"])

    # Axis labels
    plt.ylabel('Sine of hour')
    plt.xlabel('Hour')
    plt.title('Sine transformation')

    plt.vlines(x=0, ymin=-1, ymax=0, color='g', linestyles='dashed')
    plt.vlines(x=22, ymin=-1, ymax=-0.25, color='g', linestyles='dashed')

After the transformation using the sine function, we see that the new values for the hours
0 and 22 are closer to each other (follow the dashed lines), which is what we intended:

.. image:: ../../images/hour_sin.png

|

The problem with trigonometric transformations, is that, because they are periodic,
2 different observations can also return similar values after the transformation. Let's
explore that:

.. code:: python

    plt.scatter(df["hour"], df["hour_sin"])

    # Axis labels
    plt.ylabel('Sine of hour')
    plt.xlabel('Hour')
    plt.title('Sine transformation')

    plt.hlines(y=0, xmin=0, xmax=11.5, color='r', linestyles='dashed')

    plt.vlines(x=0, ymin=-1, ymax=0, color='g', linestyles='dashed')
    plt.vlines(x=11.5, ymin=-1, ymax=0, color='g', linestyles='dashed')

In the plot below, we see that the hours 0 and 11.5 obtain very similar values after the
sine transformation, but they are not close to each other. So how can we differentiate them?

.. image:: ../../images/hour_sin2.png

|

To fully encode the information of the hour, we must use the sine and cosine trigonometric
transformations together. Adding the cosine function, which is out of phase respect to the sine
function, breaks the symmetry and assigns a unique codification to each hour.

Let's explore that:

.. code:: python

    plt.scatter(df["hour"], df["hour_sin"])
    plt.scatter(df["hour"], df["hour_cos"])

    # Axis labels
    plt.ylabel('Sine and cosine of hour')
    plt.xlabel('Hour')
    plt.title('Sine and Cosine transformation')

    plt.hlines(y=0, xmin=0, xmax=11.5, color='r', linestyles='dashed')

    plt.vlines(x=0, ymin=-1, ymax=1, color='g', linestyles='dashed')
    plt.vlines(x=11.5, ymin=-1, ymax=1, color='g', linestyles='dashed')

The hour 0, after the transformation, takes the values of sine 0 and cosine 1, which makes
it different from the hour 11.5, which takes the values of sine 0 and cosine -1. In other
words, with the two functions together, we are able to distinguish all observations within
our original variable.

.. image:: ../../images/hour_sin3.png

|

Finally, let's vizualise the (x, y) circle coordinates generated by the sine and cosine
features.

.. code:: python

    fig, ax = plt.subplots(figsize=(7, 5))
    sp = ax.scatter(df["hour_sin"], df["hour_cos"], c=df["hour"])
    ax.set(
        xlabel="sin(hour)",
        ylabel="cos(hour)",
    )
    _ = fig.colorbar(sp)

The following plot conveys the intended effect resulting from applying cyclical encoding
to periodic features.

.. image:: ../../images/hour_sin4.png

|

That's it, you now know how to represent cyclical data through the use of trigonometric
functions and cyclical encoding.

Feature-engine vs Scikit-learn
------------------------------

Let's compare the implementations of cyclical encoding between Feature-engine and Scikit-learn.
We'll work with the Bike sharing demand dataset, and we'll follow the implementation of
Cyclical encoding found in the `Time related features documentation <https://scikit-learn.org/stable/auto_examples/applications/plot_cyclical_feature_engineering.html#trigonometric-features>`_
from Scikit-learn.

Let's load the libraries and dataset:

.. code:: python

    import numpy as np
    import pandas as pd

    from sklearn.compose import ColumnTransformer
    from sklearn.datasets import fetch_openml
    from sklearn.preprocessing import FunctionTransformer

    from feature_engine.creation import CyclicalFeatures

    df = fetch_openml("Bike_Sharing_Demand", version=2, as_frame=True).frame

    print(df.head())

In the following output we see the bike sharing dataset:

.. code:: python

       season  year  month  hour holiday  weekday workingday weather  temp  \
    0  spring     0      1     0   False        6      False   clear  9.84
    1  spring     0      1     1   False        6      False   clear  9.02
    2  spring     0      1     2   False        6      False   clear  9.02
    3  spring     0      1     3   False        6      False   clear  9.84
    4  spring     0      1     4   False        6      False   clear  9.84

       feel_temp  humidity  windspeed  count
    0     14.395      0.81        0.0     16
    1     13.635      0.80        0.0     40
    2     13.635      0.80        0.0     32
    3     14.395      0.75        0.0     13
    4     14.395      0.75        0.0      1

To apply cyclical encoding with Scikit-learn, we can use the `FunctionTransformer`:

.. code:: python

    def sin_transformer(period):
        return FunctionTransformer(lambda x: np.sin(x / period * 2 * np.pi))

    def cos_transformer(period):
        return FunctionTransformer(lambda x: np.cos(x / period * 2 * np.pi))

To apply this transformation to multiple variables with different maximum values, we
can combine the transformers within an instance of the `ColumnTransformer`:

.. code:: python

    cyclic_cossin_transformer = ColumnTransformer(
        transformers=[
            ("month_sin", sin_transformer(12), ["month"]),
            ("month_cos", cos_transformer(12), ["month"]),
            ("weekday_sin", sin_transformer(7), ["weekday"]),
            ("weekday_cos", cos_transformer(7), ["weekday"]),
            ("hour_sin", sin_transformer(24), ["hour"]),
            ("hour_cos", cos_transformer(24), ["hour"]),
        ],
    ).set_output(transform="pandas")

Now we can obtain the cyclical encoded features:

.. code:: python

    Xt = cyclic_cossin_transformer.fit_transform(df)

    print(Xt)

In the following output we see the sine and cosine features of the variables month, weekday
and hour:

.. code:: python

           month_sin__month  month_cos__month  weekday_sin__weekday  \
    0          5.000000e-01          0.866025             -0.781831
    1          5.000000e-01          0.866025             -0.781831
    2          5.000000e-01          0.866025             -0.781831
    3          5.000000e-01          0.866025             -0.781831
    4          5.000000e-01          0.866025             -0.781831
    ...                 ...               ...                   ...
    17374     -2.449294e-16          1.000000              0.781831
    17375     -2.449294e-16          1.000000              0.781831
    17376     -2.449294e-16          1.000000              0.781831
    17377     -2.449294e-16          1.000000              0.781831
    17378     -2.449294e-16          1.000000              0.781831

           weekday_cos__weekday  hour_sin__hour  hour_cos__hour
    0                   0.62349        0.000000        1.000000
    1                   0.62349        0.258819        0.965926
    2                   0.62349        0.500000        0.866025
    3                   0.62349        0.707107        0.707107
    4                   0.62349        0.866025        0.500000
    ...                     ...             ...             ...
    17374               0.62349       -0.965926        0.258819
    17375               0.62349       -0.866025        0.500000
    17376               0.62349       -0.707107        0.707107
    17377               0.62349       -0.500000        0.866025
    17378               0.62349       -0.258819        0.965926

    [17379 rows x 6 columns]

With Feature-engine, we can do the same as follows:

.. code:: python

    tr = CyclicalFeatures(drop_original=True)
    Xt = tr.fit_transform(df[["month", "weekday", "hour"]])

    print(Xt)

Note that with less lines of code we obtained a similar result:

.. code:: python

              month_sin  month_cos   weekday_sin  weekday_cos      hour_sin  \
    0      5.000000e-01   0.866025 -2.449294e-16          1.0  0.000000e+00
    1      5.000000e-01   0.866025 -2.449294e-16          1.0  2.697968e-01
    2      5.000000e-01   0.866025 -2.449294e-16          1.0  5.195840e-01
    3      5.000000e-01   0.866025 -2.449294e-16          1.0  7.308360e-01
    4      5.000000e-01   0.866025 -2.449294e-16          1.0  8.878852e-01
    ...             ...        ...           ...          ...           ...
    17374 -2.449294e-16   1.000000  8.660254e-01          0.5 -8.878852e-01
    17375 -2.449294e-16   1.000000  8.660254e-01          0.5 -7.308360e-01
    17376 -2.449294e-16   1.000000  8.660254e-01          0.5 -5.195840e-01
    17377 -2.449294e-16   1.000000  8.660254e-01          0.5 -2.697968e-01
    17378 -2.449294e-16   1.000000  8.660254e-01          0.5 -2.449294e-16

           hour_cos
    0      1.000000
    1      0.962917
    2      0.854419
    3      0.682553
    4      0.460065
    ...         ...
    17374  0.460065
    17375  0.682553
    17376  0.854419
    17377  0.962917
    17378  1.000000

    [17379 rows x 6 columns]

Note, however, that the dataframes are not identical, because by default, :class:`CyclicalFeatures()`
divides the variables by their maximum values:

.. code:: python

    tr.max_values_

With the default implementation, we divided the variable weekday by 6, instead of 7, and
the variable hour by 23, instead of 24, because the values of these variables vary between
0 and 6, and 0 and 23, respectively.

.. code:: python

    {'month': 12, 'weekday': 6, 'hour': 23}

Practically, there isn't a big difference between the values of the dataframes returned by
Scikit-learn and Feature-engine, and I doubt that this subtle difference will incur in a big
change in model performance.

However, if you want to divide the varibles weekday and hour by 7 and 24 respectively, you can
do so like this:

.. code:: python

    tr = CyclicalFeatures(
        max_values={"month": 12, "weekday": 7, "hour": 24},
        drop_original=True,
    )
    Xt = tr.fit_transform(df[["month", "weekday", "hour"]])

    print(Xt)

And now, the values of the dataframes are identical:

.. code:: python

              month_sin  month_cos  weekday_sin  weekday_cos  hour_sin  hour_cos
    0      5.000000e-01   0.866025    -0.781831      0.62349  0.000000  1.000000
    1      5.000000e-01   0.866025    -0.781831      0.62349  0.258819  0.965926
    2      5.000000e-01   0.866025    -0.781831      0.62349  0.500000  0.866025
    3      5.000000e-01   0.866025    -0.781831      0.62349  0.707107  0.707107
    4      5.000000e-01   0.866025    -0.781831      0.62349  0.866025  0.500000
    ...             ...        ...          ...          ...       ...       ...
    17374 -2.449294e-16   1.000000     0.781831      0.62349 -0.965926  0.258819
    17375 -2.449294e-16   1.000000     0.781831      0.62349 -0.866025  0.500000
    17376 -2.449294e-16   1.000000     0.781831      0.62349 -0.707107  0.707107
    17377 -2.449294e-16   1.000000     0.781831      0.62349 -0.500000  0.866025
    17378 -2.449294e-16   1.000000     0.781831      0.62349 -0.258819  0.965926

    [17379 rows x 6 columns]

Ultimately, choosing the right period for the cyclical encoding is the responsibility of
the user, with automation, we can only go that far.

Additional resources
--------------------

For tutorials on how to create cyclical features, check out the following courses:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

.. figure::  ../../images/fetsf.png
   :width: 300
   :figclass: align-center
   :align: right
   :target: https:

   Feature Engineering for Time Series Forecasting

|
|
|
|
|
|
|
|
|
|

For a comparison between one-hot encoding, ordinal encoding, cyclical encoding and spline
encoding of cyclical features check out the following
`sklearn demo <https://scikit-learn.org/stable/auto_examples/applications/plot_cyclical_feature_engineering.html>`_.

Check also these Kaggle demo on the use of cyclical encoding with neural networks:

- `Encoding Cyclical Features for Deep Learning <https://www.kaggle.com/code/avanwyk/encoding-cyclical-features-for-deep-learning>`_.

================================================
FILE: docs/user_guide/creation/DecisionTreeFeatures.rst
================================================
.. _dtree_features:

.. currentmodule:: feature_engine.creation

DecisionTreeFeatures
====================

The winners of the KDD 2009 competition observed that many features had high
mutual information with the target, but low correlation, leading them to conclude
that the relationships were non-linear. While non-linear relationships can be
captured by non-linear models, to leverage the information from these features with
linear models, we need to somehow transform that information into a linear, or
monotonic relationship with the target.

The output of decision trees, that is, their predictions, should be monotonic with
the target, if there is a good fit for the tree.

In addition, decision trees trained on 2 or more features could capture feature
interactions that simpler models would miss.

By enriching the dataset with features resulting from the predictions of decision trees,
we can create better performing models. On the downside the features resulting
from decision trees, are not easy to interpret or explain.

:class:`DecisionTreeFeatures()` creates and adds features resulting from the predictions
of decision trees trained on 1 or more features.

Values of the tree based features
---------------------------------

If we create features for regression, :class:`DecisionTreeFeatures()` will train scikit-learn's
`DecisionTreeRegressor` under the hood, and the features are derived from the `predict` method
of these regressors. Hence, the features will be in the scale of the target. Remember however,
that the output of decision tree regressors is not continuous, it is a piecewise function.

If we create features for classification, :class:`DecisionTreeFeatures()` will train scikit-learn's
`DecisionTreeClassifier` under the hood. If the target is binary, the resulting features
are the output of the `predict_proba` method of the model corresponding to the predictions
of class 1. If the output is multiclass, on the other hand, the features are derived from
the `predict` method, and hence return the predicted class.

Examples
--------

In the rest of the document, we'll show the versatility of :class:`DecisionTreeFeatures()`
to create multiple features by using decision trees.

Let's start by loading and displaying the California housing dataset from sklearn

.. code:: python

    from sklearn.datasets import fetch_california_housing
    from sklearn.model_selection import train_test_split
    from feature_engine.creation import DecisionTreeFeatures

    X, y = fetch_california_housing(return_X_y=True, as_frame=True)
    X.drop(labels=["Latitude", "Longitude"], axis=1, inplace=True)
    print(X.head())

In the following output we see the dataframe:

.. code:: python

       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup
    0  8.3252      41.0  6.984127   1.023810       322.0  2.555556
    1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842
    2  7.2574      52.0  8.288136   1.073446       496.0  2.802260
    3  5.6431      52.0  5.817352   1.073059       558.0  2.547945
    4  3.8462      52.0  6.281853   1.081081       565.0  2.181467

Let's split the dataset into a training and a testing set:

.. code:: python

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=0)

Combining features - integers
-----------------------------

We'll set up :class:`DecisionTreeFeatures()` to create **all possible** combinations of 2
features. To create all possible combinations we use integers with the `features_to_combine`
parameter:

.. code:: python

    dtf = DecisionTreeFeatures(features_to_combine=2)
    dtf.fit(X_train, y_train)

If we leave the parameter `variables` to `None`, :class:`DecisionTreeFeatures()` will combine
all numerical variables in the training set, in the way we indicate in `features_to_combine`.
Since we set `features_to_combine=2`, the transformer will create all possible combinations of
1 or 2 variables.

We can find the feature combinations that will be used to train the trees as follows:

.. code:: python

    dtf.input_features_

In the following output we see the combinations of 1 and 2 features that will be used
to train decision trees, based of all the numerical variables in the training set:

.. code:: python

    ['MedInc',
     'HouseAge',
     'AveRooms',
     'AveBedrms',
     'Population',
     'AveOccup',
     ['MedInc', 'HouseAge'],
     ['MedInc', 'AveRooms'],
     ['MedInc', 'AveBedrms'],
     ['MedInc', 'Population'],
     ['MedInc', 'AveOccup'],
     ['HouseAge', 'AveRooms'],
     ['HouseAge', 'AveBedrms'],
     ['HouseAge', 'Population'],
     ['HouseAge', 'AveOccup'],
     ['AveRooms', 'AveBedrms'],
     ['AveRooms', 'Population'],
     ['AveRooms', 'AveOccup'],
     ['AveBedrms', 'Population'],
     ['AveBedrms', 'AveOccup'],
     ['Population', 'AveOccup']]

Let's now add the new features to the data:

.. code:: python

    train_t = dtf.transform(X_train)
    test_t = dtf.transform(X_test)

    print(test_t.head())

In the following output we see the resulting data with the features derived from
decision trees:

.. code:: python

           MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  \
    14740  4.1518      22.0  5.663073   1.075472      1551.0  4.180593
    10101  5.7796      32.0  6.107226   0.927739      1296.0  3.020979
    20566  4.3487      29.0  5.930712   1.026217      1554.0  2.910112
    2670   2.4511      37.0  4.992958   1.316901       390.0  2.746479
    15709  5.0049      25.0  4.319261   1.039578       649.0  1.712401

           tree(MedInc)  tree(HouseAge)  tree(AveRooms)  tree(AveBedrms)  ...  \
    14740      2.204822        2.130618        2.001950         2.080254  ...
    10101      2.975513        2.051980        2.001950         2.165554  ...
    20566      2.204822        2.051980        2.001950         2.165554  ...
    2670       1.416771        2.051980        1.802158         1.882763  ...
    15709      2.420124        2.130618        1.802158         2.165554  ...

           tree(['HouseAge', 'AveRooms'])  tree(['HouseAge', 'AveBedrms'])  \
    14740                        1.885406                         2.124812
    10101                        1.885406                         2.124812
    20566                        1.885406                         2.124812
    2670                         1.797902                         1.836498
    15709                        1.797902                         2.124812

           tree(['HouseAge', 'Population'])  tree(['HouseAge', 'AveOccup'])  \
    14740                          2.004703                        1.437440
    10101                          2.004703                        2.257968
    20566                          2.004703                        2.257968
    2670                           2.123579                        2.257968
    15709                          2.123579                        2.603372

           tree(['AveRooms', 'AveBedrms'])  tree(['AveRooms', 'Population'])  \
    14740                         2.099977                          1.878989
    10101                         2.438937                          2.077321
    20566                         2.099977                          1.878989
    2670                          1.728401                          1.843904
    15709                         1.821467                          1.843904

           tree(['AveRooms', 'AveOccup'])  tree(['AveBedrms', 'Population'])  \
    14740                        1.719582                           2.056003
    10101                        2.156884                           2.056003
    20566                        2.156884                           2.056003
    2670                         1.747990                           1.882763
    15709                        2.783690                           2.221092

           tree(['AveBedrms', 'AveOccup'])  tree(['Population', 'AveOccup'])
    14740                         1.400491                          1.484939
    10101                         2.153210                          2.059187
    20566                         2.153210                          2.059187
    2670                          1.861020                          2.235743
    15709                         2.727460                          2.747390

    [5 rows x 27 columns]

Combining features - Lists
--------------------------

Let's say that we want to create features based of trees trained of 2 or more variables. Instead of using
an integer in `features_to_combine`, we need to pass a list of integers, telling :class:`DecisionTreeFeatures()`
to make all possible combinations of the integers mentioned in the list.

We'll set up the transformer to create features based on all possible combinations of
2 and 3 features of just 3 of the numerical variables this time:

.. code:: python

    dtf = DecisionTreeFeatures(
        variables=["AveRooms", "AveBedrms", "Population"],
        features_to_combine=[2,3])

    dtf.fit(X_train, y_train)

If we now examine the feature combinations:

.. code:: python

    dtf.input_features_

We see that they are based of combinations of 2 or 3 of the variables that we set in
the `variables` parameter:

.. code:: python

    [['AveRooms', 'AveBedrms'],
     ['AveRooms', 'Population'],
     ['AveBedrms', 'Population'],
     ['AveRooms', 'AveBedrms', 'Population']]

We can now add the features to the data and inspect the result:

.. code:: python

    train_t = dtf.transform(X_train)
    test_t = dtf.transform(X_test)

    print(test_t.head())

In the following output we see the dataframe with the new features:

.. code:: python

           MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  \
    14740  4.1518      22.0  5.663073   1.075472      1551.0  4.180593
    10101  5.7796      32.0  6.107226   0.927739      1296.0  3.020979
    20566  4.3487      29.0  5.930712   1.026217      1554.0  2.910112
    2670   2.4511      37.0  4.992958   1.316901       390.0  2.746479
    15709  5.0049      25.0  4.319261   1.039578       649.0  1.712401

           tree(['AveRooms', 'AveBedrms'])  tree(['AveRooms', 'Population'])  \
    14740                         2.099977                          1.878989
    10101                         2.438937                          2.077321
    20566                         2.099977                          1.878989
    2670                          1.728401                          1.843904
    15709                         1.821467                          1.843904

           tree(['AveBedrms', 'Population'])  \
    14740                           2.056003
    10101                           2.056003
    20566                           2.056003
    2670                            1.882763
    15709                           2.221092

           tree(['AveRooms', 'AveBedrms', 'Population'])
    14740                                       2.099977
    10101                                       2.438937
    20566                                       2.099977
    2670                                        1.843904
    15709                                       1.843904

Specifying the feature combinations - tuples
--------------------------------------------

We can indicate precisely the features that we want to use as input of the decision trees.
Let's make a tuple containing the features combinations. We want a tree trained with
Population, a tree trained with Population and AveOccup, and a tree trained with
those 2 variables plus HouseAge:

.. code:: python

    features = (('Population'), ('Population', 'AveOccup'),
                ('Population', 'AveOccup', 'HouseAge'))

Now, we pass this tuple to :class:`DecisionTreeFeatures()` and note that we can leave
the parameter `variables` to the default, because with tuples, that parameter is
ignored:

.. code:: python

    dtf = DecisionTreeFeatures(
        variables=None,
        features_to_combine=features,
        cv=5,
        scoring="neg_mean_squared_error"
    )

    dtf.fit(X_train, y_train)

If we inspect the input features, it will coincide with the tuple we passed to
`features_to_combine`:

.. code:: python

    dtf.input_features_

We see that the input features are those from the tuple:

.. code:: python

    ['Population',
     ['Population', 'AveOccup'],
     ['Population', 'AveOccup', 'HouseAge']]

And now we can go ahead and add the features to the data:

.. code:: python

    train_t = dtf.transform(X_train)
    test_t = dtf.transform(X_test)

Examining the new features
--------------------------

:class:`DecisionTreeFeatures()` appends the word `tree` to the new features, so if
we wanted to display only the new features, we can do so as follows

.. code:: python

    tree_features = [var for var in test_t.columns if "tree" in var]
    print(test_t[tree_features].head())

.. code:: python

           tree(Population)  tree(['Population', 'AveOccup'])  \
    14740          2.008283                          1.484939
    10101          2.008283                          2.059187
    20566          2.008283                          2.059187
    2670           2.128961                          2.235743
    15709          2.128961                          2.747390

           tree(['Population', 'AveOccup', 'HouseAge'])
    14740                                      1.443097
    10101                                      2.257968
    20566                                      2.257968
    2670                                       2.257968
    15709                                      3.111251

Evaluating individual trees
---------------------------

We can evaluate the performance of each of the trees used to create the features, if
we so wish. Let's set up the :class:`DecisionTreeFeatures()`:

.. code:: python

    dtf = DecisionTreeFeatures(features_to_combine=2)
    dtf.fit(X_train, y_train)

:class:`DecisionTreeFeatures()` trains each tree with cross-validation. If we do not
pass a grid with hyperparameters, it will optimize the depth by default. We can find
the trained estimators like this:

.. code:: python

    dtf.estimators_

Because the estimators are trained with sklearn's `GridSearchCV`, what is stored is
the result of the search:

.. code:: python

    [GridSearchCV(cv=3, estimator=DecisionTreeRegressor(random_state=0),
                  param_grid={'max_depth': [1, 2, 3, 4]},
                  scoring='neg_mean_squared_error'),
     GridSearchCV(cv=3, estimator=DecisionTreeRegressor(random_state=0),
                  param_grid={'max_depth': [1, 2, 3, 4]},
                  scoring='neg_mean_squared_error'),
     ...

     GridSearchCV(cv=3, estimator=DecisionTreeRegressor(random_state=0),
                  param_grid={'max_depth': [1, 2, 3, 4]},
                  scoring='neg_mean_squared_error'),
     GridSearchCV(cv=3, estimator=DecisionTreeRegressor(random_state=0),
                  param_grid={'max_depth': [1, 2, 3, 4]},
                  scoring='neg_mean_squared_error')]

If you want to inspect an individual tree and it's performance, you can do so like this:

.. code:: python

    tree = dtf.estimators_[4]
    tree.best_params_

In the following output, we see the best parameters obtained for a tree trained based
of the feature **Population** to predict house price:

.. code:: python

    {'max_depth': 2}

If we want to check out the performance of the best tree during found in the grid search,
we can do so like this:

.. code:: python

    tree.score(X_test[['Population']], y_test)

The following performance value corresponds to the negative of the mean squared error
which is the metric optimised durign the search (you can select the metric to optimize
through the `scoring` parameter of :class:`DecisionTreeFeatures()`).

.. code:: python

    -1.3308515769033213

Note that you can also isolate the tree, and then obtain a performance metric:

.. code:: python

    tree.best_estimator_.score(X_test[['Population']], y_test)

In this case, the following performance metric corresponds to the R2, which is the
default metric returned by scikit-learn's DecisionTreeRegressor.

.. code:: python

    0.0017890442253447603

Dropping the original variables
-------------------------------

With :class:`DecisionTreeFeatures()`, we can automatically remove from the resulting
dataframe the features used as input from the decision trees. We need to set `drop_original`
to `True`.

.. code:: python

    dtf = DecisionTreeFeatures(
        variables=["AveRooms", "AveBedrms", "Population"],
        features_to_combine=[2,3],
        drop_original=True
    )

    dtf.fit(X_train, y_train)

    train_t = dtf.transform(X_train)
    test_t = dtf.transform(X_test)

    print(test_t.head())

We see in the resulting dataframe that the variables ["AveRooms", "AveBedrms", "Population"]
are not there:

.. code:: python

           MedInc  HouseAge  AveOccup  tree(['AveRooms', 'AveBedrms'])  \
    14740  4.1518      22.0  4.180593                         2.099977
    10101  5.7796      32.0  3.020979                         2.438937
    20566  4.3487      29.0  2.910112                         2.099977
    2670   2.4511      37.0  2.746479                         1.728401
    15709  5.0049      25.0  1.712401                         1.821467

           tree(['AveRooms', 'Population'])  tree(['AveBedrms', 'Population'])  \
    14740                          1.878989                           2.056003
    10101                          2.077321                           2.056003
    20566                          1.878989                           2.056003
    2670                           1.843904                           1.882763
    15709                          1.843904                           2.221092

           tree(['AveRooms', 'AveBedrms', 'Population'])
    14740                                       2.099977
    10101                                       2.438937
    20566                                       2.099977
    2670                                        1.843904
    15709                                       1.843904

Creating features for classification
------------------------------------

If we are creating features for a classifier instead of a regressor, the procedure is
identical. We just need to set the parameter `regression` to False.

Note that if you are creating features for binary classification, the added features
will contain the probabilities of class 1. If you are creating features for multi-class
classification, on the other hand, the features will contain the prediction of the class.

Additional resources
--------------------

For more details about this and other feature engineering methods check out these resources:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/creation/index.rst
================================================
.. -*- mode: rst -*-

Feature Creation
================

Feature creation, is a common step during data preprocessing, and consists of constructing new
variables from the dataset’s original features. By combining two or more variables, we develop
new features that can improve the performance of a machine learning model, capture additional
information or relationships among variables, or simply make more sense within the domain we
are working on.

One of the most common feature creation methods in data science is `one-hot
encoding <https:
is a feature engineering technique used to transform a categorical feature into multiple
binary variables that represent each category.

Another common feature extraction procedure consist of creating new features from past
values of time series data, for example through the use of lags and windows.

In general, creating features requires a dose of domain knowledge and significant time
invested in analyzing the raw data, including evaluating the relationship between the independent or
predictor variables and the dependent or target variable in the dataset.

Feature creation can be one of the more creative aspects of feature engineering, and the new
features can help improve a predictive model’s performance.

Lastly, a data scientist should be mindful that creating new features may increase the dimensionality
of the dataset quite dramatically. For example, one hot encoding of highly cardinal categorical
features results in lots of binary variables, and so does polynomial combinations of high powers.
This may have downstream effects depending on the machine learning algorithm being used. For example,
decision trees are known for not being able to cope with huge number of features.

Creating New Features with Feature-engine
-----------------------------------------

Feature-engine has several transformers that create and add new features to the dataset. One of
the most popular ones is the `OneHotEncoder <https://feature-engine.trainindata.com/en/latest/user_guide/encoding/OneHotEncoder.html>`_
that creates dummy variables from categorical features.

With Feature-engine we can also create new features from time series data through lags and windows by using
`LagFeatures <https://feature-engine.trainindata.com/en/latest/user_guide/timeseries/forecasting/LagFeatures.html>`_
or `WindowFeatures <https://feature-engine.trainindata.com/en/latest/user_guide/timeseries/forecasting/WindowFeatures.html>`_.

Feature-engine’s creation module, supports transformers that create and add new features to a pandas
dataframe by either combining existing features through different mathematical or statistical operations,
or through feature transformations. These transformers operate with numerical variables, that is, those
with integer and float data types.

Summary of Feature-engine’s feature-creation transformers:

- **CyclicalFeatures** - Creates two new features per variable by applying the trigonometric operations sine and cosine to the original feature.

- **MathFeatures** - Combines a set of features into new variables by applying basic mathematical functions like the sum, mean, maximum or standard deviation.

- **RelativeFeatures** - Utilizes basic mathematical functions between a group of variables and one or more reference features, appending the new features to the pandas dataframe.

- **DecisionTreeFeatures** - Creates new features as the output of decision trees trained on 1 or more feature combinations.

Feature creation module
-----------------------

.. toctree::
   :maxdepth: 1

   CyclicalFeatures
   MathFeatures
   RelativeFeatures
   DecisionTreeFeatures

Feature-engine in Practice
--------------------------

Here, you'll get a taste of the transformers from the feature creation module from Feature-engine.
We'll use the wine quality dataset. The dataset is comprised of 11 features, including `alcohol`,
`ash`, and ``flavonoids``, and has `quality` as its target variable.

Through exploratory data analysis and our domain knowledge which includes real-world
experimentation, i.e., drinking various brands/types of wine, we believe that we can
create better features to train our algorithm by combining original features with various
mathematical operations.

Let's load the dataset from Scikit-learn.

.. code:: python

    import pandas as pd
    from sklearn.datasets import load_wine
    from feature_engine.creation import RelativeFeatures, MathFeatures

    X, y = load_wine(return_X_y=True, as_frame=True)
    print(X.head())

Below we see the wine quality dataset:

.. code:: python

       alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \
    0    14.23        1.71  2.43               15.6      127.0           2.80
    1    13.20        1.78  2.14               11.2      100.0           2.65
    2    13.16        2.36  2.67               18.6      101.0           2.80
    3    14.37        1.95  2.50               16.8      113.0           3.85
    4    13.24        2.59  2.87               21.0      118.0           2.80

       flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \
    0        3.06                  0.28             2.29             5.64  1.04
    1        2.76                  0.26             1.28             4.38  1.05
    2        3.24                  0.30             2.81             5.68  1.03
    3        3.49                  0.24             2.18             7.80  0.86
    4        2.69                  0.39             1.82             4.32  1.04

       od280/od315_of_diluted_wines  proline
    0                          3.92   1065.0
    1                          3.40   1050.0
    2                          3.17   1185.0
    3                          3.45   1480.0
    4                          2.93    735.0

Now, we create a new feature by removing non-flavonoid phenols from the total phenols to
obtain the phenols that are not flavonoid.

.. code:: python

    rf = RelativeFeatures(
        variables=["total_phenols"],
        reference=["nonflavanoid_phenols"],
        func=["sub"],
    )

    rf.fit(X)
    X_tr = rf.transform(X)

    print(X_tr.head())

We see the new feature and its data points at the right of the pandas dataframe:

.. code:: python

       alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \
    0    14.23        1.71  2.43               15.6      127.0           2.80
    1    13.20        1.78  2.14               11.2      100.0           2.65
    2    13.16        2.36  2.67               18.6      101.0           2.80
    3    14.37        1.95  2.50               16.8      113.0           3.85
    4    13.24        2.59  2.87               21.0      118.0           2.80

       flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \
    0        3.06                  0.28             2.29             5.64  1.04
    1        2.76                  0.26             1.28             4.38  1.05
    2        3.24                  0.30             2.81             5.68  1.03
    3        3.49                  0.24             2.18             7.80  0.86
    4        2.69                  0.39             1.82             4.32  1.04

       od280/od315_of_diluted_wines  proline  \
    0                          3.92   1065.0
    1                          3.40   1050.0
    2                          3.17   1185.0
    3                          3.45   1480.0
    4                          2.93    735.0

       total_phenols_sub_nonflavanoid_phenols
    0                                    2.52
    1                                    2.39
    2                                    2.50
    3                                    3.61
    4                                    2.41

Let's now create new features by combining a subset of 3 existing variables:

.. code:: python

    mf = MathFeatures(
        variables=["flavanoids", "proanthocyanins", "proline"],
        func=["sum", "mean"],
    )

    mf.fit(X_tr)
    X_tr = mf.transform(X_tr)

    print(X_tr.head())

We see the new features at the right of the resulting pandas dataframe:

.. code:: python

       alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \
    0    14.23        1.71  2.43               15.6      127.0           2.80
    1    13.20        1.78  2.14               11.2      100.0           2.65
    2    13.16        2.36  2.67               18.6      101.0           2.80
    3    14.37        1.95  2.50               16.8      113.0           3.85
    4    13.24        2.59  2.87               21.0      118.0           2.80

       flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \
    0        3.06                  0.28             2.29             5.64  1.04
    1        2.76                  0.26             1.28             4.38  1.05
    2        3.24                  0.30             2.81             5.68  1.03
    3        3.49                  0.24             2.18             7.80  0.86
    4        2.69                  0.39             1.82             4.32  1.04

       od280/od315_of_diluted_wines  proline  \
    0                          3.92   1065.0
    1                          3.40   1050.0
    2                          3.17   1185.0
    3                          3.45   1480.0
    4                          2.93    735.0

       total_phenols_sub_nonflavanoid_phenols  \
    0                                    2.52
    1                                    2.39
    2                                    2.50
    3                                    3.61
    4                                    2.41

       sum_flavanoids_proanthocyanins_proline  \
    0                                 1070.35
    1                                 1054.04
    2                                 1191.05
    3                                 1485.67
    4                                  739.51

       mean_flavanoids_proanthocyanins_proline
    0                               356.783333
    1                               351.346667
    2                               397.016667
    3                               495.223333
    4                               246.503333

In the above examples, we used `RelativeFeature()` and `MathFeatures` to perform automated feature
engineering on the input data by applying the transformations defined in the `func` parameter on
the features identified in `variables`  and ``reference`` parameters.

The original and new features can now be used to train a regression model, or a multiclass
classification algorithm, to predict the `quality` of the wine.

Summary
-------

Through feature engineering and feature creation, we can optimize the machine learning algorithm's
learning process and improve its performance metrics.

We'd strongly recommend the creation of features based on domain knowledge, exploratory data
analysis and thorough data mining. We also understand that this is not always possible, particularly
with big datasets and limited time allocated to each project. In this situation, we can combine
the creation of features with feature selection procedures to let machine learning algorithms
select what works best for them.

Good luck with your models!

Tutorials, books and courses
----------------------------

For tutorials about this and other feature engineering for machine learning methods check out
our online course:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

Transformers in other Libraries
-------------------------------

Check also the following transformer from Scikit-learn:

* `PolynomialFeatures <https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html>`_
* `SplineTransformer <https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.SplineTransformer.html>`_

================================================
FILE: docs/user_guide/creation/MathFeatures.rst
================================================
.. _math_features:

.. currentmodule:: feature_engine.creation

MathFeatures
============

:class:`MathFeatures()` applies basic functions to groups of features, returning one or
more additional variables as a result.  It uses `pandas.agg()` to create the features,
so in essence, you can pass any function that is accepted by this method. One exception
is that :class:`MathFeatures()` does not accept dictionaries for the parameter `func`.

The functions can be passed as strings, numpy methods, i.e., np.mean, or any function
that you create, as long as, it returns a scalar from a vector.

For supported aggregation functions, see
`pandas documentation <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.agg.html>`_.

As an example, if we have the variables:

- **number_payments_first_quarter**
- **number_payments_second_quarter**
- **number_payments_third_quarter**
- **number_payments_fourth_quarter**

we can use :class:`MathFeatures()` to calculate the total number of payments
and mean number of payments as follows:

.. code-block:: python

    transformer = MathFeatures(
        variables=[
            'number_payments_first_quarter',
            'number_payments_second_quarter',
            'number_payments_third_quarter',
            'number_payments_fourth_quarter'
        ],
        func=['sum','mean'],
        new_variables_name=[
            'total_number_payments',
            'mean_number_payments'
        ]
    )

    Xt = transformer.fit_transform(X)

The transformed dataset, Xt, will contain the additional features
**total_number_payments** and **mean_number_payments**, plus the original set of
variables.

The variable **total_number_payments** is obtained by adding up the features
indicated in `variables`, whereas the variable **mean_number_payments** is
the mean of those 4 features.

Examples
--------

Let's dive into how we can use :class:`MathFeatures()` in more details. Let's first
create a toy dataset:

.. code:: python

    import numpy as np
    import pandas as pd
    from feature_engine.creation import MathFeatures

    df = pd.DataFrame.from_dict(
        {
            "Name": ["tom", "nick", "krish", "jack"],
            "City": ["London", "Manchester", "Liverpool", "Bristol"],
            "Age": [20, 21, 19, 18],
            "Marks": [0.9, 0.8, 0.7, 0.6],
            "dob": pd.date_range("2020-02-24", periods=4, freq="T"),
        })

    print(df)

The dataset looks like this:

.. code:: python

        Name        City  Age  Marks                 dob
    0    tom      London   20    0.9 2020-02-24 00:00:00
    1   nick  Manchester   21    0.8 2020-02-24 00:01:00
    2  krish   Liverpool   19    0.7 2020-02-24 00:02:00
    3   jack     Bristol   18    0.6 2020-02-24 00:03:00

We can now apply several functions over the numerical variables Age and Marks using
strings to indicate the functions:

.. code:: python

    transformer = MathFeatures(
        variables=["Age", "Marks"],
        func = ["sum", "prod", "min", "max", "std"],
    )

    df_t = transformer.fit_transform(df)

    print(df_t)

And we obtain the following dataset, where the new variables are named after the function
used to obtain them, plus the group of variables that were used in the computation:

.. code:: python

        Name        City  Age  Marks                 dob  sum_Age_Marks  \
    0    tom      London   20    0.9 2020-02-24 00:00:00           20.9
    1   nick  Manchester   21    0.8 2020-02-24 00:01:00           21.8
    2  krish   Liverpool   19    0.7 2020-02-24 00:02:00           19.7
    3   jack     Bristol   18    0.6 2020-02-24 00:03:00           18.6

       prod_Age_Marks  min_Age_Marks  max_Age_Marks  std_Age_Marks
    0            18.0            0.9           20.0      13.505740
    1            16.8            0.8           21.0      14.283557
    2            13.3            0.7           19.0      12.940054
    3            10.8            0.6           18.0      12.303658

For more flexibility, we can pass existing functions to the `func` argument as follows:

.. code:: python

    transformer = MathFeatures(
        variables=["Age", "Marks"],
        func = [np.sum, np.prod, np.min, np.max, np.std],
    )

    df_t = transformer.fit_transform(df)

    print(df_t)

And we obtain the following dataframe:

.. code:: python

        Name        City  Age  Marks                 dob  sum_Age_Marks  \
    0    tom      London   20    0.9 2020-02-24 00:00:00           20.9
    1   nick  Manchester   21    0.8 2020-02-24 00:01:00           21.8
    2  krish   Liverpool   19    0.7 2020-02-24 00:02:00           19.7
    3   jack     Bristol   18    0.6 2020-02-24 00:03:00           18.6

       prod_Age_Marks  amin_Age_Marks  amax_Age_Marks  std_Age_Marks
    0            18.0             0.9            20.0      13.505740
    1            16.8             0.8            21.0      14.283557
    2            13.3             0.7            19.0      12.940054
    3            10.8             0.6            18.0      12.303658

We have the option to set the parameter `drop_original` to True to drop the variables
after performing the calculations.

We can obtain the names of all the features in the transformed data as follows:

.. code:: python

    transformer.get_feature_names_out(input_features=None)

Which will return the names of all the variables in the transformed data:

.. code:: python

    ['Name',
     'City',
     'Age',
     'Marks',
     'dob',
     'sum_Age_Marks',
     'prod_Age_Marks',
     'amin_Age_Marks',
     'amax_Age_Marks',
     'std_Age_Marks']

New variables names
^^^^^^^^^^^^^^^^^^^

Even though the transfomer allows to combine variables automatically, its use is intended
to combine variables with domain knowledge. In this case, we normally want to
give meaningful names to the variables. We can do so through the parameter
`new_variables_names`.

`new_variables_names` takes a list of strings, with the new variable names. In this
parameter, you need to enter a list of names for the newly created features. You must
enter one name for each function indicated in the `func` parameter.
That is, if you want to perform mean and sum of features, you should enter 2 new
variable names. If you compute only the mean of features, enter 1 variable name.

The name of the variables should coincide with the order of the functions in `func`.
That is, if you set `func = ['mean', 'prod']`, the first new variable name will be
assigned to the mean of the variables and the second variable name to the product of the
variables.

Let's look at an example. In the following code snippet, we add up, and find the maximum
and minimum value of 2 variables, which results in 3 new features. We add the names
for the new features in a list:

.. code:: python

    transformer = MathFeatures(
        variables=["Age", "Marks"],
        func = ["sum", "min", "max"],
        new_variables_names = ["sum_vars", "min_vars", "max_vars"]
    )

    df_t = transformer.fit_transform(df)

    print(df_t)

The resulting dataframe contains the new features under the variable names that we
provided:

.. code:: python

        Name        City  Age  Marks                 dob  sum_vars  min_vars  \
    0    tom      London   20    0.9 2020-02-24 00:00:00      20.9       0.9
    1   nick  Manchester   21    0.8 2020-02-24 00:01:00      21.8       0.8
    2  krish   Liverpool   19    0.7 2020-02-24 00:02:00      19.7       0.7
    3   jack     Bristol   18    0.6 2020-02-24 00:03:00      18.6       0.6

       max_vars
    0      20.0
    1      21.0
    2      19.0
    3      18.0

Additional resources
--------------------

For more details about this and other feature engineering methods check out these resources:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/creation/RelativeFeatures.rst
================================================
.. _relative_features:

.. currentmodule:: feature_engine.creation

RelativeFeatures
================

:class:`RelativeFeatures()` applies basic mathematical operations between a group
of variables and one or more reference features, adding the resulting features
to the dataframe.

:class:`RelativeFeatures()` uses the pandas methods `pd.DataFrame.add()`, `pd.DataFrame.sub()`,
`pd.DataFrame.mul()`, `pd.DataFrame.div()`, `pd.DataFrame.truediv()`, `pd.DataFrame.floordiv()`,
`pd.DataFrame.mod()` and `pd.DataFrame.pow()` to transform a group of variables by a group
of reference variables.

For example, if we have the variables:

- **number_payments_first_quarter**
- **number_payments_second_quarter**
- **number_payments_third_quarter**
- **number_payments_fourth_quarter**
- **total_payments**,

we can use :class:`RelativeFeatures()` to determine the percentage of payments per
quarter as follows:

.. code-block:: python

    transformer = RelativeFeatures(
        variables=[
            'number_payments_first_quarter',
            'number_payments_second_quarter',
            'number_payments_third_quarter',
            'number_payments_fourth_quarter',
        ],
        reference=['total_payments'],
        func=['div'],
    )

    Xt = transformer.fit_transform(X)

The precedent code block will return a new dataframe, Xt, with 4 new variables that are
calculated as the division of each one of the variables in `variables` and
'total_payments'.

Examples
--------

Let's dive into how we can use :class:`RelativeFeatures()` in more details. Let's first
create a toy dataset:

.. code:: python

    import pandas as pd
    from feature_engine.creation import RelativeFeatures

    df = pd.DataFrame.from_dict(
        {
            "Name": ["tom", "nick", "krish", "jack"],
            "City": ["London", "Manchester", "Liverpool", "Bristol"],
            "Age": [20, 21, 19, 18],
            "Marks": [0.9, 0.8, 0.7, 0.6],
            "dob": pd.date_range("2020-02-24", periods=4, freq="T"),
        })

    print(df)

The dataset looks like this:

.. code:: python

        Name        City  Age  Marks                 dob
    0    tom      London   20    0.9 2020-02-24 00:00:00
    1   nick  Manchester   21    0.8 2020-02-24 00:01:00
    2  krish   Liverpool   19    0.7 2020-02-24 00:02:00
    3   jack     Bristol   18    0.6 2020-02-24 00:03:00

We can now apply several functions between the numerical variables Age and Marks and Age
as follows:

.. code:: python

    transformer = RelativeFeatures(
        variables=["Age", "Marks"],
        reference=["Age"],
        func = ["sub", "div", "mod", "pow"],
    )

    df_t = transformer.fit_transform(df)

    print(df_t)

And we obtain the following dataset, where the new variables are named after the variables
that were used for the calculation and the function in the middle of their names. Thus,
`Mark_sub_Age` means `Mark - Age`, and `Marks_mod_Age` means `Mark % Age`.

.. code:: python

        Name        City  Age  Marks                 dob  Age_sub_Age  \
    0    tom      London   20    0.9 2020-02-24 00:00:00            0
    1   nick  Manchester   21    0.8 2020-02-24 00:01:00            0
    2  krish   Liverpool   19    0.7 2020-02-24 00:02:00            0
    3   jack     Bristol   18    0.6 2020-02-24 00:03:00            0

       Marks_sub_Age  Age_div_Age  Marks_div_Age  Age_mod_Age  Marks_mod_Age  \
    0          -19.1          1.0       0.045000            0            0.9
    1          -20.2          1.0       0.038095            0            0.8
    2          -18.3          1.0       0.036842            0            0.7
    3          -17.4          1.0       0.033333            0            0.6

               Age_pow_Age  Marks_pow_Age
    0 -2101438300051996672       0.121577
    1 -1595931050845505211       0.009223
    2  6353754964178307979       0.001140
    3  -497033925936021504       0.000102

We can obtain the names of all the features in the transformed data as follows:

.. code:: python

    transformer.get_feature_names_out(input_features=None)

Which will return the names of all the variables in the transformed data:

.. code:: python

    ['Name',
     'City',
     'Age',
     'Marks',
     'dob',
     'Age_sub_Age',
     'Marks_sub_Age',
     'Age_div_Age',
     'Marks_div_Age',
     'Age_mod_Age',
     'Marks_mod_Age',
     'Age_pow_Age',
     'Marks_pow_Age']

Additional resources
--------------------

For more details about this and other feature engineering methods check out these resources:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/datetime/DatetimeFeatures.rst
================================================
.. _datetime_features:

.. currentmodule:: feature_engine.datetime

DatetimeFeatures
================

In datasets commonly used in data science and machine learning projects, the variables very
often contain information about date and time. **Date of birth** and **time of purchase** are two
examples of these variables. They are commonly referred to as “datetime features”, that is,
data whose data type is date and time.

We don’t normally use datetime variables in their raw format to train machine learning models,
like those for regression, classification, or clustering. Instead, we can extract a lot of information
from these variables by extracting the different date and time components of the datetime
variable.

Examples of date and time components are the year, the month, the week_of_year, the day
of the week, the hour, the minutes, and the seconds.

Datetime features with pandas
-----------------------------

In Python, we can extract date and time components through the `dt` module of the open-source
library pandas. For example, by executing the following:

.. code:: python

    data = pd.DataFrame({"date": pd.date_range("2019-03-05", periods=20, freq="D")})

    data["year"] = data["date"].dt.year
    data["quarter"] = data["date"].dt.quarter
    data["month"] = data["date"].dt.month

In the former code block we created 3 features from the timestamp variable: the *year*, the
*quarter* and the *month*.

Datetime features with Feature-engine
-------------------------------------

:class:`DatetimeFeatures()` automatically extracts several date and time features from
datetime variables. It works with variables whose dtype is datetime, as well as with
object-like and categorical variables, provided that they can be parsed into datetime
format. It *cannot* extract features from numerical variables.

:class:`DatetimeFeatures()` uses the pandas `dt` module under the hood, therefore automating
datetime feature engineering. In two lines of code and by specifying which features we
want to create with :class:`DatetimeFeatures()`, we can create multiple date and time variables
from various variables simultaneously.

:class:`DatetimeFeatures()` can automatically create all features supported by pandas `dt`
and a few more, like, for example, a binary feature indicating if the event occurred on
a weekend and also the semester.

With :class:`DatetimeFeatures()` we can choose which date and time features to extract
from the datetime variables. We can also extract date and time features from one or more
datetime variables at the same time.

Through the following examples we highlight the functionality and versatility of :class:`DatetimeFeatures()`
for tabular data.

Extract date features
~~~~~~~~~~~~~~~~~~~~~

In this example, we are going to extract three **date features** from a
specific variable in the dataframe. In particular, we are interested
in the *month*, the *day of the year*, and whether that day was the *last
day the month*.

First, we will create a toy dataframe with 2 date variables:

.. code:: python

    import pandas as pd
    from feature_engine.datetime import DatetimeFeatures

    toy_df = pd.DataFrame({
        "var_date1": ['May-1989', 'Dec-2020', 'Jan-1999', 'Feb-2002'],
        "var_date2": ['06/21/2012', '02/10/1998', '08/03/2010', '10/31/2020'],
    })

Now, we will extract the variables month, month-end and the day of the year from the
second datetime variable in our dataset.

.. code:: python

    dtfs = DatetimeFeatures(
        variables="var_date2",
        features_to_extract=["month", "month_end", "day_of_year"]
    )

    df_transf = dtfs.fit_transform(toy_df)

    df_transf

With `transform()`, the features extracted from the datetime variable are added to the
dataframe.

We see the new features in the following output:

.. code:: python

      var_date1  var_date2_month  var_date2_month_end  var_date2_day_of_year
    0  May-1989                6                    0                    173
    1  Dec-2020                2                    0                     41
    2  Jan-1999                8                    0                    215
    3  Feb-2002               10                    1                    305

By default, :class:`DatetimeFeatures()` drops the variable from which the date and time
features were extracted, in this case, *var_date2*. To keep the variable, we just need
to indicate `drop_original=False` when initializing the transformer.

Finally, we can obtain the name of the variables in the returned data as follows:

.. code:: python

    dtfs.get_feature_names_out()

.. code:: python

    ['var_date1',
     'var_date2_month',
     'var_date2_month_end',
     'var_date2_day_of_year']

Extract time features
~~~~~~~~~~~~~~~~~~~~~

In this example, we are going to extract the feature *minute* from the two time
variables in our dataset.

First, let's create a toy dataset with 2 time variables and an object variable.

.. code:: python 

    import pandas as pd
    from feature_engine.datetime import DatetimeFeatures

    toy_df = pd.DataFrame({
        "not_a_dt": ['not', 'a', 'date', 'time'],
        "var_time1": ['12:34:45', '23:01:02', '11:59:21', '08:44:23'],
        "var_time2": ['02:27:26', '10:10:55', '17:30:00', '18:11:18'],
    })

:class:`DatetimeFeatures()` automatically finds all variables that can be parsed to
datetime. So if we want to extract time features from all our datetime variables, we
don't need to specify them.

Note that from version 2.0.0 pandas deprecated the parameter `infer_datetime_format`.
Hence, if you want pandas to infer the datetime format and you have different formats,
you need to explicitly say so by passing `"mixed"` to the `format` parameter as shown
below.

.. code:: python

    dfts = DatetimeFeatures(features_to_extract=["minute"], format="mixed")

    df_transf = dfts.fit_transform(toy_df)

    df_transf

We see the new features in the following output:

.. code:: python

      not_a_dt  var_time1_minute  var_time2_minute
    0      not                34                27
    1        a                 1                10
    2     date                59                30
    3     time                44                11

The transformer found two variables in the dataframe that can be cast to datetime and
proceeded to extract the requested feature from them.

The variables detected as datetime are stored in the transformer's `variables_` attribute:

.. code:: python

    dfts.variables_

.. code:: python

    ['var_time1', 'var_time2']

The original datetime variables are dropped from the data by default. This leaves the
dataset ready to train machine learning algorithms like linear regression or random forests.

If we want to keep the datetime variables, we just need to indicate `drop_original=False`
when initializing the transformer.

Finally, if we want to obtain the names of the variables in the output data, we can use:

.. code:: python

    dfts.get_feature_names_out()

.. code:: python

    ['not_a_dt', 'var_time1_minute', 'var_time2_minute']

Extract date and time features
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In this example, we will combine what we have seen in the previous two examples
and extract a date feature - *year* - and time feature - *hour* -
from two variables that contain both date and time information.

Let's go ahead and create a toy dataset with 3 datetime variables.

.. code:: python

    import pandas as pd
    from feature_engine.datetime import DatetimeFeatures

    toy_df = pd.DataFrame({
        "var_dt1": pd.date_range("2018-01-01", periods=3, freq="H"),
        "var_dt2": ['08/31/00 12:34:45', '12/01/90 23:01:02', '04/25/01 11:59:21'],
        "var_dt3": ['03/02/15 02:27:26', '02/28/97 10:10:55', '11/11/03 17:30:00'],
    })

Now, we set up the :class:`DatetimeFeatures()` to extract features from 2 of the datetime
variables. In this case, we do not want to drop the datetime variable after extracting
the features.

.. code:: python

    dfts = DatetimeFeatures(
        variables=["var_dt1", "var_dt3"],
        features_to_extract=["year", "hour"],
        drop_original=False,
        format="mixed",
    )
    df_transf = dfts.fit_transform(toy_df)

    df_transf

We can see the resulting dataframe in the following output:

.. code:: python

                  var_dt1            var_dt2            var_dt3  var_dt1_year  \
    0 2018-01-01 00:00:00  08/31/00 12:34:45  03/02/15 02:27:26          2018
    1 2018-01-01 01:00:00  12/01/90 23:01:02  02/28/97 10:10:55          2018
    2 2018-01-01 02:00:00  04/25/01 11:59:21  11/11/03 17:30:00          2018

       var_dt1_hour  var_dt3_year  var_dt3_hour
    0             0          2015             2
    1             1          1997            10
    2             2          2003            17

And that is it. The new features are now added to the dataframe.

Time series
~~~~~~~~~~~

Time series data consists of datapoints indexed in time order. The time is usually in
the index of the dataframe. We can extract features from the timestamp index and use them
for time series regression or classification, as well as for time series forecasting.

With :class:`DatetimeFeatures()` we can also create date and time features from the
dataframe index.

Let's create a toy dataframe with datetime in the index.

.. code:: python

    import pandas as pd

    X = {"ambient_temp": [31.31, 31.51, 32.15, 32.39, 32.62, 32.5, 32.52, 32.68],
         "module_temp": [49.18, 49.84, 52.35, 50.63, 49.61, 47.01, 46.67, 47.52],
         "irradiation": [0.51, 0.79, 0.65, 0.76, 0.42, 0.49, 0.57, 0.56],
         "color": ["green"] * 4 + ["blue"] * 4,
         }

    X = pd.DataFrame(X)
    X.index = pd.date_range("2020-05-15 12:00:00", periods=8, freq="15min")

    X.head()

Below we see the output of our toy dataframe:

.. code:: python

                         ambient_temp  module_temp  irradiation  color
    2020-05-15 12:00:00         31.31        49.18         0.51  green
    2020-05-15 12:15:00         31.51        49.84         0.79  green
    2020-05-15 12:30:00         32.15        52.35         0.65  green
    2020-05-15 12:45:00         32.39        50.63         0.76  green
    2020-05-15 13:00:00         32.62        49.61         0.42   blue

We can extract features from the index as follows:

..  code:: python

    from feature_engine.datetime import DatetimeFeatures

    dtf = DatetimeFeatures(variables="index")

    Xtr = dtf.fit_transform(X)

    Xtr

We can see that the transformer created the default time features and added them at
the end of the dataframe.

.. code:: python

                         ambient_temp  module_temp  irradiation  color  month  \
    2020-05-15 12:00:00         31.31        49.18         0.51  green      5
    2020-05-15 12:15:00         31.51        49.84         0.79  green      5
    2020-05-15 12:30:00         32.15        52.35         0.65  green      5
    2020-05-15 12:45:00         32.39        50.63         0.76  green      5
    2020-05-15 13:00:00         32.62        49.61         0.42   blue      5
    2020-05-15 13:15:00         32.50        47.01         0.49   blue      5
    2020-05-15 13:30:00         32.52        46.67         0.57   blue      5
    2020-05-15 13:45:00         32.68        47.52         0.56   blue      5

                         year  day_of_week  day_of_month  hour  minute  second
    2020-05-15 12:00:00  2020            4            15    12       0       0
    2020-05-15 12:15:00  2020            4            15    12      15       0
    2020-05-15 12:30:00  2020            4            15    12      30       0
    2020-05-15 12:45:00  2020            4            15    12      45       0
    2020-05-15 13:00:00  2020            4            15    13       0       0
    2020-05-15 13:15:00  2020            4            15    13      15       0
    2020-05-15 13:30:00  2020            4            15    13      30       0
    2020-05-15 13:45:00  2020            4            15    13      45       0

We can obtain the name of all the variables in the output dataframe as follows:

.. code:: python

    dtf.get_feature_names_out()

.. code:: python

    ['ambient_temp',
     'module_temp',
     'irradiation',
     'color',
     'month',
     'year',
     'day_of_week',
     'day_of_month',
     'hour',
     'minute',
     'second']

Important
---------

We highly recommend specifying the date and time features that you would like to extract
from your datetime variables.

If you have too many time variables, this might not be possible. In this case, keep in
mind that if you extract date features from variables that have only time, or time features
from variables that have only dates, your features will be meaningless.

Let's explore the outcome with an example. We create a dataset with only time variables.

.. code:: python

    import pandas as pd
    from feature_engine.datetime import DatetimeFeatures

    toy_df = pd.DataFrame({
        "not_a_dt": ['not', 'a', 'date', 'time'],
        "var_time1": ['12:34:45', '23:01:02', '11:59:21', '08:44:23'],
        "var_time2": ['02:27:26', '10:10:55', '17:30:00', '18:11:18'],
    })

And now we mistakenly extract only date features:

.. code:: python

    dfts = DatetimeFeatures(
        features_to_extract=["year", "month", "day_of_week"],
        format="mixed",
    )
    df_transf = dfts.fit_transform(toy_df)

    df_transf

.. code:: python

      not_a_dt  var_time1_year  var_time1_month  var_time1_day_of_week  var_time2_year \
    0      not            2021               12                      2            2021
    1        a            2021               12                      2            2021
    2     date            2021               12                      2            2021
    3     time            2021               12                      2            2021

       var_time2_month  var_time2_day_of_week
    0               12                      2
    1               12                      2
    2               12                      2
    3               12                      2

The transformer will still create features derived from today's date (the date of
creating the docs).

If instead we have a dataframe with only date variables:

.. code:: python

    import pandas as pd
    from feature_engine.datetime import DatetimeFeatures

    toy_df = pd.DataFrame({
        "var_date1": ['May-1989', 'Dec-2020', 'Jan-1999', 'Feb-2002'],
        "var_date2": ['06/21/12', '02/10/98', '08/03/10', '10/31/20'],
    })

And we mistakenly extract the hour and the minute:

.. code:: python

    dfts = DatetimeFeatures(
        features_to_extract=["hour", "minute"],
        format="mixed",
    )
    df_transf = dfts.fit_transform(toy_df)

    print(df_transf)

.. code:: python

       var_date1_hour  var_date1_minute  var_date2_hour  var_date2_minute
    0               0                 0               0                 0
    1               0                 0               0                 0
    2               0                 0               0                 0
    3               0                 0               0                 0

The new features will contain the value 0.

Automating feature extraction
-----------------------------

We can indicate which features we want to extract from the datetime variables as we did
in the previous examples, by passing the feature names in lists.

Alternatively, :class:`DatetimeFeatures()` has default options to extract a group of
commonly used features, or all supported features.

Let's first create a toy dataframe:

.. code:: python

    import pandas as pd
    from feature_engine.datetime import DatetimeFeatures

    toy_df = pd.DataFrame({
        "var_dt1": pd.date_range("2018-01-01", periods=3, freq="H"),
        "var_dt2": ['08/31/00 12:34:45', '12/01/90 23:01:02', '04/25/01 11:59:21'],
        "var_dt3": ['03/02/15 02:27:26', '02/28/97 10:10:55', '11/11/03 17:30:00'],
    })

Most common features
~~~~~~~~~~~~~~~~~~~~

Now, we will extract the **most common** date and time features from one of the variables.
To do this, we leave the parameter `features_to_extract` to `None`.

.. code:: python

    dfts = DatetimeFeatures(
        variables=["var_dt1"],
        features_to_extract=None,
        drop_original=False,
    )

    df_transf = dfts.fit_transform(toy_df)

    df_transf

.. code:: python

                  var_dt1            var_dt2            var_dt3  var_dt1_month  \
    0 2018-01-01 00:00:00  08/31/00 12:34:45  03/02/15 02:27:26              1
    1 2018-01-01 01:00:00  12/01/90 23:01:02  02/28/97 10:10:55              1
    2 2018-01-01 02:00:00  04/25/01 11:59:21  11/11/03 17:30:00              1

       var_dt1_year  var_dt1_day_of_week  var_dt1_day_of_month  var_dt1_hour  \
    0          2018                    0                     1             0
    1          2018                    0                                   1
    2          2018                    0                  1                2

        var_dt1_minute    var_dt1_second
    0               0                  0
    1               0                  0
    2               0                  0

Our new dataset contains the original features plus the new variables extracted
from them.

We can find the group of features extracted by the transformer in its attribute:

.. code:: python

    dfts.features_to_extract_

.. code:: python

    ['month',
     'year',
     'day_of_week',
     'day_of_month',
     'hour',
     'minute',
     'second']

All supported features
~~~~~~~~~~~~~~~~~~~~~~

We can also extract all supported features automatically, by setting the parameter
`features_to_extract` to `"all"`:

.. code:: python

    dfts = DatetimeFeatures(
        variables=["var_dt1"],
        features_to_extract='all',
        drop_original=False,
    )

    df_transf = dfts.fit_transform(toy_df)

    print(df_transf)

.. code:: python

                  var_dt1            var_dt2            var_dt3  var_dt1_month  \
    0 2018-01-01 00:00:00  08/31/00 12:34:45  03/02/15 02:27:26              1
    1 2018-01-01 01:00:00  12/01/90 23:01:02  02/28/97 10:10:55              1
    2 2018-01-01 02:00:00  04/25/01 11:59:21  11/11/03 17:30:00              1

       var_dt1_quarter  var_dt1_semester  var_dt1_year  \
    0                1                 1          2018
    1                1                 1          2018
    2                1                 1          2018

       var_dt1_week  var_dt1_day_of_week  ...  var_dt1_month_end  var_dt1_quarter_start  \
    0             1                    0  ...                  0                      1
    1             1                    0  ...                  0                      1
    2             1                    0  ...                  0                      1

       var_dt1_quarter_end  var_dt1_year_start  var_dt1_year_end  \
    0                    0                   1                 0
    1                    0                   1                 0
    2                    0                   1                 0

       var_dt1_leap_year  var_dt1_days_in_month  var_dt1_hour  var_dt1_minute  \
    0                  0                     31             0               0
    1                  0                     31             1               0
    2                  0                     31             2               0

       var_dt1_second
    0               0
    1               0
    2               0

We can find the group of features extracted by the transformer in its attribute:

.. code:: python

    dfts.features_to_extract_

.. code:: python

    ['month',
     'quarter',
     'semester',
     'year',
     'week',
     'day_of_week',
     'day_of_month',
     'day_of_year',
     'weekend',
     'month_start',
     'month_end',
     'quarter_start',
     'quarter_end',
     'year_start',
     'year_end',
     'leap_year',
     'days_in_month',
     'hour',
     'minute',
     'second']

Extract and select features automatically
-----------------------------------------

If we have a dataframe with date variables, time variables and date and time variables,
we can extract all features, or the most common features from all the variables, and then
go ahead and remove the irrelevant features with the `DropConstantFeatures()` class.

Let's create a dataframe with a mix of datetime variables.

.. code:: python

    import pandas as pd
    from sklearn.pipeline import Pipeline
    from feature_engine.datetime import DatetimeFeatures
    from feature_engine.selection import DropConstantFeatures

    toy_df = pd.DataFrame({
        "var_date": ['06/21/12', '02/10/98', '08/03/10', '10/31/20'],
        "var_time1": ['12:34:45', '23:01:02', '11:59:21', '08:44:23'],
        "var_dt": ['08/31/00 12:34:45', '12/01/90 23:01:02', '04/25/01 11:59:21', '04/25/01 11:59:21'],
    })

Now, we line up in a Scikit-learn pipeline the :class:`DatetimeFeatures` and the
`DropConstantFeatures()`. The :class:`DatetimeFeatures` will create date features
derived from today for the time variable, and time features with the value 0 for the
date only variable. `DropConstantFeatures()` will identify and remove these features
from the dataset.

.. code:: python

    pipe = Pipeline([
        ('datetime', DatetimeFeatures(format="mixed")),
        ('drop_constant', DropConstantFeatures()),
    ])

    pipe.fit(toy_df)

.. code:: python

    Pipeline(steps=[('datetime', DatetimeFeatures()),
                    ('drop_constant', DropConstantFeatures())])

.. code:: python

    df_transf = pipe.transform(toy_df)

    print(df_transf)

.. code:: python

       var_date_month  var_date_year  var_date_day_of_week  var_date_day_of_month  \
    0               6           2012                     3                     21
    1               2           1998                     1                     10
    2               8           2010                     1                      3
    3              10           2020                     5                     31

       var_time1_hour  var_time1_minute  var_time1_second  var_dt_month  \
    0              12                34                45             8
    1              23                 1                 2            12
    2              11                59                21             4
    3               8                44                23             4

       var_dt_year  var_dt_day_of_week  var_dt_day_of_month  var_dt_hour  \
    0         2000                   3                   31           12
    1         1990                   5                    1           23
    2         2001                   2                   25           11
    3         2001                   2                   25           11

       var_dt_minute   var_dt_second
    0             34              45
    1              1               2
    2             59              21
    3             59              21

As you can see, we do not have the constant features in the transformed dataset.

Working with different timezones
--------------------------------

Time-aware datetime variables can be particularly cumbersome to work with as far
as the format goes. We will briefly show how :class:`DatetimeFeatures()` deals
with such variables in three different scenarios.

**Case 1**: our dataset contains a time-aware variable in object format,
with potentially different timezones across different observations. 
We pass `utc=True` when initializing the transformer to make sure it
converts all data to UTC timezone.

.. code:: python

    import pandas as pd
    from feature_engine.datetime import DatetimeFeatures

    toy_df = pd.DataFrame({
        "var_tz": ['12:34:45+3', '23:01:02-6', '11:59:21-8', '08:44:23Z']
    })

    dfts = DatetimeFeatures(
        features_to_extract=["hour", "minute"],
        drop_original=False,
        utc=True,
        format="mixed",
    )

    df_transf = dfts.fit_transform(toy_df)

    df_transf

.. code:: python

           var_tz  var_tz_hour  var_tz_minute
    0  12:34:45+3            9             34
    1  23:01:02-6            5              1
    2  11:59:21-8           19             59
    3   08:44:23Z            8             44

**Case 2**: our dataset contains a variable that is cast as a localized
datetime in a particular timezone. However, we decide that we want to get all
the datetime information extracted as if it were in UTC timezone.

.. code:: python

    import pandas as pd
    from feature_engine.datetime import DatetimeFeatures

    var_tz = pd.Series(['08/31/00 12:34:45', '12/01/90 23:01:02', '04/25/01 11:59:21'])
    var_tz = pd.to_datetime(var_tz, format="mixed")
    var_tz = var_tz.dt.tz_localize("US/eastern")
    var_tz

.. code:: python

    0   2000-08-31 12:34:45-04:00
    1   1990-12-01 23:01:02-05:00
    2   2001-04-25 11:59:21-04:00
    dtype: datetime64[ns, US/Eastern]

We need to pass `utc=True` when initializing the transformer to revert back to the UTC
timezone.

.. code:: python

    toy_df = pd.DataFrame({"var_tz": var_tz})

    dfts = DatetimeFeatures(
        features_to_extract=["day_of_month", "hour"],
        drop_original=False,
        utc=True,
    )

    df_transf = dfts.fit_transform(toy_df)

    df_transf

.. code:: python

                         var_tz  var_tz_day_of_month  var_tz_hour
    0 2000-08-31 12:34:45-04:00                   31           16
    1 1990-12-01 23:01:02-05:00                    2            4
    2 2001-04-25 11:59:21-04:00                   25           15

**Case 3**: given a variable like *var_tz* in the example above, we now want
to extract the features keeping the original timezone localization,
therefore we pass `utc=False` or `None`. In this case, we leave it to `None` which
is the default option.

.. code:: python

    dfts = DatetimeFeatures(
        features_to_extract=["day_of_month", "hour"],
        drop_original=False,
        utc=None,
    )

    df_transf = dfts.fit_transform(toy_df)

    print(df_transf)

.. code:: python

                         var_tz  var_tz_day_of_month  var_tz_hour
    0 2000-08-31 12:34:45-04:00                   31           12
    1 1990-12-01 23:01:02-05:00                    1           23
    2 2001-04-25 11:59:21-04:00                   25           11

Note that the hour extracted from the variable differ in this dataframe respect to the
one obtained in **Case 2**.

Missing timestamps
------------------

:class:`DatetimeFeatures` has the option to ignore missing timestamps, or raise an error
when a missing value is encountered in a datetime variable.

Additional resources
--------------------

You can find an example of how to use :class:`DatetimeFeatures()` with a real dataset in
the following `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/datetime/DatetimeFeatures.ipynb>`_

For tutorials on how to create and use features from datetime columns, check the following courses:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

.. figure::  ../../images/fetsf.png
   :width: 300
   :figclass: align-center
   :align: right
   :target: https:

   Feature Engineering for Time Series Forecasting

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/datetime/DatetimeSubtraction.rst
================================================
.. _datetime_subtraction:

.. currentmodule:: feature_engine.datetime

DatetimeSubtraction
===================

Very often, we have datetime variables in our datasets, and we want to determine the time
difference between them. For example, if we work with financial data, we may have the
variable **date of loan application**, with the date and time when the customer applied for
a loan, and also the variable **date of birth**, with the customer's date of birth. With those
two variables, we want to infer the **age of the customer** at the time of application. In order
to do this, we can compute the difference in years between `date_of_loan_application` and
`date_of_birth` and capture it in a new variable.

In a different example, if we are trying to predict the price of the house and we have
information about the year in which the house was built, we can infer the age of the house
at the point of sale. Generally, older houses cost less. To calculate the age of the house,
we’d simply compute the difference in years between the sale date and the date at which
it was built.

The Python program offers many options for making operations between datetime objects, like,
for example, the datetime module. Since most likely you will be working with Pandas dataframes,
we will focus this guide on pandas and then how we can automate the procedure with Feature-engine.

Subtracting datetime features with pandas
-----------------------------------------

In Python, we can subtract datetime objects with pandas. To work with datetime variables
in pandas, we need to make sure that the timestamp, which can be represented in various
formats, like strings (str), objects (`"O"`), or datetime, is cast as a datetime. If not, we
can convert strings to datetime objects by executing `pd.to_datetime(df[variable_of_interest])`.

Let’s create a toy dataframe with 2 datetime variables for a short demo:

.. code:: python

    import numpy as np
    import pandas as pd

    data = pd.DataFrame({
        "date1": pd.date_range("2019-03-05", periods=5, freq="D"),
        "date2": pd.date_range("2018-03-05", periods=5, freq="W")})

    print(data)

This is the data that we created, containing two datetime variables:

.. code:: python

           date1      date2
    0 2019-03-05 2018-03-11
    1 2019-03-06 2018-03-18
    2 2019-03-07 2018-03-25
    3 2019-03-08 2018-04-01
    4 2019-03-09 2018-04-08

Now, we can subtract `date2` from `date1` and capture the difference in a new variable by
utilizing the pandas subtraction operator:

.. code:: python

    data["diff"] = data["date1"].sub(data["date2"])

    print(data)

The new variable, which expresses the difference in number of days, is at the right of the
dataframe:

.. code:: python

           date1      date2     diff
    0 2019-03-05 2018-03-11 359 days
    1 2019-03-06 2018-03-18 353 days
    2 2019-03-07 2018-03-25 347 days
    3 2019-03-08 2018-04-01 341 days
    4 2019-03-09 2018-04-08 335 days

If we want the units in something other than days, we can use numpy’s timedelta. The following
example shows how to use this syntax:

.. code:: python

    data["diff"] = data["date1"].sub(data["date2"], axis=0).div(
        np.timedelta64(1, "Y").astype("timedelta64[ns]"))

    print(data)

We see the new variable now expressing the difference in years, at the right of the dataframe:

.. code:: python

           date1      date2      diff
    0 2019-03-05 2018-03-11  0.982909
    1 2019-03-06 2018-03-18  0.966481
    2 2019-03-07 2018-03-25  0.950054
    3 2019-03-08 2018-04-01  0.933626
    4 2019-03-09 2018-04-08  0.917199

If you wanted to subtract various datetime variables, you would have to write lines of code
for every subtraction. Fortunately, we can automate this procedure with :class:`DatetimeSubstraction()`.

Datetime subtraction with Feature-engine
----------------------------------------

:class:`DatetimeSubstraction()` automatically subtracts several date and time features from
each other. You just need to indicate the features at the right of the subtraction operation
in the `variables` parameters and those on the left in the `reference parameter`. You can also
change the output unit through the `output_unit` parameter.

:class:`DatetimeSubstraction()` works with variables whose `dtype` is datetime, as well as
with object-like and categorical variables, provided that they can be parsed into datetime
format. This will be done under the hood by the transformer.

Following up with the former example, here is how we obtain the difference in number of
days using :class:`DatetimeSubstraction()`:

.. code:: python

    import pandas as pd
    from feature_engine.datetime import DatetimeSubtraction

    data = pd.DataFrame({
        "date1": pd.date_range("2019-03-05", periods=5, freq="D"),
        "date2": pd.date_range("2018-03-05", periods=5, freq="W")})

    dtf = DatetimeSubtraction(
        variables="date1",
        reference="date2",
        output_unit="Y")

    data = dtf.fit_transform(data)

    print(data)

With `transform()`, :class:`DatetimeSubstraction()` returns a new dataframe containing the
original variables and also the new variables with the time difference:

.. code:: python

           date1      date2  date1_sub_date2
    0 2019-03-05 2018-03-11         0.982909
    1 2019-03-06 2018-03-18         0.966481
    2 2019-03-07 2018-03-25         0.950054
    3 2019-03-08 2018-04-01         0.933626
    4 2019-03-09 2018-04-08         0.917199

Drop original variables after computation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We have the option to drop the original datetime variables after the computation:

.. code:: python

    import pandas as pd
    from feature_engine.datetime import DatetimeSubtraction

    data = pd.DataFrame({
        "date1": pd.date_range("2019-03-05", periods=5, freq="D"),
        "date2": pd.date_range("2018-03-05", periods=5, freq="W")})

    dtf = DatetimeSubtraction(
        variables="date1",
        reference="date2",
        output_unit="M",
        drop_original=True
    )

    data = dtf.fit_transform(data)

    print(data)

In this case, the resulting dataframe contains only the time difference between the two
original variables:

.. code:: python

       date1_sub_date2
    0        11.794903
    1        11.597774
    2        11.400645
    3        11.203515
    4        11.006386

Subtract multiple variables simultaneously
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We can perform multiple subtractions at the same time. In this example, we will add new
datetime variables to the toy dataframe as strings. The idea is to show that
:class:`DatetimeSubstraction()` will convert those strings to datetime under the hood to
carry out the subtraction operation.

.. code:: python

    import pandas as pd
    from feature_engine.datetime import DatetimeSubtraction

    data = pd.DataFrame({
        "date1" : ["2022-09-01", "2022-10-01", "2022-12-01"],
        "date2" : ["2022-09-15", "2022-10-15", "2022-12-15"],
        "date3" : ["2022-08-01", "2022-09-01", "2022-11-01"],
        "date4" : ["2022-08-15", "2022-09-15", "2022-11-15"],
    })

    dtf = DatetimeSubtraction(variables=["date1", "date2"], reference=["date3", "date4"])

    data = dtf.fit_transform(data)

    print(data)

The resulting dataframe contains the original variables plus the  new variables expressing
the time difference between the date objects.

.. code:: python

            date1       date2       date3       date4  date1_sub_date3  \
    0  2022-09-01  2022-09-15  2022-08-01  2022-08-15             31.0
    1  2022-10-01  2022-10-15  2022-09-01  2022-09-15             30.0
    2  2022-12-01  2022-12-15  2022-11-01  2022-11-15             30.0

       date2_sub_date3  date1_sub_date4  date2_sub_date4
    0             45.0             17.0             31.0
    1             44.0             16.0             30.0
    2             44.0             16.0             30.0

Working with missing values
~~~~~~~~~~~~~~~~~~~~~~~~~~~

By default, :class:`DatetimeSubstraction()`  will raise an error if the dataframe passed
to the `fit()` or `transform()` methods contains NA in the variables to subtract. We can
override this behaviour and allow computations between variables with nan by setting the
parameter `missing_values` to `"ignore"`. Here is a code example:

.. code:: python

    import numpy as np
    import pandas as pd
    from feature_engine.datetime import DatetimeSubtraction

    data = pd.DataFrame({
        "date1" : ["2022-09-01", "2022-10-01", "2022-12-01"],
        "date2" : ["2022-09-15", np.nan, "2022-12-15"],
        "date3" : ["2022-08-01", "2022-09-01", "2022-11-01"],
        "date4" : ["2022-08-15", "2022-09-15", np.nan],
    })

    dtf = DatetimeSubtraction(
        variables=["date1", "date2"],
        reference=["date3", "date4"],
        missing_values="ignore")

    data = dtf.fit_transform(data)

    print(data)

When any of the variables contains NAN, the new features with the time difference will also
display NANs:

.. code:: python

            date1       date2       date3       date4  date1_sub_date3  \
    0  2022-09-01  2022-09-15  2022-08-01  2022-08-15             31.0
    1  2022-10-01         NaN  2022-09-01  2022-09-15             30.0
    2  2022-12-01  2022-12-15  2022-11-01         NaN             30.0

       date2_sub_date3  date1_sub_date4  date2_sub_date4
    0             45.0             17.0             31.0
    1              NaN             16.0              NaN
    2             44.0              NaN              NaN

Working with different timezones
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If we have timestamps in different timezones or variables in different timezones, we can
still perform subtraction operations with :class:`DatetimeSubstraction()` by first setting
all timestamps to the universal central time zone. Here is a code example, were we return
the time difference in microseconds:

.. code:: python

    import pandas as pd
    from feature_engine.datetime import DatetimeSubtraction

    data = pd.DataFrame({
        "date1": ['12:34:45+3', '23:01:02-6', '11:59:21-8', '08:44:23Z'],
        "date2": ['09:34:45+1', '23:01:02-6+1', '11:59:21-8-2', '08:44:23+3']
    })

    dfts = DatetimeSubtraction(
        variables="date1",
        reference="date2",
        utc=True,
        output_unit="ms",
        format="mixed"
    )

    new = dfts.fit_transform(data)

    print(new)

We see the resulting dataframe with the time difference in microseconds:

.. code:: python

            date1         date2  date1_sub_date2
    0  12:34:45+3    09:34:45+1        3600000.0
    1  23:01:02-6  23:01:02-6+1       25200000.0
    2  11:59:21-8  11:59:21-8-2       21600000.0
    3   08:44:23Z    08:44:23+3       10800000.0

Adding arbitrary names to the new variables
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Often, we want to compute just a few time differences. In this case, we may want as well
to assign the new variables specific names. In this code example, we do so:

.. code:: python

    import pandas as pd
    from feature_engine.datetime import DatetimeSubtraction

    data = pd.DataFrame({
        "date1": pd.date_range("2019-03-05", periods=5, freq="D"),
        "date2": pd.date_range("2018-03-05", periods=5, freq="W")})

    dtf = DatetimeSubtraction(
        variables="date1",
        reference="date2",
        new_variables_names=["my_new_var"]
        )

    data = dtf.fit_transform(data)

    print(data)

In the resulting dataframe, we see that the time difference was captured in a variable
called `my_new_var`:

.. code:: python

           date1      date2  my_new_var
    0 2019-03-05 2018-03-11       359.0
    1 2019-03-06 2018-03-18       353.0
    2 2019-03-07 2018-03-25       347.0
    3 2019-03-08 2018-04-01       341.0
    4 2019-03-09 2018-04-08       335.0

We should be mindful to pass a list of variales containing as many names as new variables.
The number of variables that will be created is obtained by multiplying the number of variables
in the parameter `variables` by the number of variables in the parameter `reference`.

get_feature_names_out()
~~~~~~~~~~~~~~~~~~~~~~~

Finally, we can extract the names of the transformed dataframe for compatibility with the
Scikit-learn pipeline:

.. code:: python

    import pandas as pd
    from feature_engine.datetime import DatetimeSubtraction

    data = pd.DataFrame({
        "date1" : ["2022-09-01", "2022-10-01", "2022-12-01"],
        "date2" : ["2022-09-15", "2022-10-15", "2022-12-15"],
        "date3" : ["2022-08-01", "2022-09-01", "2022-11-01"],
        "date4" : ["2022-08-15", "2022-09-15", "2022-11-15"],
    })

    dtf = DatetimeSubtraction(variables=["date1", "date2"], reference=["date3", "date4"])
    dtf.fit(data)

    dtf.get_feature_names_out()

Below the name of the variables that will appear in any dataframe resulting from applying
the `transform()` method:

.. code:: python

    ['date1',
     'date2',
     'date3',
     'date4',
     'date1_sub_date3',
     'date2_sub_date3',
     'date1_sub_date4',
     'date2_sub_date4']

Combining extraction and subtraction of datetime features
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We can also combine the creation of numerical variables from datetime features with the
creation of new features by subtraction of datetime variables:

.. code:: python

    import pandas as pd
    from sklearn.pipeline import Pipeline
    from feature_engine.datetime import DatetimeFeatures, DatetimeSubtraction

    data = pd.DataFrame({
        "date1" : ["2022-09-01", "2022-10-01", "2022-12-01"],
        "date2" : ["2022-09-15", "2022-10-15", "2022-12-15"],
        "date3" : ["2022-08-01", "2022-09-01", "2022-11-01"],
        "date4" : ["2022-08-15", "2022-09-15", "2022-11-15"],
    })

    dtf = DatetimeFeatures(variables=["date1", "date2"], drop_original=False)
    dts = DatetimeSubtraction(
        variables=["date1", "date2"],
        reference=["date3", "date4"],
        drop_original=True,
    )

    pipe = Pipeline([
        ("features", dtf),("subtraction", dts)
    ])

    data = pipe.fit_transform(data)

    print(data)

In the following output we see the new dataframe contaning the features that were extracted
from the different datetime variables followed by those created by capturing the time
difference:

.. code:: python

       date1_month  date1_year  date1_day_of_week  date1_day_of_month  date1_hour  \
    0            9        2022                  3                   1           0
    1           10        2022                  5                   1           0
    2           12        2022                  3                   1           0

       date1_minute  date1_second  date2_month  date2_year  date2_day_of_week  \
    0             0             0            9        2022                  3
    1             0             0           10        2022                  5
    2             0             0           12        2022                  3

       date2_day_of_month  date2_hour  date2_minute  date2_second  \
    0                  15           0             0             0
    1                  15           0             0             0
    2                  15           0             0             0

       date1_sub_date3  date2_sub_date3  date1_sub_date4  date2_sub_date4
    0             31.0             45.0             17.0             31.0
    1             30.0             44.0             16.0             30.0
    2             30.0             44.0             16.0             30.0

Additional resources
--------------------

For tutorials on how to create and use features from datetime columns, check the following courses:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

.. figure::  ../../images/fetsf.png
   :width: 300
   :figclass: align-center
   :align: right
   :target: https:

   Feature Engineering for Time Series Forecasting

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/datetime/index.rst
================================================
.. -*- mode: rst -*-

Datetime Features
=================

Feature-engine’s datetime transformers are able to extract a wide variety of datetime
features from existing datetime or object-like data.

.. toctree::
   :maxdepth: 1

   DatetimeFeatures
   DatetimeSubtraction

================================================
FILE: docs/user_guide/discretisation/ArbitraryDiscretiser.rst
================================================
.. _arbitrary_discretiser:

.. currentmodule:: feature_engine.discretisation

ArbitraryDiscretiser
====================

The :class:`ArbitraryDiscretiser()` sorts the variable values into contiguous intervals
which limits are arbitrarily defined by the user. Thus, you must provide a dictionary
with the variable names as keys and a list with the limits of the intervals as values,
when setting up the discretiser.

The :class:`ArbitraryDiscretiser()` works only with numerical variables. The discretiser
will check that the variables entered by the user are present in the train set and cast
as numerical.

Example
-------

Let's take a look at how this transformer works. First, let's load a dataset and plot a
histogram of a continuous variable. We use the california housing dataset that comes
with Scikit-learn.

.. code:: python

    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    from sklearn.datasets import fetch_california_housing
    from feature_engine.discretisation import ArbitraryDiscretiser

    X, y = fetch_california_housing( return_X_y=True, as_frame=True)

    X['MedInc'].hist(bins=20)
    plt.xlabel('MedInc')
    plt.ylabel('Number of obs')
    plt.title('Histogram of MedInc')
    plt.show()

In the following plot we see a histogram of the variable median income:

.. image:: ../../images/medinc_hist.png

Now, let's discretise the variable into arbitrarily determined intervals. We want the
intervals as integers in the resulting transformation, so we set `return_boundaries` to
`False`.

.. code:: python

    user_dict = {'MedInc': [0, 2, 4, 6, np.inf]}

    transformer = ArbitraryDiscretiser(
        binning_dict=user_dict, return_object=False, return_boundaries=False)

    X = transformer.fit_transform(X)

Now, we can go ahead and plot the variable after the transformation:

.. code:: python

    X['MedInc'].value_counts().plot.bar(rot=0)
    plt.xlabel('MedInc - bins')
    plt.ylabel('Number of observations')
    plt.title('Discretised MedInc')
    plt.show()

In the following plot we see the number of observations per interval:

.. image:: ../../images/medinc_disc_arbitrarily.png

Note that in the above figure the intervals are represented by digits.

Alternatively, we can return the interval limits in the discretised variable by
setting `return_boundaries` to `True`.

.. code:: python

    X, y = fetch_california_housing( return_X_y=True, as_frame=True)

    user_dict = {'MedInc': [0, 2, 4, 6, np.inf]}

    transformer = ArbitraryDiscretiser(
        binning_dict=user_dict, return_object=False, return_boundaries=True)
    X = transformer.fit_transform(X)

    X['MedInc'].value_counts().plot.bar(rot=0)
    plt.xlabel('MedInc - bins')
    plt.ylabel('Number of observations')
    plt.title('Discretised MedInc')
    plt.show()

In the following plot we see the number of observations per interval:

.. image:: ../../images/medinc_disc_arbitrarily2.png

**Discretisation plus encoding**

If we return the interval values as integers, the discretiser has the option to return
the transformed variable as integer or as object. Why would we want the transformed
variables as object?

Categorical encoders in Feature-engine are designed to work with variables of type
object by default. Thus, if you wish to encode the returned bins further, say to try and
obtain monotonic relationships between the variable and the target, you can do so
seamlessly by setting `return_object` to True. You can find an example of how to use
this functionality `here <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/discretisation/ArbitraryDiscretiser_plus_MeanEncoder.ipynb>`_.

Additional resources
--------------------

Check also:

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/discretisation/ArbitraryDiscretiser.ipynb>`_
- `Jupyter notebook - Discretiser plus Mean Encoding <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/discretisation/ArbitraryDiscretiser_plus_MeanEncoder.ipynb>`_

For more details about this and other feature engineering methods check out these resources:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/discretisation/DecisionTreeDiscretiser.rst
================================================
.. _decisiontree_discretiser:

.. currentmodule:: feature_engine.discretisation

DecisionTreeDiscretiser
=======================

Discretization consists of transforming continuous variables into discrete features by creating
a set of contiguous intervals, or bins, that span the range of the variable values.

Discretization is a common data preprocessing step in many data science projects, as it simplifies
continuous attributes and has the potential to improve model performance or speed up model training.﻿

Decision tree discretization
----------------------------

Decision trees make decisions based on discrete partitions over continuous features. During
training, a decision tree evaluates all possible feature values to find the best cut-point, that is,
the feature value at which the split maximizes the information gain, or in other words, reduces the
impurity. It repeats the procedure at each node until it allocates all samples to certain leaf
nodes or end nodes. Hence, classification and regression trees can naturally find the optimal limits
of the intervals to maximize class coherence.

Discretization with decision trees consists of using a decision tree algorithm to identify the optimal
partitions for each continuous variable. After finding the optimal partitions, we sort the variable's
values into those intervals.

Discretization with decision trees is a supervised discretization method, in that, the interval
limits are found based on class or target coherence. In simpler words, we need the target variable
to train the decision trees.

Advantages
~~~~~~~~~~

- The output returned by the decision tree is monotonically related to the target.
- The tree end nodes, or bins, show decreased entropy, that is, the observations within each bin are more similar among themselves than to those of other bins.

Limitations
~~~~~~~~~~~

- Could cause over-fitting
- We need to tune some of the decision tree parameters to obtain the optimal number of intervals.

Decision tree discretizer
-------------------------

The :class:`DecisionTreeDiscretiser()` applies discretization based on the interval limits found
by decision trees algorithms. It uses decision trees to find the optimal interval limits. Next,
it sorts the variable into those intervals.

The transformed variable can either have the limits of the intervals as values, an ordinal number
representing the interval into which the value was sorted, or alternatively, the prediction of the
decision tree. In any case, the number of values of the variable will be finite.

In theory, decision tree discretization creates discrete variables with a monotonic relationship
with the target, and hence, the transformed features would be more suitable to train linear models,
like linear or logistic regression.

Original idea
-------------

The method of decision tree discretization is based on the winning solution of the KDD 2009 competition:

`Niculescu-Mizil, et al. "Winning the KDD Cup Orange Challenge with Ensemble
Selection". JMLR: Workshop and Conference Proceedings 7: 23-34. KDD 2009
<http:

In the original article, each feature in the dataset was re-coded by training a decision tree of limited
depth (2, 3 or 4) using that feature alone, and letting the tree predict the target. The probabilistic
predictions of this decision tree were used as an additional feature that was now linearly (or at least
monotonically) related with the target.

According to the authors, the addition of these new features had a significant impact
on the performance of linear models.

Code examples
-------------

In the following sections, we will do decision tree discretization to showcase the functionality of
the :class:`DecisionTreeDiscretiser()`. We will discretize 2 numerical variables of the Ames house
prices dataset using decision trees.

First, we will transform the variables using the predictions of the decision trees, next, we will
return the interval limits, and finally, we will return the bin order.

Discretization with the predictions of the decision tree
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

First we load the data and separate it into a training set and a test set:

.. code:: python

    from sklearn.datasets import fetch_openml
    from sklearn.model_selection import train_test_split

    data = fetch_openml(name='house_prices', as_frame=True)
    data = data.frame

    X = data.drop(['SalePrice', 'Id'], axis=1)
    y = data['SalePrice']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42)

    print(X_train.head())

In the following output we see the predictor variables of the house prices dataset:

.. code:: python

          MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \
    254           20       RL         70.0     8400   Pave   NaN      Reg
    1066          60       RL         59.0     7837   Pave   NaN      IR1
    638           30       RL         67.0     8777   Pave   NaN      Reg
    799           50       RL         60.0     7200   Pave   NaN      Reg
    380           50       RL         50.0     5000   Pave  Pave      Reg

         LandContour Utilities LotConfig  ... ScreenPorch PoolArea PoolQC  Fence  \
    254          Lvl    AllPub    Inside  ...           0        0    NaN    NaN
    1066         Lvl    AllPub    Inside  ...           0        0    NaN    NaN
    638          Lvl    AllPub    Inside  ...           0        0    NaN  MnPrv
    799          Lvl    AllPub    Corner  ...           0        0    NaN  MnPrv
    380          Lvl    AllPub    Inside  ...           0        0    NaN    NaN

         MiscFeature MiscVal  MoSold  YrSold  SaleType  SaleCondition
    254          NaN       0       6    2010        WD         Normal
    1066         NaN       0       5    2009        WD         Normal
    638          NaN       0       5    2008        WD         Normal
    799          NaN       0       6    2007        WD         Normal
    380          NaN       0       5    2010        WD         Normal

    [5 rows x 79 columns]

We set up the decision tree discretiser to find the optimal intervals using decision trees.

The :class:`DecisionTreeDiscretiser()` will optimize the depth of the decision tree classifier
or regressor by default and using cross-validation. That's why we need to select the appropriate
metric for the optimization. In this example, we are using decision tree regression, so we select
the mean squared error metric.

We specify in the `bin_output` that we want to replace the continuous attribute values with the
predictions of the decision tree.

.. code:: python

   from feature_engine.discretisation import DecisionTreeDiscretiser

   disc = DecisionTreeDiscretiser(bin_output="prediction",
                                  cv=3,
                                  scoring='neg_mean_squared_error',
                                  variables=['LotArea', 'GrLivArea'],
                                  regression=True)

   disc.fit(X_train, y_train)

The scoring and cv parameter work exactly as those from any scikit-learn estimator. So we can pass
any value that is also valid for those estimators. Check scikit-learn's documentation for more information.

With `fit()` the transformer fits a decision tree for each one of the continuous features. Then,
we can go ahead replace the variable values by the predictions of the trees and display the transformed
variables:

.. code:: python

   train_t = disc.transform(X_train)
   test_t = disc.transform(X_test)

   print(train_t[['LotArea', 'GrLivArea']].head())

In this case, the original values were replaced with the predictions of each one of the decision trees:

.. code:: python

                LotArea      GrLivArea
    254   144174.283688  152471.713568
    1066  144174.283688  191760.966667
    638   176117.741848   97156.250000
    799   144174.283688  202178.409091
    380   144174.283688  202178.409091

Decision trees make discrete predictions, that's why we'll see a limited number of values in the
transformed variables:

.. code:: python

    train_t[['LotArea', 'GrLivArea']].nunique()

.. code:: python

    LotArea       4
    GrLivArea    16
    dtype: int64

The `binner_dict_` stores the details of each decision tree.

.. code:: python

   disc.binner_dict_

.. code:: python

    {'LotArea': GridSearchCV(cv=3, estimator=DecisionTreeRegressor(),
                  param_grid={'max_depth': [1, 2, 3, 4]},
                  scoring='neg_mean_squared_error'),
     'GrLivArea': GridSearchCV(cv=3, estimator=DecisionTreeRegressor(),
                  param_grid={'max_depth': [1, 2, 3, 4]},
                  scoring='neg_mean_squared_error')}

With decision tree discretization, each bin, that is, each prediction value in this case, does not
necessarily contain the same number of observations. Let's check that out with a visualization:

.. code:: python

   import matplotlib.pyplot as plt
   train_t.groupby('GrLivArea')['GrLivArea'].count().plot.bar()
   plt.ylabel('Number of houses')
   plt.show()

.. image:: ../../images/treediscretisation.png

Finally, we can determine if we have a monotonic relationship with the target after the transformation:

.. code:: python

    plt.scatter(test_t['GrLivArea'], y_test)
    plt.xlabel('GrLivArea')
    plt.ylabel('Sale Price')
    plt.show()

.. image:: ../../images/treemonotonicprediction.png

Rounding the prediction value
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Sometimes, the output of the prediction can have multiple values after the comma, which makes the
visualization and interpretation a bit uncomfortable. Fortunately, we can round those values through
the `precision` parameter:

.. code:: python

    disc = DecisionTreeDiscretiser(
        bin_output="prediction",
        precision=1,
        cv=3,
        scoring='neg_mean_squared_error',
        variables=['LotArea', 'GrLivArea'],
        regression=True)

    disc.fit(X_train, y_train)

    train_t= disc.transform(X_train)
    test_t= disc.transform(X_test)

    train_t.groupby('GrLivArea')['GrLivArea'].count().plot.bar()
    plt.ylabel('Number of houses')
    plt.show()

.. image:: ../../images/treepredictionrounded.png

In this example, we are predicting house prices, which is a continuous target. The procedure for
classification models is identical, we just need to set the parameter `regression` to False.

Discretization with interval limits
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In this section, instead of replacing the original variable values with the predictions of the
decision tree, we will return the limits of the intervals. When returning interval boundaries,
we need to set the precision to a positive integer.

.. code:: python

    disc = DecisionTreeDiscretiser(
        bin_output="boundaries",
        precision=3,
        cv=3,
        scoring='neg_mean_squared_error',
        variables=['LotArea', 'GrLivArea'],
        regression=True)

    # fit the transformer
    disc.fit(X_train, y_train)

In this case, when we explore the `binner_dict_` attribute, we will see the interval limits instead
of the decision trees:

.. code:: python

    disc.binner_dict_

.. code:: python

    {'LotArea': [-inf, 8637.5, 10924.0, 13848.5, inf],
     'GrLivArea': [-inf,
      749.5,
      808.0,
      1049.0,
      1144.5,
      1199.0,
      1413.0,
      1438.5,
      1483.0,
      1651.5,
      1825.0,
      1969.5,
      2386.0,
      2408.0,
      2661.0,
      4576.0,
      inf]}

The :class:`DecisionTreeDiscretiser()` will use these limits with `pandas.cut` to discretize the
continuous variable values during transform:

.. code:: python

    train_t = disc.transform(X_train)
    test_t = disc.transform(X_test)

    print(train_t[['LotArea', 'GrLivArea']].head())

In the following output we see the interval limits into which the values of the continuous attributes were sorted:

.. code:: python

                    LotArea         GrLivArea
    254      (-inf, 8637.5]  (1199.0, 1413.0]
    1066     (-inf, 8637.5]  (1483.0, 1651.5]
    638   (8637.5, 10924.0]    (749.5, 808.0]
    799      (-inf, 8637.5]  (1651.5, 1825.0]
    380      (-inf, 8637.5]  (1651.5, 1825.0]

To train machine learning algorithms we would follow that up with any categorical data encoding method.

Discretization with ordinal numbers
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the last part of this guide, we will replace the variable values with the number of bin into
which the value was sorted. Here, 0 is the first bin, 1 the second, and so on.

.. code:: python

    disc = DecisionTreeDiscretiser(
        bin_output="bin_number",
        cv=3,
        scoring='neg_mean_squared_error',
        variables=['LotArea', 'GrLivArea'],
        regression=True,
    )

    # fit the transformer
    disc.fit(X_train, y_train)

The `binner_dict_` will also contain the limits of the intervals:

.. code:: python

    disc.binner_dict_

.. code:: python

    {'LotArea': [-inf, 8637.5, 10924.0, 13848.5, inf],
     'GrLivArea': [-inf,
      749.5,
      808.0,
      1049.0,
      1144.5,
      1199.0,
      1413.0,
      1438.5,
      1483.0,
      1651.5,
      1825.0,
      1969.5,
      2386.0,
      2408.0,
      2661.0,
      4576.0,
      inf]}

When we apply transform, :class:`DecisionTreeDiscretiser()` will use these limits with `pandas.cut` to
discretize the continuous variable:

.. code:: python

    train_t = disc.transform(X_train)
    test_t = disc.transform(X_test)

    print(train_t[['LotArea', 'GrLivArea']].head())

In the following output we see the interval numbers into which the values of the continuous attributes
were sorted:

.. code:: python

          LotArea  GrLivArea
    254         0          5
    1066        0          8
    638         1          1
    799         0          9
    380         0          9

Additional considerations
-------------------------

Decision tree discretization uses scikit-learn's DecisionTreeRegressor or DecisionTreeClassifier under
the hood to find the optimal interval limits. These models do not support missing data. Hence, we need
to replace missing values with numbers before proceeding with the disrcretization.

Tutorials, books and courses
----------------------------

Check also for more details on how to use this transformer:

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/discretisation/DecisionTreeDiscretiser.ipynb>`_
- `tree_pipe in cell 21 of this Kaggle kernel <https://www.kaggle.com/solegalli/feature-engineering-and-model-stacking>`_

For tutorials about this and other discretization methods and feature engineering techniques check out our online course:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/discretisation/EqualFrequencyDiscretiser.rst
================================================
.. _equal_freq_discretiser:

.. currentmodule:: feature_engine.discretisation

EqualFrequencyDiscretiser
=========================

Equal frequency discretization consists of dividing continuous attributes into equal-frequency bins. These bins
contain roughly the same number of observations, with boundaries set at specific quantile values determined by the desired
number of bins.

Equal frequency discretization ensures a uniform distribution of data points across the range of values, enhancing the
handling of skewed data and outliers.

Discretization is a common data preprocessing technique used in data science. It's also known as binning data (or simply "binning").

Advantages and Limitations
--------------------------

Equal frequency discretization has some advantages and shortcomings:

Advantages
~~~~~~~~~~

Some advantages of equal frequency binning:

- **Algorithm Efficiency:** Enhances the performance of data mining and machine learning algorithms by providing a simplified representation of the dataset.
- **Outlier Management:** Efficiently mitigates the effect of outliers by grouping them into the extreme bins.
- **Data Smoothing:** Helps smooth the data, reduces noise, and improves the model's ability to generalize.
- **Improved value distribution:** Returns an uniform distribution of values across the value range.

Equal frequency discretization improves the data distribution, **optimizing the spread of values**. This is particularly
beneficial for datasets with skewed distributions (see the Python example code).

Limitations
~~~~~~~~~~~

On the other hand, equal frequency binning can lead to a loss of information by aggregating data into broader categories.
This is particularly concerning if the data in the same bin has predictive information about the target.

Let's consider a binary classifier task using a decision tree model. A bin with a high proportion of both target categories
would potentially impact the model's performance in this scenario.

EqualFrequencyDiscretiser
-------------------------

Feature-engine's :class:`EqualFrequencyDiscretiser` applies equal frequency discretization to numerical variables. It uses
the `pandas.qcut()` function under the hood, to determine the interval limits.

You can specify the variables to be discretized by passing their names in a list when you set up the transformer. Alternatively,
:class:`EqualFrequencyDiscretiser` will automatically infer the data types to compute the interval limits for all numeric variables.

**Optimal number of intervals:** With :class:`EqualFrequencyDiscretiser`, the user defines the number of bins. Smaller intervals
may be required if the variable is highly skewed or not continuous.

**Integration with scikit-learn:** :class:`EqualFrequencyDiscretiser` and all other feature-engine transformers seamlessly integrate
with scikit-learn `pipelines <https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html>`_.

Python code example
-------------------

In this section, we'll show the main functionality of :class:`EqualFrequencyDiscretiser`

Load dataset
~~~~~~~~~~~~

In this example, we'll use the Ames House Prices' Dataset. First, let's load the dataset and split it into train and
test sets:

.. code:: python

	import matplotlib.pyplot as plt
	from sklearn.datasets import fetch_openml
	from sklearn.model_selection import train_test_split

	from feature_engine.discretisation import EqualFrequencyDiscretiser

	# Load dataset
	X, y = fetch_openml(name='house_prices', version=1, return_X_y=True, as_frame=True)
	X.set_index('Id', inplace=True)

	# Separate into train and test sets
	X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.3, random_state=42)

Equal-frequency Discretization
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In this example, let's discretize two variables, LotArea and GrLivArea, into 10 intervals of approximately equal
number of observations.

.. code:: python
	
	# List the target numeric variables to be transformed
	TARGET_NUMERIC_FEATURES= ['LotArea','GrLivArea']

	# Set up the discretization transformer
	disc = EqualFrequencyDiscretiser(q=10, variables=TARGET_NUMERIC_FEATURES)

	# Fit the transformer
	disc.fit(X_train)

Note that if we do not specify the variables (default=`None`), :class:`EqualFrequencyDiscretiser` will automatically
infer the data types to compute the interval limits for all numeric variables.

With the `fit()` method, the discretizer learns the bin boundaries and saves them into a dictionary so we can use them
to transform unseen data:

.. code:: python

	# Learned limits for each variable
	disc.binner_dict_

.. code:: python

	{'LotArea': [-inf,
	  5000.0,
	  7105.6,
	  8099.200000000003,
	  8874.0,
	  9600.0,
	  10318.400000000001,
	  11173.5,
	  12208.2,
	  14570.699999999999,
	  inf],
	 'GrLivArea': [-inf,
	  918.5,
	  1080.4,
	  1218.0,
	  1348.4,
	  1476.5,
	  1601.6000000000001,
	  1717.6999999999998,
	  1893.0000000000005,
	  2166.3999999999996,
	  inf]}

Note that the lower and upper boundaries are set to -inf and inf, respectively. his behavior ensures that the transformer
will be able to allocate to the extreme bins values that are smaller or greater than the observed minimum and maximum
values in the training set.

:class:`EqualFrequencyDiscretiser` will not work in the presence of missing values. Therefore, we should either remove or
impute missing values before fitting the transformer.

.. code:: python

	# Transform the data
	train_t = disc.transform(X_train)
	test_t = disc.transform(X_test)

Let's visualize the first rows of the raw data and the transformed data:

.. code:: python

	# Raw data
	print(X_train[TARGET_NUMERIC_FEATURES].head())

Here we see the original variables:

.. code:: python

            LotArea  GrLivArea
    Id
    136     10400       1682
    1453     3675       1072
    763      8640       1547
    933     11670       1905
    436     10667       1661

.. code:: python

	# Transformed data
	print(train_t[TARGET_NUMERIC_FEATURES].head())

Here we observe the variables after discretization:

.. code:: python

		  LotArea  GrLivArea
	Id                      
	136         6          6
	1453        0          1
	763         3          5
	933         7          8
	436         6          6

The transformed data now contains discrete values corresponding to the ordered computed buckets (0 being the first and q-1 the last).

Now, let's visualize the plots for equal-width intervals with a histogram and the transformed data with equal-frequency discretiser:

.. code:: python

	# Instantiate a figure with two axes
	fig, axes = plt.subplots(ncols=2, figsize=(10,5))

	# Plot raw distribution
	X_train['GrLivArea'].plot.hist(bins=disc.q, ax=axes[0])
	axes[0].set_title('Raw data with equal width binning')
	axes[0].set_xlabel('GrLivArea')

	# Plot transformed distribution
	train_t['GrLivArea'].value_counts().sort_index().plot.bar(ax=axes[1])
	axes[1].set_title('Transformed data with equal frequency binning')

	plt.tight_layout(w_pad=2)
	plt.show()

As we see in the following image, the intervals contain approximately the same number of observations:

.. image:: ../../images/equalfrequencydiscretisation_gaussian.png

Finally, as the default value for the `return_object` parameter is `False`, the transformer outputs integer variables:

.. code:: python

	train_t[TARGET_NUMERIC_FEATURES].dtypes

.. code:: python

	LotArea      int64
	GrLivArea    int64
	dtype: object

Return variables as object
~~~~~~~~~~~~~~~~~~~~~~~~~~

Categorical encoders in Feature-engine are designed to work by default with variables of type object. Therefore, to further
encode the discretised output with Feature-engine, we can set `return_object=True` instead. This will return the transformed
variables as object.

Let's say we want to obtain monotonic relationships between the variable and the target. We can do that seamlessly by setting
`return_object` to True. A tutorial of how to use this functionality is available
`here <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/discretisation/EqualFrequencyDiscretiser_plus_WoEEncoder.ipynb>`_.

Return bin boundaries
~~~~~~~~~~~~~~~~~~~~~

If we want to output the intervals limits instead of integers, we can set `return_boundaries` to `True`:

.. code:: python

    # Set up the discretization transformer
    disc = EqualFrequencyDiscretiser(
        q=10,
        variables=TARGET_NUMERIC_FEATURES,
        return_boundaries=True)

    # Fit the transformer
    disc.fit(X_train)

    # Transform test set & visualize limit
    test_t = disc.transform(X_test)

    # Visualize output (boundaries)
    print(test_t[TARGET_NUMERIC_FEATURES].head())

The transformed variables now show the interval limits in the output. We can immediately see that the bin width for these
intervals varies. In other words, they don't have the same width, contrarily to what we see with :ref:`equal width discretization <equal_width_discretiser>`.

Unlike the variables discretized into integers, these variables cannot be used to train machine learning models; however,
they are still highly helpful for data analysis in this format, and they may be sent to any Feature-engine encoder for
additional processing.

.. code:: python

                  LotArea         GrLivArea
	Id                                        
	893     (8099.2, 8874.0]   (918.5, 1080.4]
	1106  (12208.2, 14570.7]     (2166.4, inf]
	414     (8874.0, 9600.0]   (918.5, 1080.4]
	523       (-inf, 5000.0]  (1601.6, 1717.7]
	1037  (12208.2, 14570.7]  (1601.6, 1717.7]

Binning skewed data
~~~~~~~~~~~~~~~~~~~

Let's now show the benefits of equal frequency discretization for skewed variables. We'll
start by importing the libraries and classes:

.. code:: python

	import numpy as np
	import pandas as pd
	import matplotlib.pyplot as plt
	from feature_engine.discretisation import EqualFrequencyDiscretiser

Now, we'll create a toy dataset with a variable that is normally distributed and another
one that is skewed:

.. code:: python

	# Set seed for reproducibility
	np.random.seed(42)

	# Generate a normally distributed data
	normal_data = np.random.normal(loc=0, scale=1, size=1000)

	# Generate a right-skewed data using exponential distribution
	skewed_data = np.random.exponential(scale=1, size=1000)

	# Create dataframe with simulated data
	X = pd.DataFrame({'feature1': normal_data, 'feature2': skewed_data})

Let's discretize both variables into 5 equal frequency bins:

.. code:: python

	# Instantiate discretizer
	disc = EqualFrequencyDiscretiser(q=5)

	# Transform simulated data
	X_transformed = disc.fit_transform(X)

Let's plot the original distribution and the distribution after discretization for the variable that was normally
distributed:

.. code:: python

	fig, axes = plt.subplots(1, 2, figsize=(12, 4))

	axes[0].hist(X.feature1, bins=disc.q)
	axes[0].set(xlabel='feature1', ylabel='count', title='Raw data')

	X_transformed.feature1.value_counts().sort_index().plot.bar(ax=axes[1])
	axes[1].set_title('Transformed data')

	plt.suptitle('Normal distributed data', weight='bold', size='large', y=1.05)

	plt.show()

In the following image, we see that after the discretization there is an even distribution of the values across
the value range, hence, the variable does no look normally distributed any more.

.. image:: ../../images/equalfrequencydiscretisation_gaussian.png

Let's now plot the original distribution and the distribution after discretization for the variable that was skewed:

.. code:: python

	fig, axes = plt.subplots(1, 2, figsize=(12, 4))

	axes[0].hist(X.feature2, bins=disc.q)
	axes[0].set(xlabel='feature2', ylabel='count', title='Raw data')

	X_transformed.feature2.value_counts().sort_index().plot.bar(ax=axes[1])
	axes[1].set_title('Transformed data')

	plt.suptitle('Skewed distributed data', weight='bold', size='large', y=1.05)

	plt.show()

In the following image, we see that after the discretization there is an even distribution of the values across
the value range.

.. image:: ../../images/equalfrequencydiscretisation_skewed.png

See Also
--------

For alternative binning techniques, check out the following resources:

- Further feature-engine :ref:`discretizers / binning methods <discretization_transformers>`
- Scikit-learn's `KBinsDiscretizer <https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html#sklearn.preprocessing.KBinsDiscretizer>`_.

Check out also:

- `Pandas qcut <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.qcut.html>`_.

Additional resources
--------------------

Check also for more details on how to use this transformer:

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/discretisation/EqualFrequencyDiscretiser.ipynb>`_
- `Jupyter notebook - Discretizer plus Weight of Evidence encoding <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/discretisation/EqualFrequencyDiscretiser_plus_WoEEncoder.ipynb>`_

For more details about this and other feature engineering methods check out these resources:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/discretisation/EqualWidthDiscretiser.rst
================================================
.. _equal_width_discretiser:

.. currentmodule:: feature_engine.discretisation

EqualWidthDiscretiser
=====================

Equal width discretization consist of dividing continuous variables into intervals of equal width, calculated
using the following formula:

.. math::

    bin_{width} = ( max(X) - min(X) ) / bins

Here, `bins` is the number of intervals specified by the user and `max(X)` and `min(X)` are the minimum and maximum values
of the variable to discretize.

Discretization is a common data preprocessing technique used in data science. It's also known as data binning (or simply
"binning").

Advantages and Limitations
--------------------------

Equal binning discretization has some advantages and also shortcomings.

Advantages
~~~~~~~~~~

Some advantages of equal width binning:

- **Algorithm Efficiency:** Enhances the performance of data mining and machine learning algorithms by providing a simplified representation of the dataset.
- **Outlier Management:** Efficiently mitigates the effect of outliers by grouping them into the extreme bins, thus preserving the integrity of the main data distribution.
- **Data Smoothing:** Helps smooth the data, reduces noise, and improves the model's ability to generalize.

Limitations
~~~~~~~~~~~

On the other hand, equal width discretzation can lead to a loss of information by aggregating data into broader categories.
This is particularly concerning if the data in the same bin has predictive information about the target.

Let's consider a binary classifier task using a decision tree model. A bin with a high proportion of both target categories would
potentially impact the model's performance in this scenario.

EqualWidthDiscretiser
---------------------

Feture-engine's :class:`EqualWidthDiscretiser()` applies equal width discretization to numerical variables. It uses
the `pandas.cut()` function under the hood to find the interval limits and then sort the continuous variables into
the bins.

You can specify the variables to be discretized by passing their names in a list when you set up the transformer. Alternatively,
:class:`EqualWidthDiscretiser()` will automatically infer the data types to compute the interval limits for all numeric
variables.

**Optimal number of intervals:** With :class:`EqualWidthDiscretiser()`, the user defines the number of bins. Smaller intervals	
may be required if the variable is highly skewed or not continuous.

**Integration with scikit-learn:** :class:`EqualWidthDiscretiser()` and all other Feature-engine transformers seamlessly
integrate with scikit-learn `pipelines <https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html>`_.

Python code example
-------------------

In this section, we'll show the main functionality of :class:`EqualWidthDiscretiser()`.

Load dataset
~~~~~~~~~~~~

In this example, we'll use the Ames House Prices' Dataset. First, let's load the dataset and split it into train and
test sets:

.. code:: python

	import matplotlib.pyplot as plt
	from sklearn.datasets import fetch_openml
	from sklearn.model_selection import train_test_split

	from feature_engine.discretisation import EqualFrequencyDiscretiser

	# Load dataset
	X, y = fetch_openml(name='house_prices', version=1, return_X_y=True, as_frame=True)
	X.set_index('Id', inplace=True)

	# Separate into train and test sets
	X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.3, random_state=42)

Equal-width Discretization
~~~~~~~~~~~~~~~~~~~~~~~~~~

In this example, let's discretize two variables, LotArea and GrLivArea, into 10 intervals of equal width:

.. code:: python

	# List the target numeric variables for equal-width discretization
	TARGET_NUMERIC_FEATURES= ['LotArea','GrLivArea']

	# Set up the discretization transformer
	disc = EqualWidthDiscretiser(bins=10, variables=TARGET_NUMERIC_FEATURES)

	# Fit the transformer
	disc.fit(X_train)

Note that if we do not specify the variables (default=`None`), :class:`EqualWidthDiscretiser` will automatically infer
the data types to compute the interval limits for all numeric variables.

With the `fit()` method, the discretizer learns the bin boundaries and saves them into a dictionary so we can use them
to transform unseen data:

.. code:: python

	# Learned limits for each variable
	disc.binner_dict_

.. code:: python

	{'LotArea': [-inf,
	  22694.5,
	  44089.0,
	  65483.5,
	  86878.0,
	  108272.5,
	  129667.0,
	  151061.5,
	  172456.0,
	  193850.5,
	  inf],
	 'GrLivArea': [-inf,
	  864.8,
	  1395.6,
	  1926.3999999999999,
	  2457.2,
	  2988.0,
	  3518.7999999999997,
	  4049.5999999999995,
	  4580.4,
	  5111.2,
	  inf]}

Note that the lower and upper boundaries are set to -inf and inf, respectively. This behavior ensures that the transformer
will be able to allocate to the extreme bins values that are smaller or greater than the observed minimum and maximum
values in the training set.

:class:`EqualWidthDiscretiser` will not work in the presence of missing values. Therefore, we should either remove or
impute missing values before fitting the transformer.

.. code:: python

	# Transform the data (data discretization)
	train_t = disc.transform(X_train)
	test_t = disc.transform(X_test)

Let's visualize the first rows of the raw data and the transformed data:

.. code:: python

	# Raw data
	print(X_train[TARGET_NUMERIC_FEATURES].head())

Here we see the original variables:

.. code:: python

		  LotArea  GrLivArea
	Id                      
	136     10400       1682
	1453     3675       1072
	763      8640       1547
	933     11670       1905
	436     10667       1661

.. code:: python

	# Transformed data
	print(train_t[TARGET_NUMERIC_FEATURES].head())

Here we observe the variables after discretization:

.. code:: python

		  LotArea  GrLivArea
	Id                      
	136         0          2
	1453        0          1
	763         0          2
	933         0          2
	436         0          2

The transformed data now contains discrete values corresponding to the ordered computed buckets (0 being the first and
bins-1 the last).

Now, let's check out the number of observations per bin by creating a bar plot:

.. code:: python

	train_t['GrLivArea'].value_counts().sort_index().plot.bar()
	plt.ylabel('Number of houses')
	plt.show()

As we see in the following image, the intervals contain different number of observations.  It's a similar output to a
histogram:

.. image:: ../../images/equalwidthdiscretisation.png

|

Equal width discretization does not improve the spread of values over the value range. If the variable is skewed, it will
still be skewed after the discretization.

Finally, since the default value for the `return_object` parameter is `False`, the transformer outputs integer variables:

.. code:: python

	train_t[TARGET_NUMERIC_FEATURES].dtypes

.. code:: python

	LotArea      int64
	GrLivArea    int64
	dtype: object

Return variables as object
~~~~~~~~~~~~~~~~~~~~~~~~~~

Categorical encoders in Feature-engine are designed to work by default with variables of type object. Therefore, to
further encode the discretized output with Feature-engine's encoders, we can set `return_object=True` instead. This will
return the transformed variables as object.

Let's say we want to obtain monotonic relationships between the variable and the target. We can do that seamlessly by
setting `return_object` to `True`. A tutorial of how to use this functionality is available
`here <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/discretisation/EqualWidthDiscretiser_plus_OrdinalEncoder.ipynb>`_.

Return bin boundaries
~~~~~~~~~~~~~~~~~~~~~

If we want to output the intervals limits instead of integers, we can set `return_boundaries` to `True`:

.. code:: python

    # Set up the discretization transformer
    disc = EqualFrequencyDiscretiser(
        bins=10,
        variables=TARGET_NUMERIC_FEATURES,
        return_boundaries=True)

    # Fit the transformer
    disc.fit(X_train)

    # Transform test set & visualize limit
    test_t = disc.transform(X_test)

    # Visualize output (boundaries)
    print(test_t[TARGET_NUMERIC_FEATURES].head())

In the following output we see that the transformed variables now display the interval limits. While we can't use these
variables to train machine learning models, as opposed to the variables discretized into integers, they are very useful
in this format for data analysis, and they can also be passed on to any Feature-engine encoder for further processing.

.. code:: python

		     LotArea         GrLivArea
	Id                                     
	893   (-inf, 22694.5]   (864.8, 1395.6]
	1106  (-inf, 22694.5]  (2457.2, 2988.0]
	414   (-inf, 22694.5]   (864.8, 1395.6]
	523   (-inf, 22694.5]  (1395.6, 1926.4]
	1037  (-inf, 22694.5]  (1395.6, 1926.4]

See Also
--------

For alternative binning techniques, check out the following resources:

- Further feature-engine :ref:`discretizers / binning methods <discretization_transformers>`
- Scikit-learn's `KBinsDiscretizer <https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html#sklearn.preprocessing.KBinsDiscretizer>`_.

Check out also:

- `Pandas cut <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html>`_.

Additional resources
--------------------

Check also for more details on how to use this transformer:

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/discretisation/EqualWidthDiscretiser.ipynb>`_
- `Jupyter notebook - Discretizer plus Ordinal encoding <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/discretisation/EqualWidthDiscretiser_plus_OrdinalEncoder.ipynb>`_

For more details about this and other feature engineering methods check out these resources:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/discretisation/GeometricWidthDiscretiser.rst
================================================
.. _increasing_width_discretiser:

.. currentmodule:: feature_engine.discretisation

GeometricWidthDiscretiser
=========================

The :class:`GeometricWidthDiscretiser()` divides continuous numerical variables into
intervals of increasing width. The width of each succeeding interval is larger than the
previous interval by a constant amount (cw).

The constant amount is calculated as:

    .. math::
        cw = (Max - Min)^{1/n}

were Max and Min are the variable's maximum and minimum value, and n is the number of
intervals.

The sizes of the intervals themselves are calculated with a geometric progression:

    .. math::
        a_{i+1} = a_i cw

Thus, the first interval's width equals cw, the second interval's width equals 2 * cw,
and so on.

Note that the proportion of observations per interval may vary.

This discretisation technique is great when the distribution of the variable is right skewed.

Note: The width of some bins might be very small. Thus, to allow this transformer
to work properly, it might help to increase the precision value, that is,
the number of decimal values allowed to define each bin. If the variable has a
narrow range or you are sorting into several bins, allow greater precision
(i.e., if precision = 3, then 0.001; if precision = 7, then 0.0001).

The :class:`GeometricWidthDiscretiser()` works only with numerical variables. A list of
variables to discretise can be indicated, or the discretiser will automatically select
all numerical variables in the train set.

**Example**

Let's look at an example using the house prices dataset (more details about the
dataset :ref:`here <datasets>`).

Let's load the house prices dataset and separate it into train and test sets:

.. code:: python

	import numpy as np
	import pandas as pd
	import matplotlib.pyplot as plt
	from sklearn.model_selection import train_test_split

	from feature_engine.discretisation import GeometricWidthDiscretiser

	# Load dataset
	data = pd.read_csv('houseprice.csv')

	# Separate into train and test sets
	X_train, X_test, y_train, y_test =  train_test_split(
		    data.drop(['Id', 'SalePrice'], axis=1),
		    data['SalePrice'], test_size=0.3, random_state=0)

Now, we want to discretise the 2 variables indicated below into 10 intervals of increasing
width:

.. code:: python

	# set up the discretisation transformer
	disc = GeometricWidthDiscretiser(bins=10, variables=['LotArea', 'GrLivArea'])

	# fit the transformer
	disc.fit(X_train)

With `fit()` the transformer learns the boundaries of each interval. Then, we can go
ahead and sort the values into the intervals:

.. code:: python

	# transform the data
	train_t= disc.transform(X_train)
	test_t= disc.transform(X_test)

The `binner_dict_` stores the interval limits identified for each variable.

.. code:: python

	disc.binner_dict_

.. code:: python

	'LotArea': [-inf,
        1303.412,
        1311.643,
        1339.727,
        1435.557,
        1762.542,
        2878.27,
        6685.32,
        19675.608,
        64000.633,
        inf],
	'GrLivArea': [-inf,
        336.311,
        339.34,
        346.34,
        362.515,
        399.894,
        486.27,
        685.871,
        1147.115,
        2212.974,
        inf]}

With increasing width discretisation, each bin does not necessarily contain the same number
of observations. This transformer is suitable for variables with right skewed distributions.

Let's compare the variable distribution before and after the discretization:

.. code:: python

    fig, ax = plt.subplots(1, 2)
    X_train['LotArea'].hist(ax=ax[0], bins=10);
    train_t['LotArea'].hist(ax=ax[1], bins=10);

We can see below that the intervals contain different number of observations. We can also
see that the shape from the distribution changed from skewed to a more "bell shaped"
distribution.

.. image:: ../../images/increasingwidthdisc.png

|

**Discretisation plus encoding**

If we return the interval values as integers, the discretiser has the option to return
the transformed variable as integer or as object. Why would we want the transformed
variables as object?

Categorical encoders in Feature-engine are designed to work with variables of type
object by default. Thus, if you wish to encode the returned bins further, say to try and
obtain monotonic relationships between the variable and the target, you can do so
seamlessly by setting `return_object` to True. You can find an example of how to use
this functionality `here <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/discretisation/GeometricWidthDiscretiser_plus_MeanEncoder.ipynb>`_.

Additional resources
--------------------

Check also for more details on how to use this transformer:

- `Jupyter notebook - Geometric Discretiser <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/discretisation/GeometricWidthDiscretiser.ipynb>`_
- `Jupyter notebook - Geometric Discretiser plus Mean encoding <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/discretisation/GeometricWidthDiscretiser_plus_MeanEncoder.ipynb>`_

For more details about this and other feature engineering methods check out these resources:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/discretisation/index.rst
================================================
.. _discretization_transformers:

.. -*- mode: rst -*-

Discretisation
==============

Feature-engine's variable discretisation transformers transform continuous numerical
variables into discrete variables. The discrete variables will contain contiguous
intervals in the case of the equal frequency and equal width transformers. The
Decision Tree discretiser will return a discrete variable, in the sense that the
new feature takes a finite number of values.

The following illustration shows the process of discretisation:

.. figure::  ../../images/Discretisation.png
   :align:   center
   :width: 500

With discretisation, sometimes we can obtain a more homogeneous value spread from an
originally skewed variable. But this is not always possible.

**Discretisation plus encoding**

Very often, after we discretise the numerical continuous variables into discrete intervals
we want to proceed their engineering as if they were categorical. This is common practice.
Throughout the user guide, we point to jupyter notebooks that showcase this functionality.

**Discretisers**

.. toctree::
   :maxdepth: 1

   EqualFrequencyDiscretiser
   EqualWidthDiscretiser
   ArbitraryDiscretiser
   DecisionTreeDiscretiser
   GeometricWidthDiscretiser

================================================
FILE: docs/user_guide/encoding/CountFrequencyEncoder.rst
================================================
.. _count_freq_encoder:

.. currentmodule:: feature_engine.encoding

CountFrequencyEncoder
=====================

Count encoding and frequency encoding are 2 categorical encoding techniques that were
commonly used during data preprocessing in Kaggle's data science competitions, even when
their predictive value is not immediately obvious.

Count encoding consists of replacing the categories of categorical features by their
counts, which are estimated from the training set. For example, in the variable color,
if 10 observations are blue and 5 observations are red, blue will be replaced by 10 and
red by 5.

Frequency encoding consists of replacing the labels of categorical data with their
frequency, which is also estimated from the training set. Then, in the variable City,
if London appears in 10% of the observations and Bristol in 1%, London will be replaced
by 0.1 and Bristol with 0.01.

Count and frequency encoding in machine learning
------------------------------------------------

We'd use count encoding or frequency encoding when we think that the representation of
the categories in the dataset has some sort of predictive value. To be honest, the only
example that I can think of where count encoding could be useful is in sales forecasting
or sales data analysis scenarios, where the count of a product or an item represents its
popularity. In other words, we may be more likely to sell a product with a high count.

Count encoding and frequency encoding can be suitable for categorical variables with high
cardinality because these types of categorical encoding will cause what is called
collisions: categories that are present in a similar number of observations will be
replaced with similar, if not the same values, which reduces the variability.

This, of course, can result in the loss of information by placing two categories that
are otherwise different in the same pot. But on the other hand, if we are using count
encoding or frequency encoding, we have reasons to believe that the count or the frequency
are a good indicator of predictive performance or somehow capture data insight, so that
categories with similar counts would show similar patterns or behaviors.

Count and Frequency encoding with Feature-engine
------------------------------------------------

The :class:`CountFrequencyEncoder()` replaces categories of categorical features by
either the count or the percentage of observations each category shows in the training set.

With :class:`CountFrequencyEncoder()` we can automatically encode all categorical
features in the dataset, or only a subset of them, by passing the variable names in a
list to the `variables` parameter when we set up the encoder.

By default, :class:`CountFrequencyEncoder()` will encode only categorical data. If we
want to encode numerical values, we need to explicitly say so by setting the parameter
`ignore_format` to True.

Count and frequency encoding with unseen categories
---------------------------------------------------

When we learn mappings from strings to numbers, either with count encoding or other
encoding techniques like ordinal encoding or target encoding, we do so by observing the
categories in the training set. Hence, we won't have mappings for categories that appear
only in the test set. These are the so-called "unseen categories."

When encountering unseen categories, :class:`CountFrequencyEncoder()` will ignore them
by default, which means that unseen categories will be replaced with missing values.
We can instruct the encoder to raise an error when a new category is encountered, or
alternatively, to encode unseen categories with zero.

Count encoding vs other encoding methods
----------------------------------------

Count and frequency encoding, similar to ordinal encoding and contrarily to one-hot
encoding, feature hashing, or binary encoding, does not increase the dataset dimensionality.
From one categorical variable, we obtain one numerical feature.

Python example
--------------

Let's examine the functionality of :class:`CountFrequencyEncoder()` by using the Titanic
dataset. We'll start by loading the libraries and functions, loading the dataset, and then
splitting it into a training and a testing set.

.. code:: python

    from sklearn.model_selection import train_test_split
    from feature_engine.datasets import load_titanic
    from feature_engine.encoding import CountFrequencyEncoder

    X, y = load_titanic(
        return_X_y_frame=True,
        handle_missing=True,
        predictors_only=True,
        cabin="letter_only",
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=0,
    )

    print(X_train.head())

We see the resulting dataframe with the predictor variables below:

.. code:: python

          pclass     sex        age  sibsp  parch     fare cabin embarked
    501        2  female  13.000000      0      1  19.5000     M        S
    588        2  female   4.000000      1      1  23.0000     M        S
    402        2  female  30.000000      1      0  13.8583     M        C
    1193       3    male  29.881135      0      0   7.7250     M        Q
    686        3  female  22.000000      0      0   7.7250     M        Q

This dataset has three obvious categorical features: cabin, embarked, and sex, and in
addition, pclass could also be handled as a categorical.

Count encoding
~~~~~~~~~~~~~~

We'll start by encoding the three categorical variables using their counts, that is,
replacing the strings with the number of times each category is present in the training
dataset.

.. code:: python

    encoder = CountFrequencyEncoder(
    encoding_method='count',
    variables=['cabin', 'sex', 'embarked'],
    )

    encoder.fit(X_train)

With `fit()`, the count encoder learns the counts of each category. We can inspect the
counts as follows:

.. code:: python

    encoder.encoder_dict_

We see the counts of each category for each of the three variables in the following output:

.. code:: python

    {'cabin': {'M': 702,
      'C': 71,
      'B': 42,
      'E': 32,
      'D': 32,
      'A': 17,
      'F': 15,
      'G': 4,
      'T': 1},
     'sex': {'male': 581, 'female': 335},
     'embarked': {'S': 652, 'C': 179, 'Q': 83, 'Missing': 2}}

Now, we can go ahead and encode the variables:

.. code:: python

    train_t = encoder.transform(X_train)
    test_t = encoder.transform(X_test)

    print(train_t.head())

We see the resulting dataframe where the categorical features are now replaced with
integer values corresponding to the category counts:

.. code:: python

          pclass  sex        age  sibsp  parch     fare  cabin  embarked
    501        2  335  13.000000      0      1  19.5000    702       652
    588        2  335   4.000000      1      1  23.0000    702       652
    402        2  335  30.000000      1      0  13.8583    702       179
    1193       3  581  29.881135      0      0   7.7250    702        83
    686        3  335  22.000000      0      0   7.7250    702        83

We can now use the encoded dataframes to train machine learning models.

Frequency encoding
~~~~~~~~~~~~~~~~~~

Let's now perform frequency encoding. We'll encode 2 categorical and 1 numerical variable,
hence, we need to set the encoder to ignore the variable's type:

.. code:: python

    encoder = CountFrequencyEncoder(
    encoding_method='frequency',
    variables=['cabin', 'pclass', 'embarked'],
    ignore_format=True,
    )

Now, we fit the frequency encoder to the train set and transform it straightaway, and
then we transform the test set:

.. code:: python

    t_train = encoder.fit_transform(X_train)
    t_test = encoder.transform(X_test)

    test.head()

In the following output we see the transformed dataframe, where the categorical features
are now encoded into their frequencies:

.. code:: python

            pclass     sex        age  sibsp  parch     fare     cabin  embarked
    1139  0.543668    male  38.000000      0      0   7.8958  0.766376   0.71179
    533   0.205240  female  21.000000      0      1  21.0000  0.766376   0.71179
    459   0.205240    male  42.000000      1      0  27.0000  0.766376   0.71179
    1150  0.543668    male  29.881135      0      0  14.5000  0.766376   0.71179
    393   0.205240    male  25.000000      0      0  31.5000  0.766376   0.71179

With `fit()` the encoder learns the frequencies of each category, which are stored in
its `encoder_dict_` parameter. We can inspect them like this:

.. code:: python

   encoder.encoder_dict_

In the `encoder_dict_` we find the frequencies for each one of the unique categories of
each variable to encode. This way, we can map the original value to the new value.

.. code:: python

   {'cabin': {'M': 0.7663755458515283,
      'C': 0.07751091703056769,
      'B': 0.04585152838427948,
      'E': 0.034934497816593885,
      'D': 0.034934497816593885,
      'A': 0.018558951965065504,
      'F': 0.016375545851528384,
      'G': 0.004366812227074236,
      'T': 0.001091703056768559},
   'pclass': {3: 0.5436681222707423,
      1: 0.25109170305676853,
      2: 0.2052401746724891},
   'embarked': {'S': 0.7117903930131004,
      'C': 0.19541484716157206,
      'Q': 0.0906113537117904,
      'Missing': 0.002183406113537118}}

We can now use these dataframes to train machine learning algorithms.

With the method `inverse_transform`, we can transform the encoded dataframes back to their
original representation, that is, we can replace the encoding with the original categorical
values.

Additional resources
--------------------

In the following notebook, you can find more details into the :class:`CountFrequencyEncoder()`
functionality and example plots with the encoded variables:

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/encoding/CountFrequencyEncoder.ipynb>`_

For more details about this and other feature engineering methods check out these resources:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

.. figure::  ../../images/fetsf.png
   :width: 300
   :figclass: align-center
   :align: right
   :target: https:

   Feature Engineering for Time Series Forecasting

|
|
|
|
|
|
|
|
|
|

Our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and courses are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/encoding/DecisionTreeEncoder.rst
================================================
.. _decisiontree_encoder:

.. currentmodule:: feature_engine.encoding

DecisionTreeEncoder
===================

Categorical encoding is the process of transforming the strings of categorical features
into numbers. Common procedures replace categories with ordinal numbers, counts,
frequencies, or the target mean value.

We can also replace the categories with the predictions made by a decision tree based
on that category value.

The process consists of fitting a decision tree using a single feature to predict the
target. The decision tree will try to find a relationship between these variables, if
one exists, and then we'll use the predictions as mappings to replace the categories.

The advantage of this procedure is that it captures some information about the relationship
between the variables during the encoding. And if there is a relationship between the
categorical feature and the target, the resulting encoded variable would have a monotonic
relationship with the target, which can be useful for linear models.

On the downside, it could cause overfitting, and it adds computational complexity to the
pipeline because we are fitting a tree per feature. If you plan to encode your features
with decision trees, make sure you have appropriate validation strategies and train the
decision trees with regularization.

DecisionTreeEncoder
-------------------

The :class:`DecisionTreeEncoder()` replaces categories in the variable with
the predictions of a decision tree.

The :class:`DecisionTreeEncoder()`  uses Scikit-learn's decision trees under the hood.
As these models can't handle non-numerical data, the :class:`DecisionTreeEncoder()` first
replaces the categories with ordinal numbers and then fits the trees.

You have the option to encode the categorical values into integers assigned arbitrarily
or ordered based on the mean target value per category (for more details, check the
:class:`OrdinalEncoder()`, which is used by :class:`DecisionTreeEncoder()` under the hood).
You can regulate this behaviour with the parameter `encoding_method`. As decision trees
are able to pick non-linear relationships, replacing categories with arbitrary numbers
should be enough in practice.

After this, the transformer fits a decision tree using this numerical variable to predict
the target. Finally, the original categorical variable is replaced by the
predictions of the decision tree.

In the attribute `encoding_dict_` you'll find the mappings from category to numerical
value. The category is the original value and the numerical value is the prediction
of the decision tree for the category.

The motivation for the :class:`DecisionTreeEncoder()` is to try and create monotonic
relationships between the categorical variables and the target.

Python example
--------------

Let's look at an example using the Titanic Dataset. First, let's load the data and
separate it into train and test:

.. code:: python

    from sklearn.model_selection import train_test_split
    from feature_engine.datasets import load_titanic
    from feature_engine.encoding import DecisionTreeEncoder

    X, y = load_titanic(
        return_X_y_frame=True,
        handle_missing=True,
        predictors_only=True,
        cabin="letter_only",
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=0,
    )

    print(X_train[['cabin', 'pclass', 'embarked']].head(10))

We will encode the following categorical variables:

.. code:: python

        cabin  pclass embarked
    501      M       2        S
    588      M       2        S
    402      M       2        C
    1193     M       3        Q
    686      M       3        Q
    971      M       3        Q
    117      E       1        C
    540      M       2        S
    294      C       1        C
    261      E       1        S

We set up the encoder to encode the variables above with 3 fold cross-validation, using
a grid search to find the optimal depth of the decision tree (this is the default
behaviour of the :class:`DecisionTreeEncoder()`). In this example, we optimize the
tree using the roc-auc metric.

.. code:: python

    encoder = DecisionTreeEncoder(
        variables=['cabin', 'pclass', 'embarked'],
        regression=False,
        scoring='roc_auc',
        cv=3,
        random_state=0,
        ignore_format=True)

    encoder.fit(X_train, y_train)

With `fit()` the :class:`DecisionTreeEncoder()` fits 1 decision tree per variable. The
mappings are stored in the `encoding_dict_`:

.. code:: python

    encoder.encoder_dict_

In the following output we see the values that will be used to replace each category
in each variable:

.. code:: python

    {'cabin': {'M': 0.30484330484330485,
      'E': 0.6116504854368932,
      'C': 0.6116504854368932,
      'D': 0.6981132075471698,
      'B': 0.6981132075471698,
      'A': 0.6981132075471698,
      'F': 0.6981132075471698,
      'T': 0.0,
      'G': 0.5},
     'pclass': {2: 0.43617021276595747,
      3: 0.25903614457831325,
      1: 0.6173913043478261},
     'embarked': {'S': 0.3389570552147239,
      'C': 0.553072625698324,
      'Q': 0.37349397590361444,
      'Missing': 1.0}}

Now we can go ahead and transform the categorical variables into numbers, using the
predictions of these trees:

.. code:: python

    train_t = encoder.transform(X_train)
    test_t = encoder.transform(X_test)

    train_t[['cabin', 'pclass', 'embarked']].head(10)

We can see the encoded variables below:

.. code:: python

            cabin    pclass  embarked
    501   0.304843  0.436170  0.338957
    588   0.304843  0.436170  0.338957
    402   0.304843  0.436170  0.553073
    1193  0.304843  0.259036  0.373494
    686   0.304843  0.259036  0.373494
    971   0.304843  0.259036  0.373494
    117   0.611650  0.617391  0.553073
    540   0.304843  0.436170  0.338957
    294   0.611650  0.617391  0.553073
    261   0.611650  0.617391  0.338957

Rounding the predictions
~~~~~~~~~~~~~~~~~~~~~~~~

The predictions of the decision tree can have a lot of decimals after the comma. When this
happens, reading the categorical variables might be confusing. We can control the precision
of the output to return less decimals after the comma like this:

.. code:: python

    encoder = DecisionTreeEncoder(
        variables=['cabin', 'pclass', 'embarked'],
        regression=False,
        scoring='roc_auc',
        cv=3,
        random_state=0,
        ignore_format=True,
        precision=2,
    )

    encoder.fit(X_train, y_train)

Now, the mappings for each category contain 2 decimals at most:

.. code:: python

    encoder.encoder_dict_

In the following output we see the values that will be used to replace each category
in each variable:

.. code:: python

    {'cabin': {'M': 0.3,
      'E': 0.61,
      'C': 0.61,
      'D': 0.7,
      'B': 0.7,
      'A': 0.7,
      'F': 0.7,
      'T': 0.0,
      'G': 0.5},
     'pclass': {2: 0.44, 3: 0.26, 1: 0.62},
     'embarked': {'S': 0.34, 'C': 0.55, 'Q': 0.37, 'Missing': 1.0}}

Now we can go ahead and transform the categorical variables into numbers, using the
predictions of these trees:

.. code:: python

    train_t = encoder.transform(X_train)
    test_t = encoder.transform(X_test)

    train_t[['cabin', 'pclass', 'embarked']].head(10)

We can see the encoded variables below:

.. code:: python

          cabin  pclass  embarked
    501    0.30    0.44      0.34
    588    0.30    0.44      0.34
    402    0.30    0.44      0.55
    1193   0.30    0.26      0.37
    686    0.30    0.26      0.37
    971    0.30    0.26      0.37
    117    0.61    0.62      0.55
    540    0.30    0.44      0.34
    294    0.61    0.62      0.55
    261    0.61    0.62      0.34

We can also revert the data to its original representation as follows:

.. code:: python

    revert = encoder.inverse_transform(test_t)
    revert[['cabin', 'pclass', 'embarked']].head(10)

In the following output we see that with inverse transform, we obtain the original
categories from the tree predictions:

.. code:: python

         cabin  pclass embarked
    1139     M       3        S
    533      M       2        S
    459      M       2        S
    1150     M       3        S
    393      M       2        S
    1189     G       3        S
    5        C       1        S
    231      C       1        S
    330      M       2        S
    887      M       3        S

Collisions
----------

This encoder can lead to collisions. Collisions are instances where different categories
are encoded with the same number. It is useful to reduce cardinality. On the other hand,
if the mappings are not meaningful we might lose the information contained in those
categories.

When there are collisions, `inverse_transform` would revert to only 1 of the categories,
so some of the original information will be lost with the inverse transformation.

Unseen categories
-----------------

Unseen categories are labels that appear in the test set, or in live data, that were not
present in the training set. As the decision tree is trained on the training set, there
will only be mappings for seen categories.

By default, :class:`DecisionTreeEncoder()` will ignore unseen categories, which means that
they will be replaced by NAN after the encoding. You can instruct the encoder to raise an
error when it encounters unseen categories (parameter `unseen`). Alternatively, this
encoder allows you to set up an arbitrary number to replace unseen categories
(param `fill_value`).

Monotonic variables
-------------------

We mentioned previously that the idea of this encoding is to create features whose values
grow monotonically with the target. Let's explore that. We'll use the house prices dataset:

.. code:: python

    import matplotlib.pyplot as plt
    from sklearn.datasets import fetch_openml
    from sklearn.model_selection import train_test_split

    from feature_engine.encoding import DecisionTreeEncoder

    # Load dataset
    X, y = fetch_openml(name='house_prices', version=1,
                        return_X_y=True, as_frame=True)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42)

Let's now plot the mean house price per category of the variable `MNZoning`:

.. code:: python

    y_train.groupby(X_train["MSZoning"]).mean().plot.bar()
    plt.ylabel("mean house price")
    plt.show()

In the following image, we see that there isn't a monotonic relationship between the
current order of the categories and the house price:

.. image:: ../../images/mzoning-price-per-cat.png

Let's explore another variable:

.. code:: python

    y_train.groupby(X_train["LotShape"]).mean().plot.bar()
    plt.ylabel("mean house price")
    plt.show()

In the following image we see that there isn't a monotonic relationship between `LotShape`
and the house price either:

.. image:: ../../images/lotshape-price-per-cat.png

Let's now encode these variables using decision trees:

.. code:: python

    encoder = DecisionTreeEncoder(
        variables=["MSZoning", 'LotShape'],
        regression=True,
        cv=3,
        random_state=0,
        ignore_format=True,
        precision=0,
    )

    encoder.fit(X_train, y_train)

Let's check out the created mappings:

.. code:: python

    encoder.encoder_dict_

Below we see the values that will be used to replace each category:

.. code:: python

    {'MSZoning': {'RL': 191179.0,
      'RM': 127281.0,
      'FV': 215063.0,
      "'C (all)'": 90450.0,
      'RH': 129380.0},
     'LotShape': {'Reg': 166233.0,
      'IR1': 203423.0,
      'IR2': 231688.0,
      'IR3': 231688.0}}

Let's now encode the variables:

.. code:: python

    train_t = encoder.transform(X_train)
    test_t = encoder.transform(X_test)

    print(train_t[["MSZoning", 'LotShape']].head(10))

We see the encoded variables in the following output:

.. code:: python

          MSZoning  LotShape
    135   191179.0  166233.0
    1452  127281.0  166233.0
    762   215063.0  166233.0
    932   191179.0  203423.0
    435   191179.0  231688.0
    629   191179.0  166233.0
    1210  191179.0  166233.0
    1118  191179.0  166233.0
    1084  191179.0  231688.0
    158   215063.0  166233.0

And now, let's plot the mean house price per category again, after the encoding:

.. code:: python

    y_test.groupby(test_t["MSZoning"]).mean().plot.bar()
    plt.ylabel("mean house price")
    plt.show()

In the following image we see that the categories are ordered in a way that created a
monotonic relationship with the target variable:

.. image:: ../../images/mzoning-price-per-cat-enc.png

Let's repeat that for the second variable:

.. code:: python

    y_test.groupby(test_t["LotShape"]).mean().plot.bar()
    plt.ylabel("mean house price")
    plt.show()

In the following image we also see a monotonic relationship after the encoding:

.. image:: ../../images/lotshape-price-per-cat-enc.png

Note
~~~~

Not every encoding will result in monotonic relationshops. For that to occur there needs to
be some sort of relationship between the target and the categories that can be captured by
the decision tree. Use with caution.

Additional resources
--------------------

In the following notebook, you can find more details into the :class:`DecisionTreeEncoder()`
functionality and example plots with the encoded variables:

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/encoding/DecisionTreeEncoder.ipynb>`_

For more details about this and other feature engineering methods check out these resources:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/encoding/index.rst
================================================
.. -*- mode: rst -*-
.. _encoding_user_guide:

.. currentmodule:: feature_engine.encoding

Categorical Encoding
====================

Categorical encoding is the process of converting categorical variables into numeric 
features. It is an important feature engineering step in most data science projects, 
as it ensures that machine learning algorithms can appropriately handle and interpret 
categorical data.

There are various categorical encoding methods that we can use to encode categorical 
features. One hot encoding and ordinal encoding are the most well known, but other 
encoding techniques can help tackle high cardinality and rare categories before and 
after training machine learning models.

Feature-engine's categorical encoders replace the variables' categorical values by 
estimated or arbitrary numerical values through various encoding methods. In this page, 
we will discuss categorical features and the importance of categorical encoding in more 
detail, and then introduce the various encoding techniques supported by Feature-engine.

Categorical features
--------------------

Categorical variables are those whose values are selected from a group of categories 
or labels. Their values can be strings or numbers.

*Color* is a categorical feature that can take values such as *red*, *yellow* or 
*orange*, among others. Similarly, *size* is another categorical feature, with 
the values of *small*, *medium* and *large*. 

Nominal data vs ordinal data
----------------------------

Categorical features can be **nominal** or **ordinal**.

**Nominal features** are categorical features whose categories don't have a defined ranking 
or inherent order. *Color* is an example of a nominal feature because each color is an 
independent entity, without a logical ordering or ranking.

**Ordinal features** are categorical features whose categories show an inherent order or 
ranking. *Size* is an example of an ordinal feature, as the different sizes can be ordered 
from smallest to largest.

Understanding if categorical features are *nominal* or *ordinal* can help us choose the 
most appropriate encoding method to transform their values into numbers.

Identifying categorical features
--------------------------------

We can identify categorical features by inspecting their data types. With pandas' `dtypes` 
we can obtain the data types of all variables in a dataframe; features with non-numeric data 
types such as *string*, *object* or *categorical* are, in general, categorical. 

Categorical features can also be numeric, however, like for example the features *Store ID*, 
*SKU ID* or *Zip Code*. Although these variables have numeric values, they are categorical.

Cardinality
-----------

**Cardinality** refers to the number of unique categories of a categorical variable. F
or example, the cardinality of the variable 'size', which takes the values 'small', 
'medium' and 'large' is 3. 

A categorical variable is said to have a **low cardinality** when the number of distinct 
values is relatively small. Alternatively, a categorical feature is said to have a **high 
cardinality** when the number of distinct categories is large. 

Highly cardinal features can pose challenges. It can lead to overfitting in tree-based 
models, and it can also lead to unseen categories in test or live data. As we will see 
later, if we do not account for unseen categories when designing the machine learning 
pipeline, the machine learning model will not know how to process them and will return 
an error or an inaccurate prediction.

Unseen categories
-----------------

**Unseen categories** are categorical values that appear in the test or validation 
datasets, or even in live data after model deployment, that were not present in the 
training data, and therefore were not **seen** by the machine learning model.

Unseen categories are challenging for various reasons. Firstly, when we create mappings 
from categorical values to numbers by using an encoding technique, we only generate 
mappings for those categories *present* in the training set. Hence, we'd lack a mapping 
for a new, unseen value.

We may feel tempted to replace unseen categories by 0, or an arbitrary value, or just have 
0s in all dummy variables if we used one hot encoding, but this may make the machine learning 
model behave unexpectedly leading to inaccurate predictions.

Ideally, we want to account for the potential presence of unseen categories during the 
training of the model, and more generally, during the training of the entire machine 
learning pipeline, that is, including the feature engineering and categorical encoding 
steps. Some categorical encoding methods can account for unseen categories, as we will 
see later on.

Importance of categorical encoding
----------------------------------

Most machine learning algorithms, like *linear regression*, *support vector machines* 
and *logistic regression*, require input data to be numeric because they use numerical 
computations to learn the relationship between the predictor features and the target 
variable. These algorithms are not inherently capable of interpreting categorical data. 
Thus, categorical encoding is a crucial step that ensures that the input data is compatible 
with the expectations of the machine learning models.

Some implementations of *decision tree* based algorithms can directly handle categorical data. 
We'd still recommend encoding categorical features, for example, to reduce cardinality and 
account for unseen categories.

Overfitting
------------

Overfitting occurs when a machine learning model learns the noise and random fluctuations 
present in the training data in addition to the underlying relationships. This results in a 
model that performs exceptionally well on the training data but fails to generalize on unseen 
data (i.e., the model shows low performance on the validation data set).

High cardinality features can lead to overfitting, particularly in tree-based models such 
as decision trees or random forests. Overfitting occurs because tree-based models will try 
to perform extensive splitting on the high cardinality feature, making the final tree overly 
complex. This often leads to poor generalization. Reducing cardinality, often helps mitigate 
the problem.

Encoding pipeline
-----------------

Many categorical encoding methods learn parameters from data. These parameters are  used to 
replace the original categorical values. To prevent overfitting and evaluate the machine learning 
pipelines accurately, it is key to split the dataset into a training and a testing sets before 
fitting the encoders. In other words, the encoders should learn the encoding parameters only 
from the **training data**.

Encoding methods
----------------

There are various methods to transform categorical variables into numerical features. One hot 
encoding and ordinal encoding are the most commonly used, but other methods can mitigate high 
cardinality and account for unseen categories.

In the rest of this page, we'll introduce various methods for encoding categorical data, and 
highlight the Feature-engine transformer that can carry out this transformation.

One hot encoding
~~~~~~~~~~~~~~~~

One-hot encoding (OHE) consists of replacing categorical variables by a set of binary variables 
each representing one of the unique categories in the variable. The binary variable takes the 
value 1, if the observation shows the category, or alternatively, 0.

One hot encoding is particularly suitable for linear models because it treats each category 
independently, and linear models can process binary variables effectively.

One hot encoding, however, increases the dimensionality of the dataset, as it adds a new 
variable per category. Hence, OHE may not be suitable for encoding high cardinality features, 
as it can drastically increase the dimensionality of the dataset, often leading to a set of 
variables that are highly correlated or even identical.

Feature-engine's :class:`OneHotEncoder` implements one hot encoding.

One hot encoding of frequent categories
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
To prevent a massive increase of the feature space, some data scientists create binary 
variables through one hot encoding of the **most frequent categories** in the variable. 
Less frequent values are treated collectively and represented as 0 in all the binary 
variables created for the frequent categories.

One hot encoding of frequent categories can then help tackle high cardinality and also 
unseen categories, because unseen categories will be also encoded as an infrequent value.

Feature-engine's :class:`OneHotEncoder` can implement one hot encoding of frequent categories.

Ordinal Encoding
~~~~~~~~~~~~~~~~

In ordinal encoding, each category is replaced with an integer value. These numbers are, 
in general, assigned arbitrarily. With Feature-engine's :class:`OrdinalEncoder`, we have the 
option to assign integers arbitrarily, or alternatively, ordered based on the mean target 
value per category.

Ordinal encoding is a preferred option when the categorical variable has an inherent order. 
Examples include the variable **size**, with values 'small', 'medium' and 'large', and the 
variable **education level**, with values such as 'high school', 'bachelors', 'masters' and 
'PhD', among others.

Ordinal encoding is suitable for decision trees-based models, because these models have the 
ability to interpret non-linear relationships between the encoded variables and the target, and 
prefer low feature spaces.

Ordinal encoding, however, lacks the ability to handle unseen categories. If a new category 
appears in later stages, the encoder will not know what number to assign to it, because the 
category to integer mappings are generated based on the training set. This can result in errors 
and disrupt the machine learning pipeline. Therefore, it's most effective to use ordinal encoding 
in situations where the number of categories is fixed and won't change over time.

Feature-engine's :class:`OrdinalEncoder` implements ordinal encoding.

Count and Frequency Encoding
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Count and frequency encoding consist of mapping the categories to either the count or percentage of 
observations per category observed in the training data set. Count and frequency encoding are suitable 
when the frequency of occurrence of categories is meaningful and has some predictive power. 

Count and frequency encoding work well with high cardinality and does not increase the dimensionality 
of the dataset, as opposed to one-hot encoding. However, when two or more categories have the same 
count, this encoding method will assign same value to them, and the model might not be able to 
distinguish two observations based on that feature alone.

Count and frequency encoding can't handle unseen categories on its own. To manage unseen 
categories, we can assign them a frequency or count of '0' explicitly, which represents 
the frequency of the category on the training set, to prevent failure during inference 
time (i.e., while making predictions) or while encoding live data.

Feature-engine's :class:`CountFrequencyEncoder` implements cound and frequency encoding.

Mean Encoding
~~~~~~~~~~~~~

Mean encoding, or target encoding, consists of replacing each category with a blend of the 
mean value of the target variable for the observations within that category with the general 
target mean. 

This encoding procedure was designed specifically to account for features with high 
cardinality. When there is high cardinality, there will be categories that are highly 
represented in the data, and those that are rare. With mean encoding, we'd replace categories 
that are highly present with a value that is closer to the target mean per category, 
whereas rare categories will be replaced by values closer to the general target mean. 
Like this, unseen categories will be accounted automatically and replaced by the target 
mean value across the training data set.

The mean encoding method directly ties the categorical feature to the target variable, 
hence, it is prone to overfitting. When encoding categories with the target mean, make 
sure to have proper validation strategies in place.

Feature-engine's :class:`MeanEncoder` implements mean encoding.

WoE Encoding
~~~~~~~~~~~~

The WoE encoding replaces categories by the weight of evidence, which is given by:

.. math::

    log( p(X=xj|Y = 1) / p(X=xj|Y=0) )

This encoding is primarily used in the finance industry. 

WoE is a measure of "goodness" or "strength" of a category at predicting the target outcome 
being '1' or 'positive'. It means that categories with high values of WoE are strong predictors 
of the positive class. 

WoE encoding can only be used with **binary classification** where the target variable has 
two possible outcomes such as '1' and '0'. This characteristic of WoE encoding makes it highly 
compatible with logistic regression whose fundamentals are also based on the concept of log odds. 

Feature-engine's :class:`WoEEncoder` implements WoE encoding.

Decision Tree Encoding
~~~~~~~~~~~~~~~~~~~~~~

The decision tree encoding consists of replacing each category with the output of a decision 
tree model.

In decision tree encoding, a decision tree is trained using the categorical feature to 
predict the target variable. The decision tree splits the data based of each category, 
eventually leading the observations to a final leaf. Each category is then replaced by 
the prediction of the tree, which consists of the mean target value calculated over the 
observations that ended in that leaf. 

Feature-engine's :class:`DecisionTreeEncoder` implements decision tree encoding.

Rare Label Encoding
~~~~~~~~~~~~~~~~~~~

We mentioned previously that high cardinality can lead to infrequent and unseen categories. 
This in turn can result in failures during the encoding of new data. A common way to account 
for infrequent categories is to treat them collectively as an additional group. We call this 
process rare label encoding.

Rare label encoding helps reduce the cardinality of the features. This can be beneficial for 
models and humans alike, as it simplifies the input feature. 

Rare label encoding can be used in conjunction with other encoding methods, like one-hot 
encoding, ordinal encoding and weight of evidence, to prevent explosion of the feature space, 
or the lack of mappings for less frequent values.

Feature-engine's :class:`RareLabelEncoder` implements rare label encoding.

String Similarity Encoding
~~~~~~~~~~~~~~~~~~~~~~~~~~

The string similarity encoding consists of replacing categorical variables with a set of 
float variables that capture the similarity between the category names. The new variables 
have values between `0` and `1`, where `0` indicates no similarity and `1` is an exact match 
between the names of the categories.

String similarity encoding is well suited for "dirty" categorical features. We call the categorical 
data as "dirty" when there are misspellings (bachelors vs bachlors), abbreviations used in 
some instances while full words in others (U.S. vs United States), etc. Most encoding methods
would consider each of these categories as distinct even if they refer to the same category. 
To prevent this from happening, we can use string similarity to first compute the "distance" 
between each unique category, and then we could group the highly similar categories together.

String similarity encoding reduces the cardinality and also enhances the model performance by 
blending similar categories together.

Feature-engine's :class:`StringSimilarityEncoder` implements string similarity encoding.

**Summary of Feature-engine's encoders characteristics**

=================================== ============ ================= ============== ===============================================================
    Transformer                     Regression	 Classification	   Multi-class    Description
=================================== ============ ================= ============== ===============================================================
:class:`OneHotEncoder()`	           √	            √               √         Adds dummy variables to represent each category
:class:`OrdinalEncoder()`	           √	            √    	        √         Replaces categories with an integer
:class:`CountFrequencyEncoder()`	   √	            √               √         Replaces categories with their count or frequency
:class:`MeanEncoder()`                 √	            √               x         Replaces categories with the targe mean value
:class:`WoEEncoder()`	               x	            √	            x         Replaces categories with the weight of the evidence
:class:`DecisionTreeEncoder()`	       √	            √     	        √         Replaces categories with the predictions of a decision tree
:class:`RareLabelEncoder()`	           √	            √     	        √         Groups infrequent categories into a single one
:class:`StringSimilarityEncoder()`	   √	            √     	        √         Replaces categories with distances values
=================================== ============ ================= ============== ===============================================================

Feature-engine's categorical encoders work only with categorical variables by default.
From version 1.1.0, you have the option to set the parameter `ignore_format` to `False`,
and make the transformers also accept numerical variables as input.

**Monotonicity**

Most Feature-engine's encoders will return, or attempt to return monotonic relationships
between the encoded variable and the target. A monotonic relationship is one in which
the variable value increases as the values in the other variable increase, or decrease.
See the following illustration as examples:

.. figure::  ../../images/monotonic.png
   :align:   center
   :width: 400

Monotonic relationships tend to help improve the performance of linear models and build
shallower decision trees.

**Regression vs Classification**

Most Feature-engine's encoders are suitable for both regression and classification, with
the exception of the :class:`WoEEncoder()` which is
designed solely for **binary** classification.

**Multi-class classification**

Finally, some Feature-engine's encoders can handle multi-class targets off-the-shelf for
example the :class:`OneHotEncoder()`, the :class:`CountFrequencyEncoder()` and the
:class:`DecisionTreeEncoder()`.

Note that while the :class:`MeanEncoder()` and the :class:`OrdinalEncoder()` will operate
with multi-class targets, but the mean of the classes may not be significant and this will
defeat the purpose of these encoding techniques.

Alternative encoding techniques
-------------------------------

In addition to the categorical encoding methods supported by Feature-engine, there are 
other methods like feature hashing or binary encoding. These methods are supported by the 
Python library category encoders. For the time being, we decided not to support these 
transformations because they return features that are not easy to interpret. And hence, 
it is very hard to make sense of the outputs of machine learning models trained on 
categorical variables encoded with these methods.

Additional resources
--------------------

For tutorials about this and other feature engineering methods check out these resources:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

Encoders
--------

.. toctree::
   :maxdepth: 1

   OneHotEncoder
   OrdinalEncoder
   CountFrequencyEncoder
   MeanEncoder
   WoEEncoder
   DecisionTreeEncoder
   RareLabelEncoder
   StringSimilarityEncoder

================================================
FILE: docs/user_guide/encoding/MeanEncoder.rst
================================================
.. _mean_encoder:

.. currentmodule:: feature_engine.encoding

MeanEncoder
===========

Mean encoding is the process of replacing the categories in categorical features by the
mean value of the target variable shown by each category. For example, if we are trying
to predict the default rate (that's the target variable), and our dataset has the categorical
variable **City**, with the categories of **London**, **Manchester**, and **Bristol**,
and the default rate per city is 0.1, 0.5, and 0.3, respectively, with mean encoding, we
would replace London by 0.1, Manchester by 0.5, and Bristol by 0.3.

Mean encoding, together with one hot encoding and ordinal encoding, belongs to the most
commonly used categorical encoding techniques in data science.

It is said that mean encoding can easily cause overfitting. That's because we are capturing
some information about the target into the predictive features during the encoding. More
importantly, the overfitting can be caused by encoding categories with low frequencies
with mean target values that are unreliable. In short, the mean target values seen for
those categories in the training set do not hold for test data or new observations.

Overfitting
-----------

When the categories in the categorical features have a good representation, or, in other
words, when there are enough observations in our dataset that show the categories that we
want to encode, then taking the simple average of the target variable per category is a
good approximation. We can trust that a new data point, say from the test data, that
shows that category will also have a target value that is similar to the target mean
value that we calculated for said category during training.

However, if there are only a few observations that show some of the categories, then the
mean target value for those categories will be unreliable. In other words, the certainty
that we have that a new observation that shows this category will have a mean target value
close to the one we estimated decreases.

To account for the uncertainty of the encoding values for rare categories, what we normally
do is **"blend"** the mean target variable per category with the general mean of the target,
calculated over the entire training dataset. And this blending is proportional to the
variability of the target within that category and the category frequency.

Smoothing
---------

To avoid overfitting, we can determine the mean target value estimates as a mixture of two
values: the mean target value per category (known as the posterior) and the mean target
value in the entire dataset (known as the prior).

The following formula shows the estimation of the mean target value with smoothing:

.. math::

    mapping = (w_i) posterior + (1-w_i) prior

The prior and posterior values are “blended” using a weighting factor (`wi`). This weighting
factor is a function of the category group size (`n_i`) and the variance of the target in
the data (`t`) and within the category (`s`):

.. math::

    w_i = n_i t / (s + n_i t)

When the category group is large, the weighing factor is close to 1, and therefore more
weight is given to the posterior (the mean of the target per category). When the category
group size is small, then the weight gets closer to 0, and more weight is given to the
prior (the mean of the target in the entire dataset).

In addition, if the variability of the target within that category is large, we also give
more weight to the prior, whereas if it is small, then we give more weight to the posterior.

In short, adding smoothing can help prevent overfitting in those cases where categorical
data have many infrequent categories or show high cardinality.

High cardinality
----------------

High cardinality refers to a high number of unique categories in the categorical features.
Mean encoding was specifically designed to tackle highly cardinal variables by taking
advantage of this smoothing function, which will essentially blend infrequent categories
together by replacing them with values very close to the overall target mean calculated
over the training data.

Another encoding method that tackles cardinality out of the box is count encoding. See for
example :class:`CountFrequencyEncoder`.

To account for highly cardinal variables in alternative encoding methods, you can group
rare categories together by using the :class:`RareLabelEncoder`.

Alternative Python implementations of mean encoding
---------------------------------------------------

In Feature-engine, we blend the probabilities considering the target variability and the
category frequency. In the original paper, there are alternative formulations to determine
the blending. If you want to check those out, use the transformers from the Python library
Category encoders:

- `M-estimate <https://contrib.scikit-learn.org/category_encoders/mestimate.html>`_
- `Target Encoder <https://contrib.scikit-learn.org/category_encoders/targetencoder.html>`_

Mean encoder
------------

Feature-engine's :class:`MeanEncoder()` replaces categories with the mean of the target per
category. By default, it does not implement smoothing. That means that it will replace
categories by the mean target value as determined during training over the training data
set (just the posterior).

To apply smoothing using the formulation that we described earlier, set the parameter
`smoothing` to `"auto"`. That would be our recommended solution. Alternatively, you can
set the parameter `smoothing` to any value that you want, in which case the weighting
factor `wi` will be calculated like this:

.. math::

    w_i = n_i / (s + n_i)

where s is the value your pass to `smoothing`.

Unseen categories
-----------------

Unseen categories are those labels that were not seen during training. Or in other words,
categories that were not present in the training data.

With the :class:`MeanEncoder()`, we can take care of unseen categories in 1 of 3 ways:

- We can set the mean encoder to ignore unseen categories, in which case those categories will be replaced by nan.
- We can set the mean encoder to raise an error when it encounters unseen categories. This is useful when we don't expect new categories for those categorical variables.
- We can instruct the mean encoder to replace unseen or new categories with the mean of the target shown in the training data, that is, the prior.

Mean encoding and machine learning
----------------------------------

Feature-engine's :class:`MeanEncoder()` can perform mean encoding for regression and binary
classification datasets. At the moment, we do not support multi-class targets.

Python examples
---------------

In the following sections, we'll show the functionality of :class:`MeanEncoder()` using the
Titanic Dataset.

First, let's load the libraries, functions and classes:

.. code:: python

    from sklearn.model_selection import train_test_split
    from feature_engine.datasets import load_titanic
    from feature_engine.encoding import MeanEncoder

To avoid data leakage, it is important to separate the data into training and test sets.
The mean target values, with or without smoothing, will be determined using the training
data only.

Let's load and split the data:

.. code:: python

    X, y = load_titanic(
        return_X_y_frame=True,
        handle_missing=True,
        predictors_only=True,
        cabin="letter_only",
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=0,
    )

    print(X_train.head())

We see the resulting dataframe containing 3 categorical columns: sex, cabin and embarked:

.. code:: python

          pclass     sex        age  sibsp  parch     fare cabin embarked
    501        2  female  13.000000      0      1  19.5000     M        S
    588        2  female   4.000000      1      1  23.0000     M        S
    402        2  female  30.000000      1      0  13.8583     M        C
    1193       3    male  29.881135      0      0   7.7250     M        Q
    686        3  female  22.000000      0      0   7.7250     M        Q

Simple mean encoding
--------------------

Let's set up the :class:`MeanEncoder()` to replace the categories in the categorical
features with the target mean, without smoothing:

.. code:: python

    encoder = MeanEncoder(
        variables=['cabin', 'sex', 'embarked'],
    )

    encoder.fit(X_train, y_train)

With `fit()` the encoder learns the target mean value for each category and stores those
values in the `encoder_dict_` attribute:

.. code:: python

   encoder.encoder_dict_

The `encoder_dict_` contains the mean value of the target per category, per variable.
We can use this dictionary to map the numbers in the encoded features to the original
categorical values.

.. code:: python

    {'cabin': {'A': 0.5294117647058824,
      'B': 0.7619047619047619,
      'C': 0.5633802816901409,
      'D': 0.71875,
      'E': 0.71875,
      'F': 0.6666666666666666,
      'G': 0.5,
      'M': 0.30484330484330485,
      'T': 0.0},
     'sex': {'female': 0.7283582089552239, 'male': 0.18760757314974183},
     'embarked': {'C': 0.553072625698324,
      'Missing': 1.0,
      'Q': 0.37349397590361444,
      'S': 0.3389570552147239}}

We can now go ahead and replace the categorical values with the numerical values:

.. code:: python

    train_t = encoder.transform(X_train)
    test_t = encoder.transform(X_test)

    print(train_t.head())

Below we see the resulting dataframe, where the categorical values are now replaced
with the target mean values:

.. code:: python

          pclass       sex        age  sibsp  parch     fare     cabin  embarked
    501        2  0.728358  13.000000      0      1  19.5000  0.304843  0.338957
    588        2  0.728358   4.000000      1      1  23.0000  0.304843  0.338957
    402        2  0.728358  30.000000      1      0  13.8583  0.304843  0.553073
    1193       3  0.187608  29.881135      0      0   7.7250  0.304843  0.373494
    686        3  0.728358  22.000000      0      0   7.7250  0.304843  0.373494

Mean encoding with smoothing
----------------------------

By default, :class:`MeanEncoder()` determines the mean target values without blending.
If we want to apply smoothing to control the cardinality of the variable and avoid
overfitting, we set up the transformer as follows:

.. code:: python

    encoder = MeanEncoder(
        variables=None,
        smoothing="auto"
    )

    encoder.fit(X_train, y_train)

In this example, we did not indicate which variables to encode. :class:`MeanEncoder()` can
automatically find the categorical variables, which are stored in one of its attributes:

.. code:: python

    encoder.variables_

Below we see the categorical features found by :class:`MeanEncoder()`:

.. code:: python

    ['sex', 'cabin', 'embarked']

We can find the categorical mappings calculated by the mean encoder:

.. code:: python

   encoder.encoder_dict_

Note that these values are different to those determined without smoothing:

.. code:: python

    {'sex': {'female': 0.7275051072923914, 'male': 0.18782635616273297},
     'cabin': {'A': 0.5210189753697639,
      'B': 0.755161569137655,
      'C': 0.5608140829162441,
      'D': 0.7100896537503179,
      'E': 0.7100896537503179,
      'F': 0.6501082490288561,
      'G': 0.47606795923242295,
      'M': 0.3049458046855866,
      'T': 0.0},
     'embarked': {'C': 0.552100581239763,
      'Missing': 1.0,
      'Q': 0.3736336816011083,
      'S': 0.3390242994568531}}

We can now go ahead and replace the categorical values with the numerical values:

.. code:: python

    train_t = encoder.transform(X_train)
    test_t = encoder.transform(X_test)

    print(train_t.head())

Below we see the resulting dataframe with the encoded features:

.. code:: python

          pclass       sex        age  sibsp  parch     fare     cabin  embarked
    501        2  0.727505  13.000000      0      1  19.5000  0.304946  0.339024
    588        2  0.727505   4.000000      1      1  23.0000  0.304946  0.339024
    402        2  0.727505  30.000000      1      0  13.8583  0.304946  0.552101
    1193       3  0.187826  29.881135      0      0   7.7250  0.304946  0.373634
    686        3  0.727505  22.000000      0      0   7.7250  0.304946  0.373634

We can now use this dataframes to train machine learning models for regression or
classification.

Mean encoding variables with numerical values
---------------------------------------------

:class:`MeanEncoder()`, and all Feature-engine encoders, have been designed to work with
variables of type object or categorical by default. If you want to encode variables that
are numeric, you need to instruct the transformer to ignore the data type:

.. code:: python

    encoder = MeanEncoder(
        variables=['cabin', 'pclass'],
        ignore_format=True,
    )

    t_train = encoder.fit_transform(X_train, y_train)
    t_test = encoder.transform(X_test)

After encoding the features we can use the data sets to train machine learning algorithms.

Last thing to note before closing in is that mean encoding does not increase the
dimensionality of the resulting dataframes: from 1 categorical feature, we obtain 1
encoded variable. Hence, this encoding method is suitable for predictive modeling that
uses models that are sensitive to the size of the feature space.

Additional resources
--------------------

In the following notebook, you can find more details into the :class:`MeanEncoder()`
functionality and example plots with the encoded variables:

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/encoding/MeanEncoder.ipynb>`_

For tutorials about this and other feature engineering methods check out these resources:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

.. figure::  ../../images/fetsf.png
   :width: 300
   :figclass: align-center
   :align: right
   :target: https:

   Feature Engineering for Time Series Forecasting

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and courses are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/encoding/OneHotEncoder.rst
================================================
.. _onehot_encoder:

.. currentmodule:: feature_engine.encoding

OneHotEncoder
=============

One-hot encoding is a method used to represent categorical data, where each category
is represented by a binary variable. The binary variable takes the value 1 if the
category is present and 0 otherwise. The binary variables are also known as dummy
variables.

To represent the categorical feature "is-smoker" with categories "Smoker" and
"Non-smoker", we can generate the dummy variable "Smoker", which takes 1 if the
person smokes and 0 otherwise. We can also generate the variable "Non-smoker", which
takes 1 if the person does not smoke and 0 otherwise.

The following table shows a possible one hot encoded representation of the variable
"is smoker":

============= ========== =============
  is smoker     smoker     non-smoker
============= ========== =============
smoker            1           0
non-smoker  	  0           1
non-smoker        0           1
smoker            1	          0
non-smoker	      0           1
============= ========== =============

For the categorical variable **Country** with values **England**, **Argentina**, and
**Germany**, we can create three variables called `England`, `Argentina`, and `Germany`.
These variables will take the value of 1 if the observation is England, Argentina, or
Germany, respectively, and 0 otherwise.

Encoding into k vs k-1 variables
--------------------------------

A categorical feature with k unique categories can be encoded using k-1 binary variables.
For `Smoker`, k is 2 as it contains two labels (Smoker and Non-Smoker), so we only
need one binary variable (k - 1 = 1) to capture all of the information.

In the following table we see that the dummy variable `Smoker` fully represents the
original categorical values:

============= ==========
  is smoker     smoker
============= ==========
smoker            1
non-smoker  	  0
non-smoker        0
smoker            1
non-smoker	      0
============= ==========

For the **Country** variable, which has three categories (k=3; England, Argentina, and
Germany), we need two (k - 1 = 2) binary variables to capture all the information. The
variable will be fully represented like this:

============= ========== =============
  Country      England     Argentina
============= ========== =============
England            1           0
Argentina  	       0           1
Germany            0           0
============= ========== =============

As we see in the previous table, if the observation is England, it will show the value 1 in
the `England` variable; if the observation is Argentina, it will show the value 1 in
the `Argentina` variable; and if the observation is Germany, it will show zeroes in
both dummy variables.

Like these, by looking at the values of the k-1 dummies, we can infer the original
categorical value of each observation.

Encoding into k-1 binary variables is well-suited for linear regression models. Linear
models evaluate all features during fit, thus, with k-1 they have all the information
about the original categorical variable.

There are a few occasions in which we may prefer to encode the categorical variables
with k binary variables.

Encode into k dummy variables if training decision trees based models or performing
feature selection. Decision tree based models and many feature selection algorithms
evaluate variables or groups of variables separately. Thus, if encoding into k-1, the
last category will not be examined. In other words, we lose the information contained
in that category.

Binary variables
----------------

When a categorical variable has only 2 categories, like "Smoker" in our previous example,
then encoding into k-1 suits all purposes, because the second dummy variable created
by one hot encoding is completely redundant.

Encoding popular categories
---------------------------

One hot encoding can increase the feature space dramatically, particularly if we have
many categorical features, or the features have high cardinality. To control the feature
space, it is common practice to encode only the most frequent categories in each
categorical variable.

When we encode the most frequent categories, we will create binary variables for each
of these frequent categories, and when the observation has a different, less popular
category, it will have a 0 in all binary variables. See the following example:

============== ========== =============
  var           popular1     popular2
============== ========== =============
popular1            1           0
popular2            0           1
popular1            1           0
non-popular         0           0
popular2	        0           1
less popular        0           0
unpopular           0           0
lonely              0           0
============== ========== =============

As we see in the previous table, less popular categories are represented as a group by
showing zeroes in all binary variables.

OneHotEncoder
-------------

Feature-engine's :class:`OneHotEncoder()` encodes categorical data as a one-hot numeric
dataframe.

:class:`OneHotEncoder()` can encode into k or k-1 dummy variables. The behaviour is
specified through the `drop_last` parameter, which can be set to `False` for k, or to
`True` for k-1 dummy variables.

:class:`OneHotEncoder()` can specifically encode binary variables into k-1 variables
(that is, 1 dummy) while encoding categorical features of higher cardinality into k
dummies. This behaviour is specified by setting the parameter `drop_last_binary=True`.
This will ensure that for every binary variable in the dataset, that is, for every
categorical variable with ONLY 2 categories, only 1 dummy is created. This is recommended,
unless you suspect that the variable could, in principle, take more than 2 values.

:class:`OneHotEncoder()` can also create binary variables for the **n** most popular
categories, n being determined by the user. For example, if we encode only the 6 more
popular categories, by setting the parameter `top_categories=6`, the transformer will
add binary variables only for the 6 most frequent categories. The most frequent categories
are those with the greatest number of observations. The remaining categories will show
zeroes in each one of the derived dummies. This behaviour is useful when the categorical
variables are highly cardinal to control the expansion of the feature space.

**Note**

The parameter `drop_last` is ignored when encoding the most popular categories.

Python implementation
---------------------

Let's look at an example of one hot encoding, using Feature-engine's  :class:`OneHotEncoder()`
utilizing the Titanic Dataset.

We'll start by importing the libraries, functions and classes, and loading the data into
a pandas dataframe and dividing it into a training and a testing set:

.. code:: python

    from sklearn.model_selection import train_test_split
    from feature_engine.datasets import load_titanic
    from feature_engine.encoding import OneHotEncoder

    X, y = load_titanic(
        return_X_y_frame=True,
        handle_missing=True,
        predictors_only=True,
        cabin="letter_only",
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=0,
    )

    print(X_train.head())

We see the first 5 rows of the training data below:

.. code:: python

          pclass     sex        age  sibsp  parch     fare cabin embarked
    501        2  female  13.000000      0      1  19.5000     M        S
    588        2  female   4.000000      1      1  23.0000     M        S
    402        2  female  30.000000      1      0  13.8583     M        C
    1193       3    male  29.881135      0      0   7.7250     M        Q
    686        3  female  22.000000      0      0   7.7250     M        Q

Let's explore the cardinality of 4 of the categorical features:

.. code:: python

    X_train[['sex', 'pclass', 'cabin', 'embarked']].nunique()

.. code:: python

    sex         2
    pclass      3
    cabin       9
    embarked    4
    dtype: int64

We see that the variable sex has 2 categories, pclass has 3 categories, the variable
cabin has 9 categories, and the variable embarked has 4 categories.

Let's now set up the OneHotEncoder to encode 2 of the categorical variables into k-1 dummy
variables:

.. code:: python

    encoder = OneHotEncoder(
        variables=['cabin', 'embarked'],
        drop_last=True,
        )

    encoder.fit(X_train)

With `fit()` the encoder learns the categories of the variables, which are stored in the
attribute `encoder_dict_`.

.. code:: python

   encoder.encoder_dict_

.. code:: python

    {'cabin': ['M', 'E', 'C', 'D', 'B', 'A', 'F', 'T'],
     'embarked': ['S', 'C', 'Q']}

The `encoder_dict_` contains the categories that will be represented by dummy variables
for each categorical variable.

With transform, we go ahead and encode the variables. Note that by default, the
:class:`OneHotEncoder()` drops the original categorical variables, which are now
represented by the one-hot array.

.. code:: python

    train_t = encoder.transform(X_train)
    test_t = encoder.transform(X_test)

    print(train_t.head())

Below we see the one hot dummy variables added to the dataset and the original variables
are no longer in the dataframe:

.. code:: python

          pclass     sex        age  sibsp  parch     fare  cabin_M  cabin_E  \
    501        2  female  13.000000      0      1  19.5000        1        0
    588        2  female   4.000000      1      1  23.0000        1        0
    402        2  female  30.000000      1      0  13.8583        1        0
    1193       3    male  29.881135      0      0   7.7250        1        0
    686        3  female  22.000000      0      0   7.7250        1        0

          cabin_C  cabin_D  cabin_B  cabin_A  cabin_F  cabin_T  embarked_S  \
    501         0        0        0        0        0        0           1
    588         0        0        0        0        0        0           1
    402         0        0        0        0        0        0           0
    1193        0        0        0        0        0        0           0
    686         0        0        0        0        0        0           0

          embarked_C  embarked_Q
    501            0           0
    588            0           0
    402            1           0
    1193           0           1
    686            0           1

Finding categorical variables automatically
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Feature-engine's :class:`OneHotEncoder()` can automatically find and encode all
categorical features in the pandas dataframe. Let's show that with an example.

Let's set up the OneHotEncoder to find and encode all categorical features:

.. code:: python

    encoder = OneHotEncoder(
        variables=None,
        drop_last=True,
        )

    encoder.fit(X_train)

With fit, the encoder finds the categorical features and identifies it's unique
categories. We can find the categorical variables like this:

.. code:: python

    encoder.variables_

.. code:: python

    ['sex', 'cabin', 'embarked']

And we can identify the unique categories for each variables like this:

.. code:: python

    encoder.encoder_dict_

.. code:: python

    {'sex': ['female'],
     'cabin': ['M', 'E', 'C', 'D', 'B', 'A', 'F', 'T'],
     'embarked': ['S', 'C', 'Q']}

We can now encode the categorical variables:

.. code:: python

    train_t = encoder.transform(X_train)
    test_t = encoder.transform(X_test)

    print(train_t.head())

And here we see the resulting dataframe:

.. code:: python

          pclass        age  sibsp  parch     fare  sex_female  cabin_M  cabin_E  \
    501        2  13.000000      0      1  19.5000           1        1        0
    588        2   4.000000      1      1  23.0000           1        1        0
    402        2  30.000000      1      0  13.8583           1        1        0
    1193       3  29.881135      0      0   7.7250           0        1        0
    686        3  22.000000      0      0   7.7250           1        1        0

          cabin_C  cabin_D  cabin_B  cabin_A  cabin_F  cabin_T  embarked_S  \
    501         0        0        0        0        0        0           1
    588         0        0        0        0        0        0           1
    402         0        0        0        0        0        0           0
    1193        0        0        0        0        0        0           0
    686         0        0        0        0        0        0           0

          embarked_C  embarked_Q
    501            0           0
    588            0           0
    402            1           0
    1193           0           1
    686            0           1

Encoding variables of type numeric
----------------------------------

By default, Feature-engine's :class:`OneHotEncoder()` will only encode categorical
features. If you attempt to encode a variable of numeric dtype, it will raise an error.
To avoid this error, you can instruct the encoder to ignore the data type format as
follows:

.. code:: python

    enc = OneHotEncoder(
        variables=['pclass'],
        drop_last=True,
        ignore_format=True,
        )

    enc.fit(X_train)

    train_t = enc.transform(X_train)
    test_t = enc.transform(X_test)

    print(train_t.head())

Note that pclass had numeric values instead of strings, and it was one hot encoded by
the transformer into 2 dummies:

.. code:: python

             sex        age  sibsp  parch     fare cabin embarked  pclass_2  \
    501   female  13.000000      0      1  19.5000     M        S         1
    588   female   4.000000      1      1  23.0000     M        S         1
    402   female  30.000000      1      0  13.8583     M        C         1
    1193    male  29.881135      0      0   7.7250     M        Q         0
    686   female  22.000000      0      0   7.7250     M        Q         0

          pclass_3
    501          0
    588          0
    402          0
    1193         1
    686          1

Encoding binary variables into 1 dummy
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

With Feature-engine's :class:`OneHotEncoder()` we can encode all categorical variables
into k dummies and the binary variables into k-1 by setting the encoder as follows:

.. code:: python

    ohe = OneHotEncoder(
        variables=['sex', 'cabin','embarked'],
        drop_last=False,
        drop_last_binary=True,
        )

    train_t = ohe.fit_transform(X_train)
    test_t = ohe.transform(X_test)

    print(train_t.head())

As we see in the following input, for the variable sex, we have only have 1 dummy,
and for all the rest we have k dummies:

.. code:: python

          pclass        age  sibsp  parch     fare  sex_female  cabin_M  cabin_E  \
    501        2  13.000000      0      1  19.5000           1        1        0
    588        2   4.000000      1      1  23.0000           1        1        0
    402        2  30.000000      1      0  13.8583           1        1        0
    1193       3  29.881135      0      0   7.7250           0        1        0
    686        3  22.000000      0      0   7.7250           1        1        0

          cabin_C  cabin_D  cabin_B  cabin_A  cabin_F  cabin_T  cabin_G  \
    501         0        0        0        0        0        0        0
    588         0        0        0        0        0        0        0
    402         0        0        0        0        0        0        0
    1193        0        0        0        0        0        0        0
    686         0        0        0        0        0        0        0

          embarked_S  embarked_C  embarked_Q  embarked_Missing
    501            1           0           0                 0
    588            1           0           0                 0
    402            0           1           0                 0
    1193           0           0           1                 0

Encoding frequent categories
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If the categorical variables are highly cardinal, we may end up with very big datasets
after one hot encoding. In addition, if some of these variables are fairly constant or
fairly similar, we may end up with one hot encoded features that are highly correlated,
if not identical. To avoid this behaviour, we can encode only the most frequent categories.

To encode the 2 most frequent categories of each categorical column, we set up the
transformer as follows:

.. code:: python

    ohe = OneHotEncoder(
        top_categories=2,
        variables=['pclass', 'cabin', 'embarked'],
        ignore_format=True,
        )

    train_t = ohe.fit_transform(X_train)
    test_t = ohe.transform(X_test)

    print(train_t.head())

As we see in the resulting dataframe, we created only 2 dummies per variable:

.. code:: python

             sex        age  sibsp  parch     fare  pclass_3  pclass_1  cabin_M  \
    501   female  13.000000      0      1  19.5000         0         0        1
    588   female   4.000000      1      1  23.0000         0         0        1
    402   female  30.000000      1      0  13.8583         0         0        1
    1193    male  29.881135      0      0   7.7250         1         0        1
    686   female  22.000000      0      0   7.7250         1         0        1

          cabin_C  embarked_S  embarked_C
    501         0           1           0
    588         0           1           0
    402         0           0           1
    1193        0           0           0
    686         0           0           0

Finally, if we want to obtain the column names in the resulting dataframe we can do the
following:

.. code:: python

    encoder.get_feature_names_out()

We see the names of the columns below:

.. code:: python

    ['sex',
     'age',
     'sibsp',
     'parch',
     'fare',
     'pclass_3',
     'pclass_1',
     'cabin_M',
     'cabin_C',
     'embarked_S',
     'embarked_C']

Considerations
--------------

Encoding categorical variables into k dummies, will handle unknown categories automatically.
Those features not seen during training will show zeroes in all dummies.

Encoding categorical features into k-1 dummies, will cause unseen data to be treated as
the category that is dropped.

Encoding the top categories will make unseen categories part of the group of less popular
categories.

If you add a big number of dummy variables to your data, many may be identical or highly
correlated. Consider dropping identical and correlated features with the transformers
from the :ref:`selection module <selection_user_guide>`.

For alternative encoding methods used in data science check the :class:`OrdinalEncoder()`
and other encoders included in the :ref:`encoding module <encoding_user_guide>`.

Tutorials, books and courses
----------------------------

For more details into :class:`OneHotEncoder()`'s functionality visit:

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/encoding/OneHotEncoder.ipynb>`_

For tutorials about this and other data preprocessing methods check out our online course:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/encoding/OrdinalEncoder.rst
================================================
.. _ordinal_encoder:

.. currentmodule:: feature_engine.encoding

Ordinal Encoding
================

Ordinal encoding consists of converting categorical data into numeric data by assigning a unique integer to each category, 
and is a common data preprocessing step in most data science projects.

Ordinal encoding is particularly useful when an inherent ordering or ranking is present within the categorical variable. 
For example, the variable **size** with values *small*, *medium*, and *large* exhibits a clear ranking, i.e., small 
< medium < large, thereby making ordinal encoding an appropriate encoding method. 

In practice, we apply ordinal encoding regardless of the intrinsic ordering of the variable because some machine learning 
models, like, for example, decision tree-based models, are able to learn from these arbitrarily assigned values. 

One of the advantages of ordinal encoding is that it keeps the feature space compact as opposed to one-hot encoding, which 
can significantly increase the dimensionality of the dataset.

Arbitrary vs ordered ordinal encoding
-------------------------------------

In ordinal encoding, the categorical variable can be encoded into numeric values either arbitrarily or based on some 
defined logic.

Arbitrary ordinal encoding
~~~~~~~~~~~~~~~~~~~~~~~~~~

**Arbitrary ordinal encoding** is the traditional way to perform ordinal encoding, where each category is replaced by a 
unique numeric value without any further consideration. This encoding method assigns numbers to the categories based on 
their order of appearance in the dataset, incrementing the value for each new category encountered.

Assigning ordinal numbers arbitrarily provides a simple way of obtaining numerical variables from categorical data and it 
tends to work well with decision tree based machine learning models.

Ordered ordinal encoding
~~~~~~~~~~~~~~~~~~~~~~~~

**Ordered ordinal encoding** is a more sophisticated way to implement ordinal encoding. It consists of first sorting the 
categories based on the mean value of the target variable associated with each category and then assigning the numeric 
values according to this order.

For example, for the variable `colour`, if the mean of the target
for blue, red and grey is 0.5, 0.8 and 0.1 respectively, then we first sort the categories by their mean values: grey (0.1), 
blue (0.5), red (0.8). Then, we replace grey with 0, blue with 1, and red with 2.

Ordered encoding attempts to define a monotonic relationship between the encoded variable and the target variable. This 
method helps machine learning algorithms, particularly linear models (like linear regression), better capture and learn 
the relationship between the encoded feature and the target.

Keep in mind that ordered ordinal encoding will create a monotonic relationship between the encoded variable and the target 
variable **only** when *there is* an intrinsic relationship between the categories and the target variable.

Unseen categories
-----------------

Ordinal encoding can't inherently deal with unseen categories. 

**Unseen categories** are categorical values that appear in test, validation, or live data but were not present in the 
training data. These categories are problematic because the encoding methods generate mappings only for categories present 
in the training data. This means that we would lack encodings for any new, unseen category values. Unseen categories cause 
errors during inference time (the phase when the machine learning model is used to make predictions on new data) because our 
feature engineering pipeline is unable to convert that value into a number.

Ordinal encoding by itself does not deal with unseen categories. However, we could replace the unseen category with an 
arbitrary value, such as -1 (remember that ordinal encoding starts at 0). This procedure might work well for linear models 
because -1 will be the smallest value for the categorical variable, and since linear models establish linear relationships 
between variables and targets, it will return the lowest (or highest) response value for unseen categories.

However, for tree-based models, this method of replacing unseen categories might not be effective because trees create 
non-linear partitions, making it difficult to predict in advance how the tree will handle a value of -1, leading to 
unpredictable results.

If we expect our variables to have a large number of unseen categories, it is better to opt for another encoding technique 
that can handle unseen categories out of the box, such as target encoding, or conversely, group rare categories together.

Pros and cons of ordinal encoding
----------------------------------

Ordinal encoding is quick and easy to implement, and it does not increase the dimensionality of the dataset, as does 
one-hot encoding.

On the downside, it can impose misleading relationships between the categories; it does not have the ability to deal with 
unseen categories; and it is not suitable for a large number of categories, i.e., features with high cardinality.

Ordinal encoding vs label encoding
----------------------------------

Ordinal encoding is sometimes also referred to as label encoding. They follow the same procedure. Scikit-learn provides 
2 different transformers: the OrdinalEncoder and the LabelEncoder. Both replace values, that is, categories, with ordinal 
data. The OrdinalEncoder is designed to transform the predictor variables (those in the training set), while the LabelEncoder 
is designed to transform the target variable. The end result of both transformers is the same; the original values are 
replaced by ordinal numbers.

In our view, this has raised some confusion as to whether label encoding and ordinal encoding consist of different ways of 
preprocessing categorical data. Some argue that label encoding consists of replacing categories with numbers assigned 
arbitrarily, whereas ordinal encoding consists of assigning numbers based on an inherent order of the variable (like 
that of the variable size). We make no such distinction and consider both techniques interchangeably. 

OrdinalEncoder
--------------

Feature-engine's :class:`OrdinalEncoder()` implements ordinal encoding. That is, it encodes categorical features by 
replacing each category with a unique number ranging from 0 to k-1, where 'k' is the distinct number of categories in 
the dataset. 

:class:`OrdinalEncoder()` supports both **arbitrary** and **ordered** encoding methods. The desired approach can be 
specified using the `encoding_method` parameter that accepts either **"arbitrary"** or **"ordered"**. If not defined, 
`encoding_method` defaults to `"ordered"`.

If the `encoding_method` is defined as **"arbitrary"**, then :class:`OrdinalEncoder()` will assign numeric values to the 
categorical variable on a first-come first-served basis i.e., in the order the categories are encountered in the dataset.

If the `encoding_method` is defined as **"ordered"**, then :class:`OrdinalEncoder()` will assign numeric values according 
to the mean of the target variable for each category. The categories with the highest target mean value will be replaced by 
an integer value k-1, while the category with the lowest target mean value will be replaced by 0. Here 'k' is the distinct 
number of categories.

When encountering unseen categories, :class:`OrdinalEncoder()` has the option to raise an error and fail, ignore the rare 
category, in which case it will be encoded as `np.nan`, or encode it into -1. You can define this behaviour through the 
`unseen` parameter.  

Python Implementation
---------------------

In the rest of the page, we'll show different ways how we can use ordinal encoding through Feature-engine's 
:class:`OrdinalEncoder()`.

Arbitrary ordinal encoding
~~~~~~~~~~~~~~~~~~~~~~~~~~

We'll show how ordinal encoding is implemented by Feature-engine's :class:`OrdinalEncoder()` using the **Titanic Dataset**.

Let's load the dataset and split it into train and test sets:

.. code:: python

    import pandas as pd
    from sklearn.model_selection import train_test_split
    from feature_engine.datasets import load_titanic
    from feature_engine.encoding import OrdinalEncoder

    X, y = load_titanic(
        return_X_y_frame=True,
        handle_missing=True,
        predictors_only=True,
        cabin="letter_only",
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=0,
    )

    print(X_train.head())

We see the Titanic dataset below:

.. code:: python

         pclass    sex     age         sibsp   parch     fare    cabin  embarked
 501        2     female   13.000000      0      1     19.5000     M        S
 588        2     female    4.000000      1      1     23.0000     M        S
 402        2     female   30.000000      1      0     13.8583     M        C
 1193       3     male     29.881135      0      0      7.7250     M        Q
 686        3     female   22.000000      0      0      7.7250     M        Q

Let's set up the :class:`OrdinalEncoder()` to encode the categorical variables `cabin', `embarked`, and `sex` with 
integers assigned arbitrarily:

.. code:: python

 encoder = OrdinalEncoder(
          encoding_method='arbitrary',
          variables=['cabin', 'embarked', 'sex'])

:class:`OrdinalEncoder()` will encode **all** categorical variables in the training set by default, unless we specify 
which variables to encode, as we did in the previous code block.

Let's fit the encoder so that it learns the mappings for each category:

.. code:: python

 encoder.fit(X_train)

The encoding mappings are stored in its `encoder_dict_` parameter. Let's display them:

.. code:: python

 encoder.encoder_dict_

In the `encoder_dict_` we find the integers that will replace each one of the categories of each variable to encode. With 
this dictionary, we can map the original value of the variable to the new value.

.. code:: python

 {'cabin': {'M': 0,
  'E': 1,
  'C': 2,
  'D': 3,
  'B': 4,
  'A': 5,
  'F': 6,
  'T': 7,
  'G': 8},
 'embarked': {'S': 0, 'C': 1, 'Q': 2, 'Missing': 3},
 'sex': {'female': 0, 'male': 1}}

According to the previous mappings, the category M in the variable cabin will be replaced by 0, the category E will be 
replaced by 1, and so on.

With the mappings ready, we can go ahead and transform data. The `transform()` method applies the learned mappings to the 
categorical features in the train and test sets, returning ordinal variables.

.. code:: python

    train_t = encoder.transform(X_train)
    test_t = encoder.transform(X_test)

    print(train_t.head())

In the following output, we see the resulting dataframe, where the original variable values in cabin, embarked and sex, 
are now replaced with integers:

.. code:: python

         pclass  sex        age     sibsp  parch   fare    cabin  embarked
 501        2      0     13.000000     0      1    19.5000     0       0
 588        2      0     4.000000      1      1    23.0000     0       0
 402        2      0     30.000000     1      0    13.8583     0       1
 1193       3      1     29.881135     0      0    7.7250      0       2
 686        3      0     22.000000     0      0    7.7250      0       2

Inverse transform
~~~~~~~~~~~~~~~~~

We can use the `inverse_transform()` method to revert the encoded values back to the original categories. This can be 
useful for model interpretation, debugging, or when we need to present results to stakeholders in their original 
categorical form.

.. code:: python

   train_inv = encoder.inverse_transform(train_t)

   print(train_inv.head())

The previous command returns a dataframe with the original category values:

.. code:: python

       pclass     sex        age      sibsp  parch     fare   cabin embarked
 501        2    female    13.000000      0      1    19.5000     M        S
 588        2    female     4.000000      1      1    23.0000     M        S
 402        2    female    30.000000      1      0    13.8583     M        C
 1193       3    male      29.881135      0      0     7.7250     M        Q
 686        3    female    22.000000      0      0     7.7250     M        Q

Encoding numerical variables
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Numerical variables can also be categorical in nature. :class:`OrdinalEncoder()` will only encode variables of data type 
object or categorical by default. However, we can encode numerical variables as well by setting `ignore_format=True`.

In the Titanic dataset, the variable **pclass** represents the class in which the passenger was traveling (that is, first 
class, second class, and third class). This variable is probably good as it is and doesn't require further data preprocessing, 
but to show how to encode numerical variables with :class:`OrdinalEncoder()`, we will treat it as categorical and proceed with 
ordinal encoding.﻿

Let's set up :class:`OrdinalEncoder()` to encode the variable pclass with ordinal numbers, and then fit it to the training 
set, so that it learns the mappings:

.. code:: python

 encoder = OrdinalEncoder(
    encoding_method='arbitrary',
    variables=['pclass'],
    ignore_format=True)

 train_t = encoder.fit_transform(X_train)

The `fit_transform()` method fits the encoder to the training data, learning the mappings for each category, and then 
transforms the training data using these mappings. Let's look at the resulting encodings.

.. code:: python

 encoder.encoder_dict_

The resulting encodings will be:

.. code:: python

 {'pclass': {2: 0, 3: 1, 1: 2}}

We see that the second class will be replaced by 0, the third class by 1, and the first class by 2.

If you want to see the resulting dataframe, go ahead and execute `train_t.head()`.

Ordered ordinal encoding
~~~~~~~~~~~~~~~~~~~~~~~~

Ordered encoding consists of assigning the integers based on the mean target.
We will use the **California Housing Dataset** to demonstrate ordered encoding. This dataset contains numeric features 
such as *MedInc*, *HouseAge* and *AveRooms*, among others. The target variable is *MedHouseVal* i.e., the median house 
value for California districts, expressed in hundreds of thousands of dollars ($100,000).

Let's first set up the dataset.

.. code:: python

 import pandas as pd
 import matplotlib.pyplot as plt
 from sklearn.model_selection import train_test_split
 from feature_engine.encoding import OrdinalEncoder
 from sklearn.datasets import fetch_california_housing

 housing = fetch_california_housing(as_frame=True)
 data = housing.frame

 print(data.head())

Below, we see the dataset:

.. code:: python

    MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \
 0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   
 1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   
 2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   
 3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   
 4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   

    Longitude  MedHouseVal  
 0    -122.23        4.526  
 1    -122.22        3.585  
 2    -122.24        3.521  
 3    -122.25        3.413  
 4    -122.25        3.422  

To demonstrate the power of ordered encoding, we will convert the `HouseAge` variable, which is continuous, into a 
categorical variable with four classes: *new*, *newish*, *old*, and *very old*.

.. code:: python

 data['HouseAgeCategorical'] = pd.qcut(data['HouseAge'], q=4, labels=['new',   'newish', 'old', 'very_old'])

 print(data[['HouseAge', 'HouseAgeCategorical']].head())

.. code:: python

      HouseAge HouseAgeCategorical
 0      41.0   very_old
 1      21.0     newish
 2      52.0   very_old
 3      52.0   very_old
 4      52.0   very_old

The categories of **HouseAgeCategorical** (*new*, *newish*, *old*, *very_old*) are discrete and represent ranges of 
house ages. They very likely have an ordinal relationship with the target, as older houses tend to be cheaper, making 
them a suitable candidate for ordered encoding.

Now let's split the data into training and test sets.

.. code:: python

 X = data.drop('MedHouseVal', axis=1)
 y = data['MedHouseVal']

 X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)

 print(X_train.head())

The training set now includes the categorical feature we created for *HouseAge*.

.. code:: python

       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \
 1989  1.9750      52.0  2.800000   0.700000       193.0  4.825000     36.73   
 256   2.2604      43.0  3.671480   1.184116       836.0  3.018051     37.77   
 7887  6.2990      17.0  6.478022   1.087912      1387.0  3.810440     33.87   
 4581  1.7199      17.0  2.518000   1.196000      3051.0  3.051000     34.06   
 1993  2.2206      50.0  4.622754   1.161677       606.0  3.628743     36.73   

      Longitude HouseAgeCategorical  
 1989    -119.79           very_old  
 256     -122.21           very_old  
 7887    -118.04                new  
 4581    -118.28                new  
 1993    -119.81           very_old  

Let's define the :class:`OrdinalEncoder()` to encode the categorical variable **HouseAgeCategorical** using **ordered** 
encoding.

.. code:: python

 ordered_encoder = OrdinalEncoder(
    encoding_method='ordered',
    variables=['HouseAgeCategorical']
 )

Let's fit the encoder so that it learns the mappings. Note that for ordered ordinal encoding, we need to pass the target 
variable to the `fit()` method:

.. code:: python

 X_train_t = ordered_encoder.fit_transform(X_train, y_train)
 X_test_t = ordered_encoder.transform(X_test)

Note that we first fit the encoder on the training data and then transformed both the training and test data, using the 
mappings learned from the training set.

Let's display the resulting dataframe:

.. code:: python

 print(X_train_t.head())

We can see the resulting dataframe below, where the variable HouseAgeCategorical now contains the encoded values.

.. code:: python

       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \
 1989  1.9750      52.0  2.800000   0.700000       193.0  4.825000     36.73   
 256   2.2604      43.0  3.671480   1.184116       836.0  3.018051     37.77   
 7887  6.2990      17.0  6.478022   1.087912      1387.0  3.810440     33.87   
 4581  1.7199      17.0  2.518000   1.196000      3051.0  3.051000     34.06   
 1993  2.2206      50.0  4.622754   1.161677       606.0  3.628743     36.73   

      Longitude   HouseAgeCategorical  
 1989    -119.79                    3  
 256     -122.21                    3  
 7887    -118.04                    0  
 4581    -118.28                    0  
 1993    -119.81                    3  

Let's check out the resulting mappings from category to integer:

.. code:: python
    
 ordered_encoder.encoder_dict_

We see the values that will be used to replace the categories in the following display:

.. code:: python

 {'HouseAgeCategorical': 
  {'new': 0, 
   'newish': 1, 
   'old': 2, 
   'very_old': 3}
 }

To understand the result, let's check out the mean target values for each category in **HouseAgeCategorical**:

.. code:: python

 test_set = X_test_t.join(y_test)
 mean_target_per_encoded_category = test_set[['HouseAgeCategorical', 'MedHouseVal']].groupby('HouseAgeCategorical').mean().reset_index()
 print(mean_target_per_encoded_category)

This will result in the following output:

.. code:: python

      HouseAgeCategorical  MedHouseVal
 0     0                   1.925929
 1     1                   2.043071
 2     2                   2.083013
 3     3                   2.237240

The categories were first sorted based on their target mean values, and then the numbers were assigned according to 
this order. For example, houses in the *very_old* age category encoded as '3' have an average median house value of 
approximately $223,724, while those in the *new* age category encoded as '0' have an average median house value of 
approximately $192,593. This is in principle, contrary to what we assumed in the first place: that older houses would be 
cheaper. But this is what the data tells us.

We can now plot the target mean value for each category after encoding for the test set to show the monotonic relationship.

.. code:: python

 mean_target_per_encoded_category['HouseAgeCategorical'] = mean_target_per_encoded_category['HouseAgeCategorical'].astype(str)
 plt.scatter(mean_target_per_encoded_category['HouseAgeCategorical'], mean_target_per_encoded_category['MedHouseVal'])
 plt.title('Mean target value per category')
 plt.xlabel('Encoded category')
 plt.ylabel('Mean target value')
 plt.show()

This will give us the following output:

.. figure::  ../../images/ordinal_encoding_monotonic.png
   :width: 400
   :height: 350
   :figclass: align-center
   :align: center

As we see in the above plot, ordered ordinal encoding was able to capture the monotonic relationship between the 
`HouseAgeCategorical` variable and the median house value, allowing the machine learning models to learn this trend that 
might otherwise go unnoticed.

The power of ordinal ordered encoder resides in its intrinsic capacity of finding monotonic relationships.

Additional resources
--------------------

In the following notebook, you can find more details into the :class:`OrdinalEncoder()`'s
functionality and example plots with the encoded variables:

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/encoding/OrdinalEncoder.ipynb>`_

For more details about this and other feature engineering methods check out these resources and tutorials:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/encoding/RareLabelEncoder.rst
================================================
.. _rarelabel_encoder:

.. currentmodule:: feature_engine.encoding

RareLabelEncoder
================

The :class:`RareLabelEncoder()` groups infrequent categories into one new category
called 'Rare' or a different string indicated by the user. We need to specify the
minimum percentage of observations a category should have to be preserved and the
minimum number of unique categories a variable should have to be re-grouped.

**tol**

In the parameter `tol` we indicate the minimum proportion of observations a category
should have, not to be grouped. In other words, categories which frequency, or proportion
of observations is <= `tol` will be grouped into a unique term.

**n_categories**

In the parameter `n_categories` we indicate the minimum cardinality of the categorical
variable in order to group infrequent categories. For example, if `n_categories=5`,
categories will be grouped only in those categorical variables with more than 5 unique
categories. The rest of the variables will be ignored.

This parameter is useful when we have big datasets and do not have time to examine all
categorical variables individually. This way, we ensure that variables with low cardinality
are not reduced any further.

**max_n_categories**

In the parameter `max_n_categories` we indicate the maximum number of unique categories
that we want in the encoded variable. If `max_n_categories=5`, then the most popular 5
categories will remain in the variable after the encoding, all other will be grouped into
a single category.

This parameter is useful if we are going to perform one hot encoding at the back of it,
to control the expansion of the feature space.

**Example**

Let's look at an example using the Titanic Dataset.

First, let's load the data and separate it into train and test:

.. code:: python

    from sklearn.model_selection import train_test_split
    from feature_engine.datasets import load_titanic
    from feature_engine.encoding import RareLabelEncoder

    X, y = load_titanic(
        return_X_y_frame=True,
        handle_missing=True,
        predictors_only=True,
        cabin="letter_only",
    )
    X["pclass"] = X["pclass"].astype("O")

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=0,
    )

    print(X_train.head())

We see the resulting data below:

.. code:: python

          pclass     sex        age  sibsp  parch     fare cabin embarked
    501        2  female  13.000000      0      1  19.5000     M        S
    588        2  female   4.000000      1      1  23.0000     M        S
    402        2  female  30.000000      1      0  13.8583     M        C
    1193       3    male  29.881135      0      0   7.7250     M        Q
    686        3  female  22.000000      0      0   7.7250     M        Q

Let's explore the number of uniue categories in the variable `"cabin"`.

.. code:: python

    X_train["cabin"].unique()

We see the number of unique categories in the output below:

.. code:: python

    array(['M', 'E', 'C', 'D', 'B', 'A', 'F', 'T', 'G'], dtype=object)

Now, we set up the :class:`RareLabelEncoder()` to group categories shown by less than 3%
of the observations into a new group or category called 'Rare'. We will group the
categories in the indicated variables if they have more than 2 unique categories each.

.. code:: python

    encoder = RareLabelEncoder(
        tol=0.03,
        n_categories=2,
        variables=['cabin', 'pclass', 'embarked'],
        replace_with='Rare',
    )

    # fit the encoder
    encoder.fit(X_train)

With `fit()`, the :class:`RareLabelEncoder()` finds the categories present in more than
3% of the observations, that is, those that will not be grouped. These categories can
be found in the `encoder_dict_` attribute.

.. code:: python

    encoder.encoder_dict_

In the `encoder_dict_` we find the most frequent categories per variable to encode.
Any category that is not in this dictionary, will be grouped.

.. code:: python

    {'cabin': ['M', 'C', 'B', 'E', 'D'],
    'pclass': [3, 1, 2],
    'embarked': ['S', 'C', 'Q']}

Now we can go ahead and transform the variables:

.. code:: python

    # transform the data
    train_t = encoder.transform(X_train)
    test_t = encoder.transform(X_test)

Let's now inspect the number of unique categories in the variable `"cabin"` after the
transformation:

.. code:: python

    train_t["cabin"].unique()

In the output below, we see that the infrequent categories have been replaced by
`"Rare"`.

.. code:: python

    array(['M', 'E', 'C', 'D', 'B', 'Rare'], dtype=object)

We can also specify the maximum number of categories that can be considered frequent
using the `max_n_categories` parameter.

Let's begin by creating a toy dataframe and count the values of observations per
category:

.. code:: python

    from feature_engine.encoding import RareLabelEncoder
    import pandas as pd
    data = {'var_A': ['A'] * 10 + ['B'] * 10 + ['C'] * 2 + ['D'] * 1}
    data = pd.DataFrame(data)
    data['var_A'].value_counts()

.. code:: python

    A    10
    B    10
    C     2
    D     1
    Name: var_A, dtype: int64

In this block of code, we group the categories only for variables with more than 3
unique categories and then we plot the result:

.. code:: python

    rare_encoder = RareLabelEncoder(tol=0.05, n_categories=3)
    rare_encoder.fit_transform(data)['var_A'].value_counts()

.. code:: python

    A       10
    B       10
    C        2
    Rare     1
    Name: var_A, dtype: int64

Now, we retain the 2 most frequent categories of the variable and group the rest into
the 'Rare' group:

.. code:: python

    rare_encoder = RareLabelEncoder(tol=0.05, n_categories=3, max_n_categories=2)
    Xt = rare_encoder.fit_transform(data)
    Xt['var_A'].value_counts()

.. code:: python

    A       10
    B       10
    Rare     3
    Name: var_A, dtype: int64

Tips
----

The :class:`RareLabelEncoder()` can be used to group infrequent categories and like this
control the expansion of the feature space if using one hot encoding.

Some categorical encodings will also return NAN if a category is present in the test
set, but was not seen in the train set. This inconvenient can usually be avoided if we
group rare labels before training the encoders.

Some categorical encoders will also return NAN if there is not enough observations for
a certain category. For example the :class:`WoEEncoder()` and the :class:`PRatioEncoder()`.
This behaviour can be also prevented by grouping infrequent labels before the encoding
with the :class:`RareLabelEncoder()`.

Additional resources
--------------------

In the following notebook, you can find more details into the :class:`RareLabelEncoder()`
functionality and example plots with the encoded variables:

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/encoding/RareLabelEncoder.ipynb>`_

For more details about this and other feature engineering methods check out these resources:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/encoding/StringSimilarityEncoder.rst
================================================
.. _string_similarity:

.. currentmodule:: feature_engine.encoding

StringSimilarityEncoder
=======================

The :class:`StringSimilarityEncoder()` replaces categorical variables with a set of float
variables that capture the similarity between the category names. The new variables
have values between 0 and 1, where 0 indicates no similarity and 1 is an exact
match between the names of the categories.

To calculate the similarity between the categories, :class:`StringSimilarityEncoder()`
uses Gestalt pattern matching. Under the hood, :class:`StringSimilarityEncoder()` uses
the `quick_ratio` method from the `SequanceMatcher()` from `difflib`.

The similarity is calculated as:

.. math::

    GPM = 2 M / T

where T is the total number of elements in both sequences and M is the number of matches.

For example, the similarity between the categories "dog" and "dig" is 0.66. T is the
total number of elements in both categories, that is 6. There are 2 matches between
the words, the letters d and g, so: 2 * M / T = 2 * 2 / 6 = 0.66.

Output of the :class:`StringSimilarityEncoder()`
------------------------------------------------

Let's create a dataframe with the categories "dog", "dig" and "cat":

.. code:: python

    import pandas as pd
    from feature_engine.encoding import StringSimilarityEncoder

    df = pd.DataFrame({"words": ["dog", "dig", "cat"]})
    df

We see the dataframe in the following output:

.. code:: python

      words
    0   dog
    1   dig
    2   cat

Let's now encode the variable:

.. code:: python

    encoder =  StringSimilarityEncoder()
    dft = encoder.fit_transform(df)
    dft

We see the encoded variables below:

.. code:: python

       words_dog  words_dig  words_cat
    0   1.000000   0.666667        0.0
    1   0.666667   1.000000        0.0
    2   0.000000   0.000000        1.0

Note that :class:`StringSimilarityEncoder()` replaces the original variables by the
distance variables.

:class:`StringSimilarityEncoder()` vs One-hot encoding
------------------------------------------------------

String similarity encoding is similar to one-hot encoding, in the sense that each category is
encoded as a new variable. But the values, instead of 1 or 0, are the similarity
between the observation's category and the dummy variable. It is suitable for poorly
defined (or 'dirty') categorical variables.

Encoding only popular categories
--------------------------------

The :class:`StringSimilarityEncoder()` can also create similarity variables for the *n* most popular
categories, *n* being determined by the user. For example, if we encode only the 6 more popular categories, by
setting the parameter `top_categories=6`, the transformer will add variables only
for the 6 most frequent categories. The most frequent categories are those with the largest
number of observations. This behaviour is useful when the categorical variables are highly cardinal,
to control the expansion of the feature space.

Specifying how :class:`StringSimilarityEncoder()` should deal with missing values
---------------------------------------------------------------------------------

The :class:`StringSimilarityEncoder()` has three options for dealing with missing values, which can be
specified with the parameter `missing_values`:

  1. Ignore NaNs (option `ignore`) - will leave the NaN in the resulting dataframe after transformation.
     Could be useful, if the next step in the pipeline is imputation or if the machine learning algorithm
     can handle missing data out-of-the-box.
  2. Impute NaNs (option `impute`) - will impute NaN with an empty string, and then calculate the similarity
     between the empty string and the variable's categories. Most of the time, the similarity value will be
     0 in resulting dataframe. This is the default option.
  3. Raise an error (option `raise`) - will raise an error if NaN is present during `fit`, `transform` or
     `fit_transform`. Could be useful for debugging and monitoring purposes.

Important
---------

:class:`StringSimilarityEncoder()` will encode unseen categories by out-of-the-box, by measuring the
string similarity to the seen categories.

No text preprocessing is applied by :class:`StringSimilarityEncoder()`. Be mindful of preparing
string categorical variables if needed.

:class:`StringSimilarityEncoder()` works with categorical variables by default. And it has the option to
encode numerical variables as well. This is useful, when the values of the numerical variables are more
useful as strings, than as numbers. For example, for variables like barcode.

Examples
--------

Let's look at an example using the Titanic Dataset. First we load the data and divide it
into a train and a test set:

.. code:: python

    import string
    from sklearn.model_selection import train_test_split
    from feature_engine.datasets import load_titanic
    from feature_engine.encoding import StringSimilarityEncoder

    def clean_titanic():
        translate_table = str.maketrans('' , '', string.punctuation)
        data = load_titanic()
        data['home.dest'] = (
        data['home.dest']
        .str.strip()
        .str.translate(translate_table)
        .str.replace('  ', ' ')
        .str.lower()
        )
        data['name'] = (
        data['name']
        .str.strip()
        .str.translate(translate_table)
        .str.replace('  ', ' ')
        .str.lower()
        )
        data['ticket'] = (
        data['ticket']
        .str.strip()
        .str.translate(translate_table)
        .str.replace('  ', ' ')
        .str.lower()
        )
        return data

    data = clean_titanic()
    # Separate into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(
        data.drop(['survived', 'sex', 'cabin', 'embarked'], axis=1),
        data['survived'],
        test_size=0.3,
        random_state=0
    )

    X_train.head()

Below, we see the first rows of the dataset:

.. code:: python

        pclass                             name  age  sibsp  parch  \
    501        2  mellinger miss madeleine violet   13      0      1   
    588        2                  wells miss joan    4      1      1   
    402        2     duran y more miss florentina   30      1      0   
    1193       3                 scanlan mr james  NaN      0      0   
    686        3       bradley miss bridget delia   22      0      0   

                ticket     fare boat body  \
    501         250644     19.5   14  NaN   
    588          29103       23   14  NaN   
    402   scparis 2148  13.8583   12  NaN   
    1193         36209    7.725  NaN  NaN   
    686         334914    7.725   13  NaN   

                                                home.dest  
    501                             england bennington vt  
    588                                 cornwall akron oh  
    402                       barcelona spain havana cuba  
    1193                                              NaN  
    686   kingwilliamstown co cork ireland glens falls ny 

Now, we set up the encoder to encode only the 2 most frequent categories of each of the
3 indicated categorical variables:

.. code:: python

    # set up the encoder
    encoder = StringSimilarityEncoder(
        top_categories=2,
        variables=['name', 'home.dest', 'ticket'],
        ignore_format=True
        )

    # fit the encoder
    encoder.fit(X_train)

With `fit()` the encoder will learn the most popular categories of the variables, which
are stored in the attribute `encoder_dict_`.

.. code:: python

	encoder.encoder_dict_

.. code:: python

    {
      'name': ['mellinger miss madeleine violet', 'barbara mrs catherine david'],
      'home.dest': ['', 'new york ny'],
      'ticket': ['ca 2343', 'ca 2144']
    }

The `encoder_dict_` contains the categories that will derive similarity variables for each
categorical variable.

With transform, we go ahead and encode the variables. Note that the
:class:`StringSimilarityEncoder()` will drop the original variables.

.. code:: python

    # transform the data
    train_t = encoder.transform(X_train)
    test_t = encoder.transform(X_test)

    test_t.head()

Below, we see the resulting dataframe:

.. code:: python

        pclass  age  sibsp  parch    fare boat body  \
    1139       3   38      0      0  7.8958  NaN  NaN   
    533        2   21      0      1      21   12  NaN   
    459        2   42      1      0      27  NaN  NaN   
    1150       3  NaN      0      0    14.5  NaN  NaN   
    393        2   25      0      0    31.5  NaN  NaN   

        name_mellinger miss madeleine violet  name_barbara mrs catherine david  \
    1139                              0.454545                          0.550000   
    533                               0.615385                          0.524590   
    459                               0.596491                          0.603774   
    1150                              0.641509                          0.693878   
    393                               0.408163                          0.666667   

        home.dest_nan  home.dest_new york ny  ticket_ca 2343  ticket_ca 2144  
    1139            1.0               0.000000        0.461538        0.461538  
    533             0.0               0.370370        0.307692        0.307692  
    459             0.0               0.352941        0.461538        0.461538  
    1150            1.0               0.000000        0.307692        0.307692  
    393             0.0               0.437500        0.666667        0.666667

More details
------------

For more details into :class:`StringSimilarityEncoder()`'s functionality visit:

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/encoding/StringSimilarityEncoder.ipynb>`_

All notebooks can be found in a `dedicated repository <https://github.com/feature-engine/feature-engine-examples>`_.

================================================
FILE: docs/user_guide/encoding/WoEEncoder.rst
================================================
.. _woe_encoder:

.. currentmodule:: feature_engine.encoding

Weight of Evidence (WoE)
========================

The term Weight of Evidence (WoE) can be traced to the financial sector, especially to
1983, when it took on an important role in describing the key components of credit risk
analysis and credit scoring. Since then, it has been used for medical research, GIS
studies, and more (see references below for review).

The WoE is a statistical data-driven method based on Bayes' theorem and the concepts of
prior and posterior probability, so the concepts of log odds, events, and non-events
are crucial to understanding how the weight of evidence works.

The WoE is only defined for binary classification problems. In other words, we can only
encode variables using the WoE when the target variable is binary.

Formulation
-----------

The weight of evidence is given by:

.. math::

    log( p(X=xj|Y = 1) / p(X=xj|Y=0) )

We discuss the formula in the next section.

Calculation
-----------

How is the WoE calculated? Let's say we have a dataset with a binary dependent variable
with two categories, 0 and 1, and a categorical predictor variable named variable A
with three categories (A1, A2, and A3). The dataset has the following characteristics:

- There are 20 positive (1) cases and 80 negative (0) cases in the target variable.
- Category A1 has 10 positive cases and 15 negative cases.
- Category A2 has 5 positive cases and 15 negative cases.
- Category A3 has 5 positive cases and 50 negative cases.

First, we find out the number of instances with a positive target value (1) per category,
and then we divide that by the total number of positive cases in the data. Then we determine
the number of instances with target value of 0 per category and divide that by the total
number of negative instances in the dataset:

- For category A1, we have 10 positive cases and 15 negative cases, resulting in a positive ratio of 10/20 and a negative ratio of 15/80. This means that the positive ratio is 0.5 and the negative ratio is 0.1875.
- For category A2, we have 5 positive cases out of 20 positive cases, giving us a  positive ratio of 5/20 and a negative ratio of 15/80. This results in a positive ratio of 0.25 and a negative ratio of 0.1875.
- For category A3, we have 5 positive cases out of 20 positive cases, resulting in a positive ratio of 5/20, and a 50/80 negative ratio. So the positive ratio is 0.25, and the negative ratio is 0.625.

Now we calculate the log of the ratio of positive cases in each category:

- For category A1, we have log (0.5/ 0.1875) = 0.98.
- For category A2, we have log (0.25/ 0.1875) = 0.28.
- For category A3, we have log (0.25/0.625) =-0.91.

Finally, we replace the categories (A1, A2, and A3) of the independent variable A with
the WoE values: 0.98, 0.28, -0.91.

Characteristics of the WoE
--------------------------

The beauty of the WoE, is that we can directly understand the impact of the category on
the probability of success (target variable being 1):

- If WoE values are negative, there are more negative cases than positive cases for the category.
- If WoE values are positive, there are more positive cases than negative cases for that category.
- If WoE is 0, then there is an equal number of positive and negative cases for that category.

In other words, for categories with positive WoE, the probability of success is high,
for categories with negative WoE, the probability of success is low, and for those with
WoE of zero, there are equal chances for both target outcomes.

Advantages of the WoE
---------------------

In addition to the intuitive interpretation of the WoE values, the WoE shows the following
advantages:

- It creates monotonic relationships between the encoded variable and the target.
- It returns numeric variables on a similar scale.

Uses of the WoE
---------------

In general, we use the WoE to encode both categorical and numerical variables. For
continuous variables, we first need to do binning, that is, sort the variables into
discrete intervals. You can do this by preprocessing the variable using any of
Feature-engine's discretizers.

Some authors have extended the Weight of Evidence approach to neural networks and other
algorithms, and although they have shown good results, the predictive modeling performance
of Weight of Evidence was superior when used with logistic regression models (see
reference below).

Limitations of the WoE
----------------------

As the methodology to calculate the WoE is based on ratios and logarithm, the WoE value
is not defined when `p(X=xj|Y = 1) = 0` or `p(X=xj|Y=0) = 0`. For the latter, the division
by 0 is not defined, and for the former, the log of 0 is not defined.

This occurs when a category shows only 1 of the possible values of the target (either it
always takes 1 or 0). In practice, this happens mostly when a category has a low frequency
in the dataset, that is, when only very few observations show that category.

To overcome this limitation, consider using a variable transformation method to group
those categories together, for example by using Feature-engine's :class:`RareLabelEncoder()`.

Taking into account the above considerations, conducting a detailed exploratory data
analysis (EDA) is essential as part of the data science and model-building process.
Integrating these considerations and practices not only enhances the feature engineering
process but also improves the performance of your models.

Unseen categories
-----------------

When using the WoE, we define the mappings, that is, the WoE values per category using
the observations from the training set. If the test set shows new (unseen) categories,
we'll lack a WoE value for them, and won't be able to encode them.

This is a known issue, without an elegant solution. If the new values appear in continuous
variables, consider changing the size and number of the intervals. If the unseen categories
appear in categorical variables, consider grouping low frequency categories before doing
the encoding.

WoEEncoder
----------

The :class:`WoEEncoder()` allows you to automate the process of calculating weight of
evidence for a given set of features. By default, :class:`WoEEncoder()` will encode all
categorical variables. You can encode just a subset by passing the variables names in a
list to the `variables` parameter.

By default, :class:`WoEEncoder()` will not encode numerical variables, instead, it will
raise an error. If you want to encode numerical, for example discrete variables, set
`ignore_format` to `True`.

:class:`WoEEncoder()` does not handle missing values automatically, so make sure to
replace them with a suitable value before the encoding. You can impute missing values
with Feature-engine's imputers.

:class:`WoEEncoder()` will ignore unseen categories by default, in which case, they will
be replaced by np.nan after the encoding. You have the option to make the encoder raise
an error instead, by setting `unseen='raise'`. You can also replace unseen categories
by an arbitrary value you need to define in `fill_value`, although we do not recommend
this option because it may lead to unpredictable results.

Python example
--------------

In the rest of the document, we'll show :class:`WoEEncoder()`'s functionality. Let's
look at an example using the Titanic Dataset.

First, let's load the data and separate the dataset into train and test:

.. code:: python

    from sklearn.model_selection import train_test_split
    from feature_engine.datasets import load_titanic
    from feature_engine.encoding import WoEEncoder, RareLabelEncoder

    X, y = load_titanic(
        return_X_y_frame=True,
        handle_missing=True,
        predictors_only=True,
        cabin="letter_only",
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=0,
    )

    print(X_train.head())

We see the resulting dataframe below:

.. code:: python

          pclass     sex        age  sibsp  parch     fare cabin embarked
    501        2  female  13.000000      0      1  19.5000     M        S
    588        2  female   4.000000      1      1  23.0000     M        S
    402        2  female  30.000000      1      0  13.8583     M        C
    1193       3    male  29.881135      0      0   7.7250     M        Q
    686        3  female  22.000000      0      0   7.7250     M        Q

Before we encode the variables, we group infrequent categories into one
category, which we'll call 'Rare'. For this, we use the :class:`RareLabelEncoder()` as
follows:

.. code:: python

    # set up a rare label encoder
    rare_encoder = RareLabelEncoder(
        tol=0.1,
        n_categories=2,
        variables=['cabin', 'pclass', 'embarked'],
        ignore_format=True,
    )

    # fit and transform data
    train_t = rare_encoder.fit_transform(X_train)
    test_t = rare_encoder.transform(X_train)

Note that we pass `ignore_format=True` because pclass is numeric.

Now, we set up :class:`WoEEncoder()` to replace the categories by the weight of the
evidence, only in the 3 indicated variables:

.. code:: python

    # set up a weight of evidence encoder
    woe_encoder = WoEEncoder(
        variables=['cabin', 'pclass', 'embarked'],
        ignore_format=True,
    )

    # fit the encoder
    woe_encoder.fit(train_t, y_train)

With `fit()` the encoder learns the weight of the evidence for each category, which are
stored in its `encoder_dict_` parameter:

.. code:: python

	woe_encoder.encoder_dict_

In the `encoder_dict_` we find the WoE for each one of the categories of the
variables to encode. This way, we can map the original values to the new values:

.. code:: python

    {'cabin': {'M': -0.35752781962490193, 'Rare': 1.083797390800775},
     'pclass': {'1': 0.9453018143294478,
      '2': 0.21009172435857942,
      '3': -0.5841726684724614},
     'embarked': {'C': 0.679904786667102,
      'Rare': 0.012075414091446468,
      'S': -0.20113381737960143}}

Now, we can go ahead and encode the variables:

.. code:: python

    train_t = woe_encoder.transform(train_t)
    test_t = woe_encoder.transform(test_t)

    print(train_t.head())

Below we see the resulting dataset with the weight of the evidence replacing the original
variable values:

.. code:: python

            pclass     sex        age  sibsp  parch     fare     cabin  embarked
    501   0.210092  female  13.000000      0      1  19.5000 -0.357528 -0.201134
    588   0.210092  female   4.000000      1      1  23.0000 -0.357528 -0.201134
    402   0.210092  female  30.000000      1      0  13.8583 -0.357528  0.679905
    1193 -0.584173    male  29.881135      0      0   7.7250 -0.357528  0.012075
    686  -0.584173  female  22.000000      0      0   7.7250 -0.357528  0.012075

WoE in categorical and numerical variables
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the previous example, we encoded only the variables 'cabin', 'pclass', 'embarked',
and left the rest of the variables untouched. In the following example, we will use
Feature-engine's pipeline to transform variables in sequence. We'll group rare categories
in categorical variables. Next, we'll discretize numerical variables. And finally, we'll
encode them all with the WoE.

First, let's load the data and separate it into train and test:

.. code:: python

    from sklearn.model_selection import train_test_split
    from feature_engine.datasets import load_titanic
    from feature_engine.encoding import WoEEncoder, RareLabelEncoder
    from feature_engine.pipeline import Pipeline
    from feature_engine.discretisation import EqualFrequencyDiscretiser

    X, y = load_titanic(
        return_X_y_frame=True,
        handle_missing=True,
        predictors_only=True,
        cabin="letter_only",
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=0,
    )

    print(X_train.head())

We see the resulting dataset below:

.. code:: python

          pclass     sex        age  sibsp  parch     fare cabin embarked
    501        2  female  13.000000      0      1  19.5000     M        S
    588        2  female   4.000000      1      1  23.0000     M        S
    402        2  female  30.000000      1      0  13.8583     M        C
    1193       3    male  29.881135      0      0   7.7250     M        Q
    686        3  female  22.000000      0      0   7.7250     M        Q

Let's define lists with the categorical and numerical variables:

.. code::: python

    categorical_features = ['cabin', 'pclass', 'embarked', 'sex', 'sibsp', 'parch']
    numerical_features = ['fare', 'age']
    all = categorical_features + numerical_features

Now, we will set up the pipeline to first discretize the numerical variables, then group
rare labels and low frequency intervals into a common group, and finally encode all
variables with the WoE:

.. code::: python

    pipe = Pipeline(
        [
            ("disc", EqualFrequencyDiscretiser(variables=numerical_features)),
            ("rare_label", RareLabelEncoder(tol=0.1, n_categories=2, variables=all, ignore_format=True)),
            ("woe", WoEEncoder(variables=all)),
        ])

We have created a variable transformation pipeline with the following steps:

- First, we use :class:`EqualFrequencyDiscretiser()` to do binning of the numerical variables.
- Next, we use :class:`RareLabelEncoder()` to group infrequent categories and intervals into one group.
- Finally, we use the :class:`WoEEncoder()` to replace values in all variables with the weight of the evidence.

Now, we can go ahead and fit the pipeline to the train set so that the different
transformers learn the parameters for the variable transformation.

.. code:: python

    X_trans_t = pipe.fit_transform(X_train, y_train)

    print(X_trans_t.head())

We see the resulting dataframe below:

.. code:: python

            pclass      sex       age     sibsp     parch      fare     cabin  \
    501   0.210092  1.45312  0.319176 -0.097278  0.764646  0.020285 -0.357528
    588   0.210092  1.45312  0.319176  0.458001  0.764646  0.248558 -0.357528
    402   0.210092  1.45312  0.092599  0.458001 -0.161255 -0.133962 -0.357528
    1193 -0.584173 -0.99882 -0.481682 -0.097278 -0.161255  0.020285 -0.357528
    686  -0.584173  1.45312  0.222615 -0.097278 -0.161255  0.020285 -0.357528

          embarked
    501  -0.201134
    588  -0.201134
    402   0.679905
    1193  0.012075
    686   0.012075

Finally, we can visualize the values of the WoE encoded variables respect to the original
values to corroborate the sigmoid function shape, which is the expected behavior of the
WoE:

.. code:: python

    import matplotlib.pyplot as plt
    age_woe = pipe.named_steps['woe'].encoder_dict_['age']

    sorted_age_woe = dict(sorted(age_woe.items(), key=lambda item: item[1]))
    categories = [str(k) for k in sorted_age_woe.keys()]
    log_odds = list(sorted_age_woe.values())

    plt.figure(figsize=(10, 6))
    plt.bar(categories, log_odds, color='skyblue')
    plt.xlabel('Age')
    plt.ylabel('WoE')
    plt.title('WoE for Age')
    plt.grid(axis='y')
    plt.show()

In the following plot, we can see the WoE for different categories of the variable
'age':

.. figure:: ../../images/woe_encoding.png
   :width: 600
   :figclass: align-center
   :align: left

|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

The WoE values are in the y-axis, and the categories are in the x-axis. We see that the
WoE values are monotonically increasing, which is the expected behavior of the WoE. If
we look at category 4, we can see the WoE is around -0.45 which means that in this age
bracket there was a small portion of positive cases (people who survived) compared to
negative cases (non-survivors). In other words, people within this age interval had
a low probability of survival.

Adding a model to the pipeline
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To complete the demo, we can add a logistic regression model to the pipeline to obtain
predictions of survival after the variable transformation.

.. code:: python

    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import accuracy_score

    pipe = Pipeline(
        [
            ("disc", EqualFrequencyDiscretiser(variables=numerical_features)),
            ("rare_label", RareLabelEncoder(tol=0.1, n_categories=2, variables=all, ignore_format=True)),
            ("woe", WoEEncoder(variables=all)),
            ('model', LogisticRegression(random_state=0)),
        ])

    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"Accuracy: {accuracy:.2f}")

The accuracy of the model is shown below:

.. code:: python

    Accuracy: 0.76

The accuracy of the model is 0.76, which is a good result for a first model. We can
improve the model by tuning the hyperparameters of the logistic regression model. Please
note that accuracy may not be the best metric for this problem, as the dataset is
imbalanced. We recommend using other metrics such as the F1 score, precision, recall, or
the ROC-AUC score. You can learn more about imbalance datasets in our
`course <https://www.trainindata.com/p/machine-learning-with-imbalanced-data>`_.

Weight of Evidence and Information Value
----------------------------------------

A common extension of the WoE is the information value (IV), which is a measure of the
predictive power of a variable. The IV is calculated as follows:

.. math::

    IV = \sum_{i=1}^{n} (p_{i} - q_{i}) \cdot WoE_{i}

Where, `pi` is the percentage of positive cases in the i-th category, `qi` is the
percentage of negative cases in the i-th category, and WoE_{i} is the weight of evidence
of the i-th category.

The IV is a measure of the predictive power of a variable. The higher the IV value, the
more predictive the variable is. So the combination of WoE with information value can be
used for feature selection for binary classification problems.

Weight of Evidence and Information Value within Feature-engine
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you're asking yourself whether Feature-engine allows you to automate this process,
the answer is: of course! You can utilize the :class:`SelectByInformationValue()` class
and it will handle all these steps for you. Again, remember the given considerations.

References
----------

- `Weight of Evidence: A Review of Concept and Methods <https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1539-6924.2005.00699.x>`_
- `Comparison and evaluation of landslide susceptibility maps obtained from weight of evidence, logistic regression, and artificial neural network models <https://link.springer.com/article/10.1007/s11069-018-3299-7>`_
- `Can weight of evidence, quantitative bias, and bounding methods evaluate robustness of real-world evidence for regulator and health technology assessment decisions on medical interventions <https://www.sciencedirect.com/science/article/pii/s0149291823003557>`_

Additional resources
--------------------

In the following notebooks, you can find more details into the :class:`WoEEncoder()`
functionality and example plots with the encoded variables:

- `WoE in categorical variables <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/encoding/WoEEncoder.ipynb>`_
- `WoE in numerical variables <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/discretisation/EqualFrequencyDiscretiser_plus_WoEEncoder.ipynb>`_

For more details about this and other feature engineering methods check out these resources:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/imputation/AddMissingIndicator.rst
================================================
.. _add_missing_indicator:

.. currentmodule:: feature_engine.imputation

AddMissingIndicator
===================

The :class:`AddMissingIndicator()` adds a binary variable indicating if observations are
missing (missing indicator). It adds missing indicators to both categorical and numerical
variables.

You can select the variables for which the missing indicators should be created passing
a variable list to the `variables` parameter. Alternatively, the imputer will
automatically select all variables.

The imputer has the option to add missing indicators to all variables or only to those
that have missing data in the train set. You can change the behaviour using the
parameter `missing_only`.

If `missing_only=True`, missing indicators will be added only to those variables with
missing data in the train set. This means that if you passed a variable list to
`variables` and some of those variables did not have missing data, no missing indicators
will be added to them. If it is paramount that all variables in your list get their
missing indicators, make sure to set `missing_only=False`.

It is recommended to use `missing_only=True` when not passing a list of variables to
impute.

Below a code example using the House Prices Dataset (more details about the dataset
:ref:`here <datasets>`).

First, let's load the data and separate it into train and test:

.. code:: python

	import numpy as np
	import pandas as pd
	import matplotlib.pyplot as plt
	from sklearn.model_selection import train_test_split

	from feature_engine.imputation import AddMissingIndicator

	# Load dataset
	data = pd.read_csv('houseprice.csv')

	# Separate into train and test sets
	X_train, X_test, y_train, y_test = train_test_split(
    	data.drop(['Id', 'SalePrice'], axis=1), data['SalePrice'], test_size=0.3, random_state=0)

Now we set up the imputer to add missing indicators to the 4 indicated variables:

.. code:: python

	# set up the imputer
	addBinary_imputer = AddMissingIndicator(
        variables=['Alley', 'MasVnrType', 'LotFrontage', 'MasVnrArea'],
        )

	# fit the imputer
	addBinary_imputer.fit(X_train)

Because we left the default value for `missing_only`, the :class:`AddMissingIndicator()`
will check if the variables indicated above have missing data in X_train. If they do,
missing indicators will be added for all 4 variables looking forward. If one of them
had not had missing data in X_train, missing indicators would have been added to the
remaining 3 variables only.

We can know which variables will have missing indicators by looking at the variable list
in the :class:`AddMissingIndicator()`'s attribute `variables_`.

Now, we can go ahead and add the missing indicators:

.. code:: python

	# transform the data
	train_t = addBinary_imputer.transform(X_train)
	test_t = addBinary_imputer.transform(X_test)

	train_t[['Alley_na', 'MasVnrType_na', 'LotFrontage_na', 'MasVnrArea_na']].head()

.. image:: ../../images/missingindicator.png
   :width: 500

Note that after adding missing indicators, we still need to replace NA in the original
variables if we plan to use them to train machine learning models.

Tip
---

Missing indicators are commonly used together with random sampling, mean or median
imputation, or frequent category imputation.

Additional resources
--------------------

In the following Jupyter notebook you will find more details on the functionality of the
:class:`AddMissingIndicator()`, including how to use the parameter `missing_indicator` and
how to select the variables automatically.

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/imputation/AddMissingIndicator.ipynb>`_

For more details about this and other feature engineering methods check out these resources:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/imputation/ArbitraryNumberImputer.rst
================================================
.. _arbitrary_number_imputer:

.. currentmodule:: feature_engine.imputation

ArbitraryNumberImputer
======================

The :class:`ArbitraryNumberImputer()` replaces missing data with an arbitrary numerical
value determined by the user. It works only with numerical variables.

The :class:`ArbitraryNumberImputer()` can find and impute all numerical variables
automatically. Alternatively, you can pass a list of the variables you want to impute
to the `variables` parameter.

You can impute all variables with the same number, in which case you need to define
the variables to impute in the `variables` parameter and the imputation number in
`arbitrary_number` parameter. For example, you can impute varA and varB with 99
like this:

.. code-block:: python

    transformer = ArbitraryNumberImputer(
            variables = ['varA', 'varB'],
            arbitrary_number = 99
            )

    Xt = transformer.fit_transform(X)

You can also impute different variables with different numbers. To do this, you need to
pass a dictionary with the variable names and the numbers to use for their imputation
to the `imputer_dict` parameter. For example, you can impute varA with 1 and varB
with 99 like this:

.. code-block:: python

    transformer = ArbitraryNumberImputer(
            imputer_dict = {'varA' : 1, 'varB': 99}
            )

    Xt = transformer.fit_transform(X)

Below a code example using the House Prices Dataset (more details about the dataset
:ref:`here <datasets>`).

First, let's load the data and separate it into train and test:

.. code:: python

	import numpy as np
	import pandas as pd
	import matplotlib.pyplot as plt
	from sklearn.model_selection import train_test_split

	from feature_engine.imputation import ArbitraryNumberImputer

	# Load dataset
	data = pd.read_csv('houseprice.csv')

	# Separate into train and test sets
	X_train, X_test, y_train, y_test = train_test_split(
                                    data.drop(['Id', 'SalePrice'], axis=1),
                                    data['SalePrice'],
                                    test_size=0.3,
                                    random_state=0,
                                    )

Now we set up the :class:`ArbitraryNumberImputer()` to impute 2 variables from the
dataset with the number -999:

.. code:: python

	# set up the imputer
	arbitrary_imputer = ArbitraryNumberImputer(
            arbitrary_number=-999,
            variables=['LotFrontage', 'MasVnrArea'],
            )

	# fit the imputer
	arbitrary_imputer.fit(X_train)

With `fit()`, the transformer does not learn any parameter. It just assigns the imputation
values to each variable, which can be found in the attribute `imputer_dict_`.

With transform, we replace the missing data with the arbitrary values both in train and
test sets:

.. code:: python

	# transform the data
	train_t= arbitrary_imputer.transform(X_train)
	test_t= arbitrary_imputer.transform(X_test)

Note that after the imputation, if the percentage of missing values is relatively big,
the variable distribution will differ from the original one (in red the imputed
variable):

.. code:: python

	fig = plt.figure()
	ax = fig.add_subplot(111)
	X_train['LotFrontage'].plot(kind='kde', ax=ax)
	train_t['LotFrontage'].plot(kind='kde', ax=ax, color='red')
	lines, labels = ax.get_legend_handles_labels()
	ax.legend(lines, labels, loc='best')

.. image:: ../../images/arbitraryvalueimputation.png

Additional resources
--------------------

In the following Jupyter notebook you will find more details on the functionality of the
:class:`ArbitraryNumberImputer()`, including how to select numerical variables automatically.
You will also see how to navigate the different attributes of the transformer.

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/imputation/ArbitraryNumberImputer.ipynb>`_

For more details about this and other feature engineering methods check out these resources:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/imputation/CategoricalImputer.rst
================================================
.. _categorical_imputer:

.. currentmodule:: feature_engine.imputation

CategoricalImputer
==================

Categorical data are common in most data science projects and can also show missing
values. There are **2 main imputation methods** that are used to replace missing data in
categorical variables. One method consists of replacing the missing values with the most
frequent category. The second method consists of replacing missing values with a dedicated
string, for example, "Missing."

Scikit-learn's machine learning algorithms can neither handle missing data nor categorical
variables out of the box. Hence, during data preprocessing, we need to use imputation
techniques to replace the nan values by any permitted value and then proceed with
categorical encoding, before training classification or regression models.

Handling missing values
-----------------------

Feature-engine's :class:`CategoricalImputer()` can replace missing data in categorical
variables with an arbitrary value, like the string 'Missing', or with the most frequent
category.

You can impute a subset of the categorical variables by passing their names to
:class:`CategoricalImputer()` in a list. Alternatively, the categorical imputer automatically
finds and imputes all variables of type object and categorical found in the training dataframe.

Originally, we designed this imputer to work only with categorical variables. In version
1.1.0, we introduced the parameter `ignore_format` to allow the imputer to also impute
numerical variables with this functionality. This is because, in some cases, variables
that are by nature categorical have numerical values.﻿

Python implementation
---------------------

We'll show the :class:`CategoricalImputer()`'s data imputation functionality using the
Ames house prices dataset. We'll start by loading the necessary libraries, functions and
classes, loading the dataset, and separating it into a training and a test set.

.. code:: python

    import matplotlib.pyplot as plt
    from sklearn.datasets import fetch_openml
    from sklearn.model_selection import train_test_split

    from feature_engine.imputation import CategoricalImputer

    data = fetch_openml(name='house_prices', as_frame=True)
    data = data.frame

    X = data.drop(['SalePrice', 'Id'], axis=1)
    y = data['SalePrice']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42)

    print(X_train.head())

In the following output we see the predictor variables of the house prices dataset:

.. code:: python

          MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \
    254           20       RL         70.0     8400   Pave   NaN      Reg
    1066          60       RL         59.0     7837   Pave   NaN      IR1
    638           30       RL         67.0     8777   Pave   NaN      Reg
    799           50       RL         60.0     7200   Pave   NaN      Reg
    380           50       RL         50.0     5000   Pave  Pave      Reg

         LandContour Utilities LotConfig  ... ScreenPorch PoolArea PoolQC  Fence  \
    254          Lvl    AllPub    Inside  ...           0        0    NaN    NaN
    1066         Lvl    AllPub    Inside  ...           0        0    NaN    NaN
    638          Lvl    AllPub    Inside  ...           0        0    NaN  MnPrv
    799          Lvl    AllPub    Corner  ...           0        0    NaN  MnPrv
    380          Lvl    AllPub    Inside  ...           0        0    NaN    NaN

         MiscFeature MiscVal  MoSold  YrSold  SaleType  SaleCondition
    254          NaN       0       6    2010        WD         Normal
    1066         NaN       0       5    2009        WD         Normal
    638          NaN       0       5    2008        WD         Normal
    799          NaN       0       6    2007        WD         Normal
    380          NaN       0       5    2010        WD         Normal

    [5 rows x 79 columns]

These 2 variables show null values, let's check that out:

.. code:: python

    X_train[['Alley', 'MasVnrType']].isnull().sum()

We see the null values in the following output:

.. code:: python

    Alley         1094
    MasVnrType       6
    dtype: int64

Imputation with an arbitrary string
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Let's set up the categorical imputer to impute these 2 variables with the arbitrary
string 'missing':

.. code:: python

   imputer = CategoricalImputer(
       variables=['Alley', 'MasVnrType'],
       fill_value="missing",
   )

   imputer.fit(X_train)

During fit, the transformer corroborates that the 2 variables are of type object or
categorical and creates a dictionary of variable to replacement value.

We can check the value that will be use to "fillna" as follows:

.. code:: python

    imputer.fill_value

We can check the dictionary with the replacement values per variable like this:

.. code:: python

    imputer.imputer_dict_

The dictionary contains the names of the variables in its keys and the imputation
value among its values. In this case, the result is not super exciting because we
are replacing nan values in all variables with the same value:

.. code:: python

    {'Alley': 'missing', 'MasVnrType': 'missing'}

We can now go ahead and impute the missing data and then plot the categories in the
resulting variable after the imputation:

.. code:: python

    train_t = imputer.transform(X_train)
    test_t = imputer.transform(X_test)

    test_t['MasVnrType'].value_counts().plot.bar()
    plt.ylabel("Number of observations")
    plt.show()

In the following plot, we see the presence of the category "missing", corresponding to
the imputed values:

.. image:: ../../images/missingcategoryimputer.png

|

Imputation with the most frequent category
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Let's now impute the variables with the most frequent category instead:

.. code:: python

   imputer = CategoricalImputer(
       variables=['Alley', 'MasVnrType'],
       imputation_method="frequent"
   )

   imputer.fit(X_train)

We can find the most frequent category per variable in the imputer dictionary:

.. code:: python

    imputer.imputer_dict_

In the following output, we see that the most frequent category for `Alley` is `'Grvl'` and the
most frequent value for `MasVnrType` is `'None'`.

.. code:: python

    {'Alley': 'Grvl', 'MasVnrType': 'None'}

We can now go ahead and impute the missing data to obtain a complete dataset, at least
for these 2 variables, and then plot the distribution of values after the
imputation:

.. code:: python

    train_t = imputer.transform(X_train)
    test_t = imputer.transform(X_test)

    test_t['MasVnrType'].value_counts().plot.bar()
    plt.ylabel("Number of observations")
    plt.show()

In the following image we see the resulting variable distribution:

.. image:: ../../images/frequentcategoryimputer.png

|

Automatically impute all categorical variables
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:class:`CategoricalImputer()` can automatically find and impute all categorical features
in the training dataset when we set the parameter `variables` to None:

.. code:: python

   imputer = CategoricalImputer(
       variables=None,
   )

   train_t = imputer.fit_transform(X_train)
   test_t = imputer.transform(X_test)

We can find the categorical variables in the `variables_` attribute:

.. code:: python

    imputer.variables_

Below, we see the list of categorical variables that were found in the training
dataframe:

.. code:: python

    ['MSZoning',
     'Street',
     'Alley',
     'LotShape',
     'LandContour',
     ...
     'SaleType',
     'SaleCondition']

Categorical features with 2 modes
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

It is possible that one variable has more than one mode. In that case, the
transformer will raise an error. For example, when you set the transformer to
impute the variable ‘PoolQC` with the most frequent value:

.. code:: python

   imputer = CategoricalImputer(
       variables=['PoolQC'],
       imputation_method="frequent"
   )

   imputer.fit(X_train)

'PoolQC`  has more than 1 mode, so the transformer raises the following error:

.. code:: python

        196     self.imputer_dict_ = {var: mode_vals[0]}
        198 # imputing multiple variables:
        199 else:
        200     # Returns a dataframe with 1 row if there is one mode per
        201     # variable, or more rows if there are more modes:

    ValueError: The variable PoolQC contains multiple frequent categories.

We can check that the variable has various modes like this:

.. code:: python

    X_train['PoolQC'].mode()

We see that this variable has 3 categories with similar maximum number of observations:

.. code:: python

    0    Ex
    1    Fa
    2    Gd
    Name: PoolQC, dtype: object

Considerations
--------------

Replacing missing values in categorical features with a bespoke category is standard
practice and perhaps the more natural thing to do. We'll probably want to impute with
the most frequent category when the percentage of missing values is small and the
cardinality of the variable is low, not to introduce unnecessary noise.

Combining imputation with data analysis is useful to decide the most convenient imputation
method as well as the impact of the imputation on the variable distribution. Note that the
variable distribution and its cardinality will affect the performance and workings of
machine learning models.

Imputation with the most frequent category will blend the missing values with the most
common values of the variable. Hence, it is common practice to add dummy variables to
indicate that the values were originally missing. See :class:`AddMissingIndicator`.

Additional resources
--------------------

For more details about this and other feature engineering methods check out these resources:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/imputation/DropMissingData.rst
================================================
.. _drop_missing_data:

.. currentmodule:: feature_engine.imputation

DropMissingData
===============

Removing rows with nan values from a dataset is a common practice in data science and
machine learning projects.

You are probably familiar with the use of pandas dropna. You basically take a pandas
dataframe or a pandas series, apply dropna, and eliminate those rows that contain nan
values in one or more columns.

Here, we have an example of that syntax:

.. code:: python

    import numpy as np
    import pandas as pd

    X = pd.DataFrame(dict(
           x1 = [np.nan,1,1,0,np.nan],
           x2 = ["a", np.nan, "b", np.nan, "a"],
           ))

    X.dropna(inplace=True)
    print(X)

The previous code returns a dataframe without missing values:

.. code:: python

        x1 x2
    2  1.0  b

Feature-engine's :class:`DropMissingData()` wraps pandas dropna in a transformer that
will remove rows with na values while adhering to scikit-learn's `fit` and `transform`
functionality.

Here we have a snapshot of :class:`DropMissingData()`'s syntax:

.. code:: python

    import pandas as pd
    import numpy as np
    from feature_engine.imputation import DropMissingData

    X = pd.DataFrame(dict(
           x1 = [np.nan,1,1,0,np.nan],
           x2 = ["a", np.nan, "b", np.nan, "a"],
           ))

    dmd = DropMissingData()
    dmd.fit(X)
    dmd.transform(X)

The previous code returns a dataframe without missing values:

.. code:: python

        x1 x2
    2  1.0  b

:class:`DropMissingData()` allows you therefore to remove null values as part of any
scikit-learn feature engineering workflow.

DropMissingData
---------------

:class:`DropMissingData()` has some advantages over pandas:

- It learns and stores the variables for which rows with nan values should be deleted.
- It can be used within a Scikit-learn like pipeline.

With :class:`DropMissingData()`, you can drop nan values from numerical and categorical
variables. In other words, you can remove null values from numerical, categorical or
object datatypes.

You have the option to remove nan values from all columns or only from a subset of
them. Alternatively, you can remove rows if they have more than a certain percentage of
nan values.

Let's better illustrate :class:`DropMissingData()`'s functionality through code examples.

Dropna
^^^^^^

Let's start by importing pandas and numpy, and creating a toy dataframe with nan values
in 2 columns:

.. code:: python

    import numpy as np
    import pandas as pd

    from feature_engine.imputation import DropMissingData

    X = pd.DataFrame(
        dict(
            x1=[2, 1, 1, 0, np.nan],
            x2=["a", np.nan, "b", np.nan, "a"],
            x3=[2, 3, 4, 5, 5],
        )
    )
    y = pd.Series([1, 2, 3, 4, 5])

    print(X.head())

Below we see the new dataframe:

.. code:: python

        x1   x2  x3
    0  2.0    a   2
    1  1.0  NaN   3
    2  1.0    b   4
    3  0.0  NaN   5
    4  NaN    a   5

We can drop nan values across all columns as follows:

.. code:: python

    dmd =  DropMissingData()
    Xt = dmd.fit_transform(X)
    Xt.head()

We see the transformed dataframe without null values:

.. code:: python

        x1 x2  x3
    0  2.0  a   2
    2  1.0  b   4

By default, :class:`DropMissingData()` will find and store the columns that had
missing data during fit, that is, in the training set. They are stored here:

.. code:: python

    dmd.variables_

.. code:: python

    ['x1', 'x2']

That means that every time that we apply `transform()` to a new dataframe, the
transformer will remove rows with nan values only in those columns.

If we want to force :class:`DropMissingData()` to drop na across all columns, regardless
of whether they had nan values during fit, we need to set up the class like this:

.. code:: python

    dmd =  DropMissingData(missing_only=False)
    Xt = dmd.fit_transform(X)

Now, when we explore the paramter `variables_`, we see that all the variables in the
train set are stored, and hence, will be used to remove nan values:

.. code:: python

    dmd.variables_

.. code:: python

    ['x1', 'x2', 'x3']

Adjust target after dropna
^^^^^^^^^^^^^^^^^^^^^^^^^^

:class:`DropMissingData()` has the option to remove rows with nan from both training set
and target variable. Like this, we can obtain a target that is aligned with the
resulting dataframe after the transformation.

The method `transform_x_y` removes rows with null values from the train set, and then
realigns the target. Let's take a look:

.. code:: python

    Xt, yt = dmd.transform_x_y(X, y)
    Xt

Below we see the dataframe without nan:

.. code:: python

        x1 x2  x3
    0  2.0  a   2
    2  1.0  b   4

.. code:: python

    yt

And here we see the target with those rows corresponing to the remaining rows in the
transformed dataframe:

.. code:: python

    0    1
    2    3
    dtype: int64

Let's check that the shape of the transformed dataframe and target are the same:

.. code:: python

    Xt.shape, yt.shape

We see that the resulting training set and target have each 2 rows, instead of the 5
original rows.

.. code:: python

    ((2, 3), (2,))

Return the rows with nan
^^^^^^^^^^^^^^^^^^^^^^^^

When we have a model in production, it might be useful to know which rows are being
removed by the transformer. We can obtain that information as follows:

.. code:: python

    dmd.return_na_data(X)

The previous command returns the rows with nan. In other words, it does the opposite
of `transform()`, or pandas.dropna.

.. code:: python

        x1   x2  x3
    1  1.0  NaN   3
    3  0.0  NaN   5
    4  NaN    a   5

Dropna from subset of variables
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We can choose to remove missing data only from a specific column or group of columns.
We just need to pass the column name or names to the `variables` parameter:

Here, we'll dropna from the variables "x1", "x3".

.. code:: python

    dmd = DropMissingData(variables=["x1", "x3"], missing_only=False)
    Xt = dmd.fit_transform(X)
    Xt.head()

Below, we see the transformed dataframe. It removed the rows with nan in "x1", and we
see that those rows with nan in "x2" are still in the dataframe:

.. code:: python

        x1   x2  x3
    0  2.0    a   2
    1  1.0  NaN   3
    2  1.0    b   4
    3  0.0  NaN   5

Only rows with nan in "x1" and "x3" are removed. We can corroborate that by examining
the `variables_` parameter:

.. code::python

    dmd.variables_

.. code::python

    ['x1', 'x3']

**Important**

When you indicate which variables should be examined to remove rows with nan, make sure
you set the parameter `missing_only` to the boolean `False`. Otherwise,
:class:`DropMissingData()` will select from your list only those variables that showed
nan values in the train set.

See for example what happens when we set up the class like this:

.. code:: python

    dmd = DropMissingData(variables=["x1", "x3"], missing_only=True)
    Xt = dmd.fit_transform(X)
    dmd.variables_

Note, that we indicated that we wanted to remove nan from "x1", "x3". Yet, only "x1"
has nan in X. So the transformer learns that nan should be only dropped from "x1":

.. code:: python

    ['x1']

:class:`DropMissingData()` took the 2 variables indicated in the list, and stored
only the one that showed nan in during fit. That means that when transforming future
dataframes, it will only remove rows with nan in "x1".

In other words, if you pass a list of variables to impute and set `missing_only=True`,
and some of the variables in your list do not have missing data in the train set,
missing data will not be removed during transform for those particular variables.

When `missing_only=True`, the transformer "double checks" that the entered
variables have missing data in the train set. If not, it ignores them during
`transform()`.

It is recommended to use `missing_only=True` when not passing a list of variables to
impute.

Dropna based on percentage of non-nan values
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

We can set :class:`DropMissingData()` to require a percentage of non-NA values in a
row to keep it. We can control this behaviour through the `threshold` parameter, which
is equivalent to pandas.dropna's `thresh` parameter.

If `threshold=1`, all variables need to have data to keep a row. If `threshold=0.5`,
50% of the variables need to have data to keep a row. If `threshold=0.01`, 10% of the
variables need to have data to keep the row. If `threshold=None`, rows with NA in any
of the variables will be dropped.

Let's see this with an example. We create a new dataframe that has different proportion
of non-nan values in every row.

.. code:: python

    X = pd.DataFrame(
        dict(
            x1=[2, 1, 1, np.nan, np.nan],
            x2=["a", np.nan, "b", np.nan, np.nan],
            x3=[2, 3, 4, 5, np.nan],
        )
    )
    X

We see that the bottom row has nan in all columns, row 3 has nan in 2 of 3 columns,
and row 1 has nan in 1 variable:

.. code:: python

        x1   x2   x3
    0  2.0    a  2.0
    1  1.0  NaN  3.0
    2  1.0    b  4.0
    3  NaN  NaN  5.0
    4  NaN  NaN  NaN

Now, we can set :class:`DropMissingData()` to drop rows if >50% of its values are nan:

.. code:: python

    dmd = DropMissingData(threshold=.5)
    dmd.fit(X)
    dmd.transform(X)

We see that the last 2 rows are dropped, because they have more than 50% nan values.

.. code:: python

        x1   x2   x3
    0  2.0    a  2.0
    1  1.0  NaN  3.0
    2  1.0    b  4.0

Instead, we can set class:`DropMissingData()` to drop rows if >70% of its values are
nan as follows:

.. code:: python

    dmd = DropMissingData(threshold=.3)
    dmd.fit(X)
    dmd.transform(X)

Now we see that only the last row was removed.

.. code:: python

        x1   x2   x3
    0  2.0    a  2.0
    1  1.0  NaN  3.0
    2  1.0    b  4.0
    3  NaN  NaN  5.0

Scikit-learn compatible
^^^^^^^^^^^^^^^^^^^^^^^

:class:`DropMissingData()` is fully compatible with the Scikit-learn API, so you will
find common methods that you also find in Scikit-learn transformers, like, for example,
the `get_feature_names_out()` method to obtain the variable names in the transformed
dataframe.

Pipeline
^^^^^^^^

When we dropna from a dataframe, we then need to realign the target. We saw previously
that we can do that by using the method `transform_x_y`.

We can align the target with the resulting dataframe automatically from within a
pipeline as well, by utilizing Feature-engine's pipeline.

Let's start by importing the necessary libraries:

.. code:: python

    import numpy as np
    import pandas as pd

    from feature_engine.imputation import DropMissingData
    from feature_engine.encoding import OrdinalEncoder
    from feature_engine.pipeline import Pipeline

Let's create a new dataframe with nan values in some rows, two numerical and one
categorical variable, and its corresponding target variable:

.. code:: python

    X = pd.DataFrame(
        dict(
            x1=[2, 1, 1, 0, np.nan],
            x2=["a", np.nan, "b", np.nan, "a"],
            x3=[2, 3, 4, 5, 5],
        )
    )
    y = pd.Series([1, 2, 3, 4, 5])

    X.head()

Below, we see the resulting dataframe:

.. code:: python

        x1   x2  x3
    0  2.0    a   2
    1  1.0  NaN   3
    2  1.0    b   4
    3  0.0  NaN   5
    4  NaN    a   5

Let's now set up a pipeline to dropna first, and then encode the categorical variable
by using ordinal encoding:

.. code:: python

    pipe = Pipeline(
        [
            ("drop", DropMissingData()),
            ("enc", OrdinalEncoder(encoding_method="arbitrary")),
        ]
    )

    pipe.fit_transform(X, y)

When we apply `fit` and `transform` or `fit_transform`, we will obtain the transformed
training set only:

.. code:: python

        x1  x2  x3
    0  2.0   0   2
    2  1.0   1   4

To obtain the transform training set and target, we use `transform_x_y`:

.. code:: python

    pipe.fit(X,y)
    Xt, yt = pipe.transform_x_y(X, y)
    Xt

Here we see the transformed training set:

.. code:: python

        x1  x2  x3
    0  2.0   0   2
    2  1.0   1   4

.. code:: python

    yt

And here we see the re-aligned target variable:

.. code:: python

    0    1
    2    3

And to wrap up, let's add an estimator to the pipeline:

.. code:: python

    import numpy as np
    import pandas as pd

    from sklearn.linear_model import Lasso

    from feature_engine.imputation import DropMissingData
    from feature_engine.encoding import OrdinalEncoder
    from feature_engine.pipeline import Pipeline

    df = pd.DataFrame(
        dict(
            x1=[2, 1, 1, 0, np.nan],
            x2=["a", np.nan, "b", np.nan, "a"],
            x3=[2, 3, 4, 5, 5],
        )
    )
    y = pd.Series([1, 2, 3, 4, 5])

    pipe = Pipeline(
        [
            ("drop", DropMissingData()),
            ("enc", OrdinalEncoder(encoding_method="arbitrary")),
            ("lasso", Lasso(random_state=2))
        ]
    )

    pipe.fit(df, y)
    pipe.predict(df)

.. code:: python

    array([2., 2.])

Dropna or fillna?
^^^^^^^^^^^^^^^^^

:class:`DropMissingData()` has the same functionality than `pandas.series.dropna` or
`pandas.dataframe.dropna``. If you want functionality compatible with `pandas.fillna`
instead, check our other imputation transformers.

Drop columns with nan
^^^^^^^^^^^^^^^^^^^^^

At the moment, Feature-engine does not have transformers that will find columns with a
certain percentage of missing values and drop them. Instead, you can find those columns
manually, and then drop them with the help of `DropFeatures` from the selection module.

See also
^^^^^^^^

Check out our tutorials on `LagFeatures` and `WindowFeatures` to see how to combine
:class:`DropMissingData()` with lags or rolling windows, to create features for
forecasting.

Tutorials, books and courses
^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In the following Jupyter notebook, in our accompanying Github repository, you will find
more examples using :class:`DropMissingData()`.

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/imputation/DropMissingData.ipynb>`_

For tutorials about this and other feature engineering methods check out our online course:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/imputation/EndTailImputer.rst
================================================
.. _end_tail_imputer:

.. currentmodule:: feature_engine.imputation

EndTailImputer
==============

The :class:`EndTailImputer()` replaces missing data with a value at the end of the distribution.
The value can be determined using the mean plus or minus a number of times the standard
deviation, or using the inter-quartile range proximity rule. The value can also be
determined as a factor of the maximum value.

You decide whether the missing data should be placed at the right or left tail of
the variable distribution.

In a sense, the :class:`EndTailImputer()` **"automates"** the work of the
:class:`ArbitraryNumberImputer()` because it will find automatically "arbitrary values"
far out at the end of the variable distributions.

:class:`EndTailImputer()` works only with numerical variables. You can impute only a
subset of the variables in the data by passing the variable names in a list. Alternatively,
the imputer will automatically select all numerical variables in the train set.

Below a code example using the House Prices Dataset (more details about the dataset
:ref:`here <datasets>`).

First, let's load the data and separate it into train and test:

.. code:: python

	import numpy as np
	import pandas as pd
	import matplotlib.pyplot as plt
	from sklearn.model_selection import train_test_split

	from feature_engine.imputation import EndTailImputer

	# Load dataset
	data = pd.read_csv('houseprice.csv')

	# Separate into train and test sets
	X_train, X_test, y_train, y_test = train_test_split(
                                            data.drop(['Id', 'SalePrice'], axis=1),
                                            data['SalePrice'],
                                            test_size=0.3,
                                            random_state=0,
                                            )

Now we set up the :class:`EndTailImputer()` to impute in this case only 2 variables
from the dataset. We instruct the imputer to find the imputation values using the mean
plus 3 times the standard deviation as follows:

.. code:: python

	# set up the imputer
	tail_imputer = EndTailImputer(imputation_method='gaussian',
                                  tail='right',
                                  fold=3,
                                  variables=['LotFrontage', 'MasVnrArea'])
	# fit the imputer
	tail_imputer.fit(X_train)

With fit, the :class:`EndTailImputer()` learned the imputation values for the indicated
variables and stored it in one of its attributes. We can now go ahead and impute both
the train and the test sets.

.. code:: python

	# transform the data
	train_t= tail_imputer.transform(X_train)
	test_t= tail_imputer.transform(X_test)

Note that after the imputation, if the percentage of missing values is relatively big,
the variable distribution will differ from the original one (in red the imputed
variable):

.. code:: python

	fig = plt.figure()
	ax = fig.add_subplot(111)
	X_train['LotFrontage'].plot(kind='kde', ax=ax)
	train_t['LotFrontage'].plot(kind='kde', ax=ax, color='red')
	lines, labels = ax.get_legend_handles_labels()
	ax.legend(lines, labels, loc='best')

.. image:: ../../images/endtailimputer.png

Additional resources
--------------------

In the following Jupyter notebook you will find more details on the functionality of the
:class:`CategoricalImputer()`, including how to select numerical variables automatically,
how to impute with the most frequent category, and how to impute with a used defined
string.

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/imputation/EndTailImputer.ipynb>`_

For more details about this and other feature engineering methods check out these resources:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/imputation/index.rst
================================================
.. -*- mode: rst -*-

Missing Data Imputation
=======================

Feature-engine's missing data imputers replace missing data by parameters estimated
from data or arbitrary values pre-defined by the user. The following image summarizes
the main imputer's functionality.

.. figure::  ../../images/summary/imputersSummary.png
   :align:   center

|

In this guide, you will find code snippets to quickly be able to apply the imputers
to your datasets, as well as general knowledge and guidance on the imputation
techniques.

Imputers
~~~~~~~~

.. toctree::
   :maxdepth: 1

   MeanMedianImputer
   ArbitraryNumberImputer
   EndTailImputer
   CategoricalImputer
   RandomSampleImputer
   AddMissingIndicator
   DropMissingData

================================================
FILE: docs/user_guide/imputation/MeanMedianImputer.rst
================================================
.. _mean_median_imputer:

.. currentmodule:: feature_engine.imputation

MeanMedianImputer
=================

Mean imputation and median imputation consist of replacing missing data in numerical 
variables with the variable's mean or median. These simple univariate missing data 
imputation techniques are among the most commonly used when preparing data for data 
science projects. Take a look, for example, at the winning solution of the KDD 2009 
cup: `Winning the KDD Cup Orange Challenge with Ensemble Selection <http://www.mtome.com/Publications/CiML/CiML-v3-book.pdf>`_. 

Typically, mean imputation and median imputation are done alongside adding missing 
indicators to tell the models that although this observation looks like the majority 
(that is, the mean or the median value), it was actually a missing value.

Mean Imputation vs Median Imputation
------------------------------------

In practice, we use mean imputation or median imputation without giving much thought to 
the distributions of the variables we want to impute. However, it's good to consider 
that the mean is a good estimate of the center of the variable when the variable has a 
symmetrical distribution. Therefore, we'd use mean imputation when the data shows a 
normal distribution, or the distribution is otherwise symmetrical, and median imputation 
when the variables are skewed.

Skewed distributions lead to biased estimates of the mean, making it an inaccurate 
representation of the distribution's center. In contrast, the median is robust to 
skewness. Additionally, the mean is sensitive to outliers, whereas the median remains 
unaffected. Therefore, in skewed distributions, the median is a better estimate of the 
center of mass.

In the following image, we see how the median is moved away from the distribution 
center when the variables have a strong left or right skew:

.. figure::  ../../images/1024px-Relationship_between_mean_and_median_under_different_skewness.png
   :align:   center
   :target: https:

These details tend to be more important when carrying out statistical analysis, 
and we tend to ignore them when preprocessing data for machine learning. 
However, keep in mind that some regression models and feature selection procedures, 
like ANOVA, make assumptions about the underlying distribution of the data.
You can complement your imputation methods with data analysis to understand how the 
imputation affects the variable's distribution and its relationship with other variables.

Effects on the variable distribution
------------------------------------

Replacing missing values with the mean or median affects the variable's distribution and
its relationships with other variables in the dataset. If there are a few missing
data points, these effects can be negligible. However, if a large percentage of data 
is missing, the impact becomes significant. 

Imputation with the mean or median reduces the variable's variability, such as its 
standard deviation, by adding more data points around the center of the distribution. 
With reduced variability, data points that were not previously considered outliers may 
now be flagged as outliers using simple detection methods like the IQR 
(interquartile range) proximity rule. 

In addition, mean and median imputation distort the relationship—such as correlation 
or covariance—between the imputed variable and other variables in the dataset, 
potentially affecting their relationship with the target variable as well. Hence, 
the outputs of models that rely on conditional joint probability estimates might be 
affected by mean imputation and median imputation, particularly if the percentage of 
missing data is large.

Mean and median imputation are often preferred when training linear regression or 
logistic regression models. In contrast, imputation with arbitrary numbers is commonly 
used with decision tree-based algorithms. When the relationship among the variables is 
crucial, you might want to consider better ways to estimate the missing data, such as 
multiple imputation (aka, multivariate imputation). 

MeanMedianImputer
-----------------

Feature-engine's :class:`MeanMedianImputer()` replaces missing data with the variable's 
mean or median value, determined over the observed values. Hence, it can only impute
numerical variables. You can pass the 
list of variables you want to impute, or alternatively, :class:`MeanMedianImputer()` 
will automatically impute all numerical variables in the training set.

Python implementation
---------------------

In this section, we will explore :class:`MeanMedianImputer()`'s functionality. Let's start by 
importing the required libraries:

.. code:: python

	import seaborn as sns
	import matplotlib.pyplot as plt
	from sklearn.datasets import fetch_openml
	from sklearn.pipeline import make_pipeline
	from sklearn.model_selection import train_test_split
	from feature_engine.imputation import MeanMedianImputer
	from feature_engine.imputation import AddMissingIndicator

We'll use the `house prices dataset <https://www.openml.org/search?type=data&status=active&id=42165>`_
from OpenML:

.. code:: python

	# Load dataset
	X, y = fetch_openml(name='house_prices', version=1, return_X_y=True, as_frame=True, parser='auto')	

In the following code chunk, we'll split the dataset into train and test retaining
only three features, and we'll set aside 4 observations with missing data from the test set:

.. code:: python

	target_features = ['Neighborhood','LotFrontage','MasVnrArea']
	X = X[target_features]

	X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

	# Select specific houses with missing data from the test set
	target_idx = [113,292,650, 1018]
	X_test_subset = X_test.loc[target_idx]

Let's visualize the subset of the test set with missing values:

.. code:: python

	print(X_test_subset)

In the following output, we see five houses; three of them with missing values for 
either LotFrontage or MasVnrArea:

.. code:: text

		 Neighborhood  LotFrontage  MasVnrArea
	113       Crawfor          NaN       184.0
	292       Edwards         60.0         0.0
	650       Somerst         65.0         NaN
	1018      Gilbert          NaN        76.0

Let's now set up and fit :class:`MeanMedianImputer()` with the strategy to mean, 
so we can impute the variables `LotFrontage` and `MasVnrArea`:

.. code:: python

	# Set up the imputer
	mmi = MeanMedianImputer(
		imputation_method='mean',
		variables=['LotFrontage', 'MasVnrArea']
	)

	# Fit transformer with training data
	mmi.fit(X_train)

It's worth noting that we have the flexibility to omit the `variables` parameter, 
in which case, :class:`MeanMedianImputer()` will automatically find and impute all 
numeric features.

After fitting :class:`MeanMedianImputer()`, we can check out the statistics 
(either mean or median; mean in this scenario) for each of the variables to impute:

.. code:: python

	# Show mean values learned with the training data
	mmi.imputer_dict_

The `imputer_dict_` attribute shows the learned statistics:

.. code:: text

	{'LotFrontage': 70.375, 'MasVnrArea': 105.26104023552503}

This dictionary is used internally to impute the missing values.

Let's transform the subset of the test data with the missing data we previewed earlier:

.. code:: python

	# Transform the subset of the test data
	X_test_subset_t = mmi.transform(X_test_subset)

If we now execute `X_test_subset_t.head()`, we'll see the completed data set, 
containing the imputed values:

.. code:: text

		 Neighborhood  LotFrontage  MasVnrArea
	113       Crawfor       70.375   184.00000
	292       Edwards       60.000     0.00000
	650       Somerst       65.000   105.26104
	1018      Gilbert       70.375    76.00000

Imputing missing values alongside missing indicators
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Mean or median imputation are commonly done alongside adding missing indicators.
We can add missing indicators with :class:`AddMissingIndicator()` from Feature-engine.

We can chain :class:`AddMissingIndicator()` with :class:`MeanMedianImputer()` using a 
`scikit-learn pipeline <https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html>`_.

For example, let's create an imputer pipeline to add missing indicators and then and 
impute the missing values:

.. code:: python

	# Create imputer pipeline
	imputer = make_pipeline(
		AddMissingIndicator(),
		MeanMedianImputer()
	)

	# Fit the pipeline
	imputer.fit(X_train)

Now, we can transform the data:

.. code:: python

	X_test_subset_t = imputer.transform(X_test_subset)

If we now execute `X_test_subset_t.head()`, we'll see a dataframe where `LotFrontage` 
and `MasVnrArea` are now complete, that is, missing values were replaced with the mean 
of the observed data, and additional columns with the missing indicators:

.. code:: text

		 Neighborhood  LotFrontage  MasVnrArea  LotFrontage_na  MasVnrArea_na
	113       Crawfor         70.0       184.0               1              0
	292       Edwards         60.0         0.0               0              0
	650       Somerst         65.0         0.0               0              1
	1018      Gilbert         70.0        76.0               1              0

The missing indicator columns flag those observations that were originally missing values.

Note that both classes automatically found the numerical variables, as
we haven't specified the `variables` parameter.

Distribution change after imputation
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Let's analyze how the imputation might change the distribution of the variables.

First, let's see how much missing data we have for LotFrontage in the training data:

.. code:: python

	X_train.LotFrontage.isnull().mean().round(2)

As a result, we can see nearly 19% of missing data:

.. code:: text

	0.19

In the following code, we'll create a plot with the raw variable distribution on the 
left panel and the distribution after imputation on the right panel:

.. code:: python

	# Customize plot
	sns.set(style="ticks")
	plt.rcParams['axes.grid'] = True
	plt.rcParams['grid.alpha'] = 0.5

	# Create figure
	fig,axes = plt.subplots(ncols=2, figsize=(10,4), sharex=True, sharey=True)

	# Plot histogram with KDE for the original data
	sns.histplot(data=X_train, x='LotFrontage', kde=True, ax=axes[0])
	axes[0].set_title('Original', weight='bold', y=1.05)

	# Plot histogram with KDE for the transformed data
	sns.histplot(data=mmi.transform(X_train), x='LotFrontage', kde=True, ax=axes[1])
	axes[1].set_title('Imputed', weight='bold', y=1.05)

	# Further customize plot
	sns.despine(offset=10, trim=True)
	plt.tight_layout(w_pad=4)

	plt.show()

After the imputation, we see on the right panel that more observations are now at the 
center of the distribution:

.. image:: ../../images/meanmedianimputater_distributions.png

Because of the increase in the number of observations at the center, the variance of 
the variable decreases, and the kurtosis coefficient increases.

Additional resources
--------------------

In the following python Jupyter notebook you will find more details on the functionality
of the :class:`MeanMedianImputer()`, including how to select numerical variables
automatically. You will also see how to navigate the different attributes of the 
transformer to find the mean or median values of the variables.

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/imputation/MeanMedianImputer.ipynb>`_

For more details about this and other feature engineering methods check out
these resources:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/imputation/RandomSampleImputer.rst
================================================
.. _random_sample_imputer:

.. currentmodule:: feature_engine.imputation

RandomSampleImputer
===================

The :class:`RandomSampleImputer()` replaces missing data with a random sample extracted from the
variable. It works with both numerical and categorical variables. A list of variables
can be indicated, or the imputer will automatically select all variables in the train
set.

**Note**

The random samples used to replace missing values may vary from execution to
execution. This may affect the results of your work. Thus, it is advisable to set a
seed.

Setting the seed
----------------

There are 2 ways in which the seed can be set in the :class:`RandomSampleImputer()`:

If `seed = 'general'` then the random_state can be either `None` or an integer.
The `random_state` then provides the seed to use in the imputation. All observations will
be imputed in one go with a single seed. This is equivalent to
`pandas.sample(n, random_state=seed)` where `n` is the number of observations with
missing data and `seed` is the number you entered in the `random_state`.

If `seed = 'observation'`, then the random_state should be a variable name
or a list of variable names. The seed will be calculated observation per
observation, either by adding or multiplying the values of the variables
indicated in the `random_state`. Then, a value will be extracted from the train set
using that seed and used to replace the NAN in that particular observation. This is the
equivalent of `pandas.sample(1, random_state=var1+var2)` if the `seeding_method` is
set to `add` or `pandas.sample(1, random_state=var1*var2)` if the `seeding_method`
is set to `multiply`.

For example, if the observation shows variables color: np.nan, height: 152, weight:52,
and we set the imputer as:

.. code:: python

	RandomSampleImputer(random_state=['height', 'weight'],
                                  seed='observation',
                                  seeding_method='add'))

the np.nan in the variable colour will be replaced using pandas sample as follows:

.. code:: python

	observation.sample(1, random_state=int(152+52))

For more details on why this functionality is important refer to the course
`Feature Engineering for Machine Learning <https://www.udemy.com/feature-engineering-for-machine-learning/>`_.

You can also find more details about this imputation in the following
`notebook <https://github.com/solegalli/feature-engineering-for-machine-learning/blob/master/Section-04-Missing-Data-Imputation/04.07-Random-Sample-Imputation.ipynb>`_.

Note, if the variables indicated in the `random_state` list are not numerical
the imputer will return an error. In addition, the variables indicated as seed
should not contain missing values themselves.

Important for GDPR
------------------

This estimator stores a copy of the training set when the `fit()` method is
called. Therefore, the object can become quite heavy. Also, it may not be GDPR
compliant if your training data set contains Personal Information. Please check
if this behaviour is allowed within your organisation.

Below a code example using the House Prices Dataset (more details about the dataset
:ref:`here <datasets>`).

First, let's load the data and separate it into train and test:

.. code:: python

	import numpy as np
	import pandas as pd
	import matplotlib.pyplot as plt
	from sklearn.model_selection import train_test_split

	from feature_engine.imputation import RandomSampleImputer

	# Load dataset
	data = pd.read_csv('houseprice.csv')

	# Separate into train and test sets
	X_train, X_test, y_train, y_test = train_test_split(
            data.drop(['Id', 'SalePrice'], axis=1),
            data['SalePrice'],
            test_size=0.3,
            random_state=0
        )

In this example, we sample values at random, observation per observation, using as seed
the value of the variable 'MSSubClass' plus the value of the variable 'YrSold'. Note
that this value might be different for each observation.

The :class:`RandomSampleImputer()` will impute all variables in the data, as we left the
default value of the parameter `variables` to `None`.

.. code:: python

	# set up the imputer
	imputer = RandomSampleImputer(
                random_state=['MSSubClass', 'YrSold'],
                seed='observation',
                seeding_method='add'
            )

	# fit the imputer
	imputer.fit(X_train)

With `fit()` the imputer stored a copy of the X_train. And with transform, it will extract
values at random from this X_train to replace NA in the datasets indicated in the `transform()`
methods.

.. code:: python

	# transform the data
	train_t = imputer.transform(X_train)
	test_t = imputer.transform(X_test)

The beauty of the random sampler is that it preserves the original variable distribution:

.. code:: python

	fig = plt.figure()
	ax = fig.add_subplot(111)
	X_train['LotFrontage'].plot(kind='kde', ax=ax)
	train_t['LotFrontage'].plot(kind='kde', ax=ax, color='red')
	lines, labels = ax.get_legend_handles_labels()
	ax.legend(lines, labels, loc='best')

.. image:: ../../images/randomsampleimputation.png

Additional resources
--------------------

In the following Jupyter notebook you will find more details on the functionality of the
:class:`RandomSampleImputer()`, including how to set the different types of seeds.

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/imputation/RandomSampleImputer.ipynb>`_

All Feature-engine notebooks can be found in a `dedicated repository <https://github.com/feature-engine/feature-engine-examples>`_.

And finally, there is also a lot of information about this and other imputation techniques
in this online course:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/outliers/ArbitraryOutlierCapper.rst
================================================
.. _arbitrary_capper:

.. currentmodule:: feature_engine.outliers

ArbitraryOutlierCapper
======================

The :class:`ArbitraryOutlierCapper()` caps the maximum or minimum values of a variable
at an arbitrary value indicated by the user. The maximum or minimum values should be
entered in a dictionary with the form {feature:capping value}.

Let's look at this in an example. First we load the Titanic dataset, and separate it
into a train and a test set:

.. code:: python

    from sklearn.model_selection import train_test_split
    from feature_engine.datasets import load_titanic
    from feature_engine.outliers import ArbitraryOutlierCapper

    X, y = load_titanic(
        return_X_y_frame=True,
        predictors_only=True,
        handle_missing=True,
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=0,
    )

    print(X_train.head())

We see the resulting data below:

.. code:: python

          pclass     sex        age  sibsp  parch     fare    cabin embarked
    501        2  female  13.000000      0      1  19.5000  Missing        S
    588        2  female   4.000000      1      1  23.0000  Missing        S
    402        2  female  30.000000      1      0  13.8583  Missing        C
    1193       3    male  29.881135      0      0   7.7250  Missing        Q
    686        3  female  22.000000      0      0   7.7250  Missing        Q

Now, we set up the :class:`ArbitraryOutlierCapper()` indicating that we want to cap the
variable 'age' at 50 and the variable 'Fare' at 200. We do not want to cap these variables
on the left side of their distribution.

.. code:: python

    capper = ArbitraryOutlierCapper(
        max_capping_dict={'age': 50, 'fare': 200},
        min_capping_dict=None,
    )

    capper.fit(X_train)

With `fit()` the transformer does not learn any parameter. It just reassigns the entered
dictionary to the attribute that will be used in the transformation:

.. code:: python

	capper.right_tail_caps_

.. code:: python

	{'age': 50, 'fare': 200}

Now, we can go ahead and cap the variables:

.. code:: python

	train_t = capper.transform(X_train)
	test_t = capper.transform(X_test)

If we now check the maximum values in the transformed data, they should be those entered
in the dictionary:

.. code:: python

    train_t[['fare', 'age']].max()

.. code:: python

    fare    200.0
    age      50.0
    dtype: float64

Additional resources
--------------------

You can find more details about the :class:`ArbitraryOutlierCapper()` functionality in the following
notebook:

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/outliers/ArbitraryOutlierCapper.ipynb>`_

For more details about this and other feature engineering methods check out these resources:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/outliers/index.rst
================================================
.. -*- mode: rst -*-

Outlier Handling
================

Feature-engine's outlier cappers cap maximum or minimum values of a variable at an
arbitrary or derived value. The OutlierTrimmer removes outliers from the dataset.

.. toctree::
   :maxdepth: 1

   Winsorizer
   ArbitraryOutlierCapper
   OutlierTrimmer

================================================
FILE: docs/user_guide/outliers/OutlierTrimmer.rst
================================================
.. _outlier_trimmer:

.. currentmodule:: feature_engine.outliers

OutlierTrimmer
===============

Outliers are data points that significantly deviate from the rest of the dataset, potentially indicating errors or rare
occurrences. Outliers can distort the learning process of machine learning models by skewing parameter estimates and
reducing predictive accuracy. To prevent this, if you suspect that the outliers are errors or rare occurrences, you can
remove them from the training data.

In this guide, we show how to remove outliers in Python using the :class:`OutlierTrimmer()`.

The first step to removing outliers consists of identifying those outliers. Outliers can be identified through various
statistical methods, such as box plots, z-scores, the interquartile range (IQR), or the median absolute deviation.
Additionally, visual inspection of the data using scatter plots or histograms is common practice in data science, and
can help detect observations that significantly deviate from the overall pattern of the dataset.

The :class:`OutlierTrimmer()` can identify outliers by using all of these methods and then remove them automatically.
Hence, we’ll begin this guide with data analysis, showing how we can identify outliers through these statistical methods
and boxplots, and then we will remove outliers by using the :class:`OutlierTrimmer()`.

Identifying outliers
--------------------

Outliers are data points that are usually far greater, or far smaller than some value that determines where most of the
values in the distribution lie. These minimum and maximum values, that delimit the data distribution, can be calculated
in 4 ways: by using the z-score if the variable is normally distributed, by using the interquartile range proximity rule
or the median absolute deviation if the variables are skewed, or by using percentiles.

Gaussian limits or z-score
~~~~~~~~~~~~~~~~~~~~~~~~~~

If the variable shows a normal distribution, most of its values lie between the mean minus 3 times the standard deviation
and the mean plus 3 times the standard deviation. Hence, we can determine the limits of the distribution as follows:

- right tail (upper_bound): mean + 3* std
- left tail (lower_bound): mean - 3* std

We can consider outliers those data points that lie beyond these limits.

Interquartile range proximity rule
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The interquartile range proximity rule can be used to detect outliers both in variables that show a normal distribution
and in variables with a skew. When using the IQR, we detect outliers as those values that lie before the 25th percentile
times a factor of the IQR, or after the 75th percentile times a factor of the IQR. This factor is normally 1.5, or 3 if
we want to be more stringent. With the IQR method, the limits are calculated as follows:

IQR limits:

- right tail (upper_limit): 75th quantile + 1.5* IQR
- left tail (lower_limit):  25th quantile - 1.5* IQR

where IQR is the inter-quartile range:

- IQR = 75th quantile - 25th quantile = third quartile - first quartile.

Observations found beyond those limits can be considered extreme values.

Maximum absolute deviation
~~~~~~~~~~~~~~~~~~~~~~~~~~

Parameters like the mean and the standard deviation are strongly affected by the presence of outliers. Therefore, it
might be a better solution to use a metric that is robust against outliers, like the median absolute deviation from the
median, commonly shortened to the median absolute deviation (MAD), to delimit the normal data distribution.

When we use MAD, we determine the limits of the distribution as follows:

MAD limits:

- right tail (upper_limit): median + 3.29* MAD
- left tail (lower_limit):  median - 3.29* MAD

MAD is the median absolute deviation from the median. In other words, MAD is the median value of the absolute difference
between each observation and its median.

- MAD = median(abs(X-median(X)))

Percentiles
~~~~~~~~~~~

A simpler way to determine the values that delimit the data distribution is by using percentiles. Like this, outlier
values would be those that lie before or after a certain percentile or quantiles:

- right tail: 95th percentile
- left tail:  5th percentile

The number of outliers identified by any of these methods will vary. These methods detect outliers, but they can’t
decide if they are true outliers or faithful data points. That required further examination and domain knowledge.

Let’s move on to removing outliers in Python.

Remove outliers in Python
-------------------------

In this demo, we'll identify and remove outliers from the Titanic Dataset. First, let's load the data and separate it
into train and test:

.. code:: python

    from sklearn.model_selection import train_test_split
    from feature_engine.datasets import load_titanic
    from feature_engine.outliers import OutlierTrimmer

    X, y = load_titanic(
        return_X_y_frame=True,
        predictors_only=True,
        handle_missing=True,
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=0,
    )

    print(X_train.head())

We see the resulting pandas dataframe below:

.. code:: python

          pclass     sex        age  sibsp  parch     fare    cabin embarked
    501        2  female  13.000000      0      1  19.5000  Missing        S
    588        2  female   4.000000      1      1  23.0000  Missing        S
    402        2  female  30.000000      1      0  13.8583  Missing        C
    1193       3    male  29.881135      0      0   7.7250  Missing        Q
    686        3  female  22.000000      0      0   7.7250  Missing        Q

Identifying outliers
~~~~~~~~~~~~~~~~~~~~

Let's now identify potential extreme values in the training set by using boxplots.

.. code:: python

    X_train.boxplot(column=['age', 'fare', 'sibsp'])
    plt.title("Box plot - outliers")
    plt.ylabel("variable values")
    plt.show()

In the following boxplots, we see that all three variables have data points that are significantly greater than the
majority of the data distribution. The variable age also shows outlier values towards the lower values.

.. figure::  ../../images/boxplot-titanic.png
   :align:   center

The variables have different scales, so let's plot them individually for better visualization. Let's start by making a
boxplot of the variable fare:

.. code:: python

    X_train.boxplot(column=['fare'])
    plt.title("Box plot - outliers")
    plt.ylabel("variable values")
    plt.show()

We see the boxplot in the following image:

.. figure::  ../../images/boxplot-fare.png
   :align:   center

Next, we plot the variable age:

.. code:: python

    X_train.boxplot(column=['age'])
    plt.title("Box plot - outliers")
    plt.ylabel("variable values")
    plt.show()

We see the boxplot in the following image:

.. figure::  ../../images/boxplot-age.png
   :align:   center

And finally, we make a boxplot of the variable sibsp:

.. code:: python

    X_train.boxplot(column=['sibsp'])
    plt.title("Box plot - outliers")
    plt.ylabel("variable values")
    plt.show()

We see the boxplot and the outlier values in the following image:

.. figure::  ../../images/boxplot-sibsp.png
   :align:   center

Outlier removal
~~~~~~~~~~~~~~~

Now, we will use the :class:`OutlierTrimmer()` to remove outliers. We'll start by using the IQR as outlier detection
method.

IQR
^^^

We want to remove outliers at the right side of the distribution only (param `tail`). We want the maximum values to be
determined using the 75th quantile of the variable (param `capping_method`) plus 1.5 times the IQR (param `fold`). And
we only want to cap outliers in 2 variables, which we indicate in a list.

.. code:: python

    ot = OutlierTrimmer(capping_method='iqr',
                        tail='right',
                        fold=1.5,
                        variables=['sibsp', 'fare'],
                        )

    ot.fit(X_train)

With `fit()`, the :class:`OutlierTrimmer()` finds the values at which it should cap the variables. These values are
stored in one of its attributes:

.. code:: python

    ot.right_tail_caps_

.. code:: python

   {'sibsp': 2.5, 'fare': 66.34379999999999}

We can now go ahead and remove the outliers:

.. code:: python

    train_t = ot.transform(X_train)
    test_t = ot.transform(X_test)

We can compare the sizes of the original and transformed datasets to check that the outliers were removed:

.. code:: python

    X_train.shape, train_t.shape

We see that the transformed dataset contains less rows:

.. code:: python

    ((916, 8), (764, 8))

If we evaluate now the maximum of the variables in the transformed datasets, they should be <= the values observed in
the attribute `right_tail_caps_`:

.. code:: python

    train_t[['fare', 'age']].max()

.. code:: python

    fare    65.0
    age     53.0
    dtype: float64

Finally, we can check the boxplots of the transformed variables to corroborate the effect on their distribution.

.. code:: python

    train_t.boxplot(column=['sibsp', "fare"])
    plt.title("Box plot - outliers")
    plt.ylabel("variable values")
    plt.show()

We see the boxplot and the `sibsp` does no longer have outliers, but as `fare` was very skewed, when removing outliers,
the parameters of the IQR change, and we continue to see outliers:

.. figure::  ../../images/boxplot-sibsp-fare-iqr.png
   :align:   center

We'll come back to this later, but now let's continue showing the functionality of the :class:`OutlierTrimmer()`.

When we remove outliers from the datasets, we then need to re-align the target variables. We can do this with pandas
loc. But the :class:`OutlierTrimmer()` can do that automatically as follows:

.. code:: python

    train_t, y_train_t = ot.transform_x_y(X_train, y_train)
    test_t, y_test_t = ot.transform_x_y(X_test, y_test)

The method `transform_x_y` will remove outliers from the predictor datasets and then align the target variable. That
means, it will remove from the target those rows corresponding to the outlier values.

We can corroborate the size adjustment in the target as follows:

.. code:: python

    y_train.shape, y_train_t.shape,

The previous command returns the following output:

.. code:: python

    ((916,), (764,))

We can obtain the names of the fetaures in the transformed dataset as follows:

.. code:: python

    ot.get_feature_names_out()

That returns the following variable namesL

.. code:: python

    ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'cabin', 'embarked']

MAD
^^^

We saw that the IQR did not work amazingly for the variable fare, because its skew is too big. So let's remove outliers
by using the MAD instead:

.. code:: python

    ot = OutlierTrimmer(capping_method='mad',
                        tail='right',
                        fold=3,
                        variables=['fare'],
                        )

    ot.fit(X_train)

    train_t, y_train_t = ot.transform_x_y(X_train, y_train)
    test_t, y_test_t = ot.transform_x_y(X_test, y_test)

    train_t.boxplot(column=["fare"])
    plt.title("Box plot - outliers")
    plt.ylabel("variable values")
    plt.show()

In the following image, we see that after this transformation, the variable fare no longer shows outlier values:

.. figure::  ../../images/boxplot-fare-mad.png
   :align:   center

Z-score
^^^^^^^

The variable age is more homogeneously distributed across its value range, so let's use the z-score or gaussian
approximation to detect outliers. We saw in the boxplot that it has outliers at both ends, so we'll cap both ends of the
distribution:

.. code:: python

    ot_age = OutlierTrimmer(capping_method='gaussian',
                        tail="both",
                        fold=3,
                        variables=['age'],
                        )

    ot_age.fit(X_train)

Let's inspect the maximum values beyond which data points will be considered outliers:

.. code:: python

    ot_age.right_tail_caps_

.. code:: python

    {'age': 67.73951212364803}

And the lower values beyond which data points will be considered outliers:

.. code:: python

    ot_age.left_tail_caps_

.. code:: python

    {'age': -7.410476010820627}

The minimum value does not make sense, because age can't be negative. So, we'll try capping this variable with
percentiles instead.

Percentiles
^^^^^^^^^^^

We'll cap age at the bottom 5 and top 95 percentile:

.. code:: python

    ot = OutlierTrimmer(capping_method='mad',
                        tail='right',
                        fold=0.05,
                        variables=['fare'],
                        )

    ot.fit(X_train)

Let's inspect the maximum values beyond which data points will be considered outliers:

.. code:: python

    ot_age.right_tail_caps_

.. code:: python

    {'age': 54.0}

And the lower values beyond which data points will be considered outliers:

.. code:: python

    ot_age.left_tail_caps_

.. code:: python

    {'age': 9.0}

Let's tranform the dataset and target:

.. code:: python

    train_t, y_train_t = ot_age.transform_x_y(X_train, y_train)
    test_t, y_test_t = ot_age.transform_x_y(X_test, y_test)

And plot the resulting variable:

.. code:: python

    train_t.boxplot(column=['age'])
    plt.title("Box plot - outliers")
    plt.ylabel("variable values")
    plt.show()

In the following image, we see that after this transformation, the variable age still shows some outlier values towards
its higher values, so we should be more stringent with the percentiles or use MAD:

.. figure::  ../../images/boxplot-age-percentiles.png
   :align:   center

Pipeline
--------

The :class:`OutlierTrimmer()` removes observations from the predictor data sets. If we want to use this transformer
within a Pipeline, we can't use Scikit-learn's pipeline because it can't readjust the target. But we can use
Feature-engine's pipeline instead.

Let's start by creating a pipeline that removes outliers and then encodes categorical variables:

.. code:: python

    from feature_engine.encoding import OneHotEncoder
    from feature_engine.pipeline import Pipeline

    pipe = Pipeline(
        [
            ("outliers", ot),
            ("enc", OneHotEncoder()),
        ]
    )

    pipe.fit(X_train, y_train)

The `transform` method will transform only the dataset with the predictors, just like scikit-learn's pipeline:

.. code:: python

    train_t = pipe.transform(X_train)

    X_train.shape, train_t.shape

We see the adjusted data size compared to the original size here:

.. code:: python

   ((916, 8), (736, 76))

Feature-engine's pipeline can also adjust the target:

.. code:: python

    train_t, y_train_t = pipe.transform_x_y(X_train, y_train)

    y_train.shape, y_train_t.shape

We see the adjusted data size compared to the original size here:

.. code:: python

    ((916,), (736,))

To wrap up, let's add a machine learning algorithm to the pipeline. We'll use logistic regression to predict survival:

.. code:: python

    from sklearn.linear_model import LogisticRegression

    pipe = Pipeline(
        [
            ("outliers", ot),
            ("enc", OneHotEncoder()),
            ("logit", LogisticRegression(random_state=10)),
        ]
    )

    pipe.fit(X_train, y_train)

Now, we can predict survival:

.. code:: python

    preds = pipe.predict(X_train)

    preds[0:10]

We see the following output:

.. code:: python

    array([1, 1, 1, 0, 1, 0, 1, 1, 0, 1], dtype=int64)

We can obtain the probability of survival:

.. code:: python

    preds = pipe.predict_proba(X_train)

    preds[0:10]

We see the following output:

.. code:: python

    array([[0.13027536, 0.86972464],
           [0.14982143, 0.85017857],
           [0.2783799 , 0.7216201 ],
           [0.86907159, 0.13092841],
           [0.31794531, 0.68205469],
           [0.86905145, 0.13094855],
           [0.1396715 , 0.8603285 ],
           [0.48403632, 0.51596368],
           [0.6299007 , 0.3700993 ],
           [0.49712853, 0.50287147]])

We can obtain the accuracy of the predictions over the test set:

.. code:: python

    pipe.score(X_test, y_test)

That returns the following accuracy:

.. code:: python

    0.7823343848580442

We can obtain the names of the features after the trasnformation:

.. code:: python

    pipe[:-1].get_feature_names_out()

That returns the following names:

.. code:: python

    ['pclass',
     'age',
     'sibsp',
     'parch',
     'fare',
     'sex_female',
     'sex_male',
     'cabin_Missing',
    ...

And finally, we can obtain the transformed dataset and target as follows:

.. code:: python

    X_test_t, y_test_t = pipe[:-1].transform_x_y(X_test, y_test)

    X_test.shape, X_test_t.shape

We see the resulting sizes here:

.. code:: python

    ((393, 8), (317, 76))

Setting up the stringency (param `fold`)
----------------------------------------

By default, :class:`OutlierTrimmer()` automatically determines the parameter fold based
on the chosen `capping_method`. This parameter determines the multiplier for standard
deviation, interquartile range (IQR), or Median Absolute Deviation (MAD), or
sets the percentile at which to cap the variables.

The default values for fold are as follows:

- 'gaussian': `fold` is set to 3.0;
- 'iqr': `fold` is set to 1.5;
- 'mad': `fold` is set to 3.29;
- 'percentiles': `fold` is set to 0.05.

You can manually adjust the fold value to make the outlier detection process more or less
conservative, thus customizing the extent of outlier trimming.

Tutorials, books and courses
----------------------------

In the following Jupyter notebook, in our accompanying Github repository, you will find more examples using
:class:`OutlierTrimmer()`.

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/outliers/OutlierTrimmer.ipynb>`_

For tutorials about this and other feature engineering methods check out our online course:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/outliers/Winsorizer.rst
================================================
.. _winsorizer:

.. currentmodule:: feature_engine.outliers

Winsorizer
==========

The :class:`Winsorizer()` caps maximum and/or minimum values of a variable at automatically
determined values. The minimum and maximum values can be calculated in 1 of 3 different ways:

Gaussian limits:

- right tail: mean + 3* std
- left tail: mean - 3* std

IQR limits:

- right tail: 75th quantile + 1.5* IQR
- left tail:  25th quantile - 1.5* IQR

where IQR is the inter-quartile range: 75th quantile - 25th quantile.

MAD limits:

    - right tail: median + 3.29* MAD
    - left tail:  median - 3.29* MAD

where MAD is the median absolute deviation from the median.

percentiles or quantiles:

- right tail: 95th percentile
- left tail:  5th percentile

**Example**

Let's cap some outliers in the Titanic Dataset. First, let's load the data and separate
it into train and test:

.. code:: python

    from sklearn.model_selection import train_test_split
    from feature_engine.datasets import load_titanic
    from feature_engine.outliers import Winsorizer

    X, y = load_titanic(
        return_X_y_frame=True,
        predictors_only=True,
        handle_missing=True,
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=0,
    )

    print(X_train.head())

We see the resulting data below:

.. code:: python

          pclass     sex        age  sibsp  parch     fare    cabin embarked
    501        2  female  13.000000      0      1  19.5000  Missing        S
    588        2  female   4.000000      1      1  23.0000  Missing        S
    402        2  female  30.000000      1      0  13.8583  Missing        C
    1193       3    male  29.881135      0      0   7.7250  Missing        Q
    686        3  female  22.000000      0      0   7.7250  Missing        Q

Now, we will set the :class:`Winsorizer()` to cap outliers at the right side of the
distribution only (param `tail`). We want the maximum values to be determined using the
mean value of the variable (param `capping_method`) plus 3 times the standard deviation
(param `fold`). And we only want to cap outliers in 2 variables, which we indicate in a
list.

.. code:: python

    capper = Winsorizer(capping_method='gaussian',
                        tail='right', 
                        fold=3, 
                        variables=['age', 'fare'])

    capper.fit(X_train)

With `fit()`, the :class:`Winsorizer()` finds the values at which it should cap the variables.
These values are stored in its attribute:

.. code:: python

    capper.right_tail_caps_

.. code:: python

	{'age': 67.73951212364803, 'fare': 174.70395336846678}

We can now go ahead and censor the outliers:

.. code:: python

    # transform the data
    train_t = capper.transform(X_train)
    test_t = capper.transform(X_test)
    
If we evaluate now the maximum of the variables in the transformed datasets, they should
coincide with the values observed in the attribute `right_tail_caps_`:

.. code:: python

    train_t[['fare', 'age']].max()

.. code:: python

    fare    174.703953
    age      67.739512
    dtype: float64

Setting up the stringency (param `fold`)
----------------------------------------

By default, :class:`Winsorizer()` automatically determines the parameter `fold` based
on the chosen `capping_method`. This parameter determines the multiplier for standard
deviation, interquartile range (IQR), or Median Absolute Deviation (MAD), or
sets the percentile at which to cap the variables.

The default values for fold are as follows:

- 'gaussian': `fold` is set to 3.0;
- 'iqr': `fold` is set to 1.5;
- 'mad': `fold` is set to 3.29;
- 'quantiles': `fold` is set to 0.05.

You can manually adjust the `fold` value to make the outlier detection process more or less
conservative, thus customizing the extent of outlier capping.

Additional resources
--------------------

You can find more details about the :class:`Winsorizer()` functionality in the following
notebook:

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/outliers/Winsorizer.ipynb>`_

For more details about this and other feature engineering methods check out these resources:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/pipeline/index.rst
================================================
.. -*- mode: rst -*-

Pipeline
========

Feature-engine's Pipeline is equivalent to Scikit-learn's pipeline, and in addition,
it accepts the method `transform_x_y`, to adjust both X and y, in those cases where
rows are removed from X.

.. toctree::
   :maxdepth: 1

   Pipeline
   make_pipeline

================================================
FILE: docs/user_guide/pipeline/make_pipeline.rst
================================================
.. _make_pipeline:

.. currentmodule:: feature_engine.pipeline

make_pipeline
=============

:class:`make_pipeline` is a shorthand for :class:`Pipeline`. While to set up :class:`Pipeline`
we create tuples with step names and transformers or estimators, with :class:`make_pipeline`
we just add a sequence of transformers and estimators, and the names will be added automatically.

Setting up a Pipeline with make_pipeline
----------------------------------------

In the following example, we set up a `Pipeline` that drops missing data, then replaces categories with ordinal
numbers, and finally fits a Lasso regression model.

.. code:: python

    import numpy as np
    import pandas as pd
    from feature_engine.imputation import DropMissingData
    from feature_engine.encoding import OrdinalEncoder
    from feature_engine.pipeline import make_pipeline

    from sklearn.linear_model import Lasso

    X = pd.DataFrame(
        dict(
            x1=[2, 1, 1, 0, np.nan],
            x2=["a", np.nan, "b", np.nan, "a"],
        )
    )
    y = pd.Series([1, 2, 3, 4, 5])

    pipe = make_pipeline(
        DropMissingData(),
        OrdinalEncoder(encoding_method="arbitrary"),
        Lasso(random_state=10),
    )
    # predict
    pipe.fit(X, y)
    preds_pipe = pipe.predict(X)
    preds_pipe

In the output we see the predictions made by the pipeline:

.. code:: python

    array([2., 2.])

The names of the pipeline were assigned automatically:

.. code:: python

   print(pipe)

.. code:: python

    Pipeline(steps=[('dropmissingdata', DropMissingData()),
                    ('ordinalencoder', OrdinalEncoder(encoding_method='arbitrary')),
                    ('lasso', Lasso(random_state=10))])

The pipeline returned by :class:`make_pipeline` has exactly the same characteristics than
:class:`Pipeline`. Hence, for additional guidelines, check out the :class:`Pipeline`
documentation.

Forecasting
-----------

Let's set up another pipeline to do direct forecasting:

.. code:: python

    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd

    from sklearn.linear_model import Lasso
    from sklearn.metrics import root_mean_squared_error
    from sklearn.multioutput import MultiOutputRegressor

    from feature_engine.timeseries.forecasting import (
        LagFeatures,
        WindowFeatures,
    )
    from feature_engine.pipeline import make_pipeline

We'll use the Australia electricity demand dataset described here:

Godahewa, Rakshitha, Bergmeir, Christoph, Webb, Geoff, Hyndman, Rob, & Montero-Manso, Pablo. (2021). Australian
Electricity Demand Dataset (Version 1) [Data set]. Zenodo. https:

.. code:: python

    url = "https://raw.githubusercontent.com/tidyverts/tsibbledata/master/data-raw/vic_elec/VIC2015/demand.csv"
    df = pd.read_csv(url)

    df.drop(columns=["Industrial"], inplace=True)

    # Convert the integer Date to an actual date with datetime type
    df["date"] = df["Date"].apply(
        lambda x: pd.Timestamp("1899-12-30") + pd.Timedelta(x, unit="days")
    )

    # Create a timestamp from the integer Period representing 30 minute intervals
    df["date_time"] = df["date"] + \
        pd.to_timedelta((df["Period"] - 1) * 30, unit="m")

    df.dropna(inplace=True)

    # Rename columns
    df = df[["date_time", "OperationalLessIndustrial"]]

    df.columns = ["date_time", "demand"]

    # Resample to hourly
    df = (
        df.set_index("date_time")
        .resample("h")
        .agg({"demand": "sum"})
    )

    print(df.head())

Here, we see the first rows of data:

.. code:: python

                              demand
    date_time
    2002-01-01 00:00:00  6919.366092
    2002-01-01 01:00:00  7165.974188
    2002-01-01 02:00:00  6406.542994
    2002-01-01 03:00:00  5815.537828
    2002-01-01 04:00:00  5497.732922

We'll predict the next 3 hours of energy demand. We'll use direct forecasting. Let's
create the target variable:

.. code:: python

    horizon = 3
    y = pd.DataFrame(index=df.index)
    for h in range(horizon):
        y[f"h_{h}"] = df.shift(periods=-h, freq="h")
    y.dropna(inplace=True)
    df = df.loc[y.index]
    print(y.head())

This is our target variable:

.. code:: python

                                 h_0          h_1          h_2
    date_time
    2002-01-01 00:00:00  6919.366092  7165.974188  6406.542994
    2002-01-01 01:00:00  7165.974188  6406.542994  5815.537828
    2002-01-01 02:00:00  6406.542994  5815.537828  5497.732922
    2002-01-01 03:00:00  5815.537828  5497.732922  5385.851060
    2002-01-01 04:00:00  5497.732922  5385.851060  5574.731890

Next, we split the data into a training set and a test set:

.. code:: python

    end_train = '2014-12-31 23:59:59'
    X_train = df.loc[:end_train]
    y_train = y.loc[:end_train]

    begin_test = '2014-12-31 17:59:59'
    X_test  = df.loc[begin_test:]
    y_test = y.loc[begin_test:]

Next, we set up `LagFeatures` and `WindowFeatures` to create features from lags and windows:

.. code:: python

    lagf = LagFeatures(
        variables=["demand"],
        periods=[1, 3, 6],
        missing_values="ignore",
        drop_na=True,
    )

    winf = WindowFeatures(
        variables=["demand"],
        window=["3h"],
        freq="1h",
        functions=["mean"],
        missing_values="ignore",
        drop_original=True,
        drop_na=True,
    )

We wrap the lasso regression within the multioutput regressor to predict multiple targets:

.. code:: python

    lasso = MultiOutputRegressor(Lasso(random_state=0, max_iter=10))

Now, we assemble `Pipeline`:

.. code:: python

    pipe = make_pipeline(lagf, winf, lasso)

    print(pipe)

The steps' names were assigned automatically:

.. code:: python

    Pipeline(steps=[('lagfeatures',
                     LagFeatures(drop_na=True, missing_values='ignore',
                                 periods=[1, 3, 6], variables=['demand'])),
                    ('windowfeatures',
                     WindowFeatures(drop_na=True, drop_original=True, freq='1h',
                                    functions=['mean'], missing_values='ignore',
                                    variables=['demand'], window=['3h'])),
                    ('multioutputregressor',
                     MultiOutputRegressor(estimator=Lasso(max_iter=10,
                                                          random_state=0)))])

Let's fit the Pipeline:

.. code:: python

    pipe.fit(X_train, y_train)

Now, we can make forecasts for the test set:

.. code:: python

    forecast = pipe.predict(X_test)

    forecasts = pd.DataFrame(
        pipe.predict(X_test),
        columns=[f"step_{i+1}" for i in range(3)]

    )

    print(forecasts.head())

We see the 3 hr ahead energy demand prediction for each hour:

.. code:: python

            step_1       step_2       step_3
    0  8031.043352  8262.804811  8484.551733
    1  7017.158081  7160.568853  7496.282999
    2  6587.938171  6806.903940  7212.741943
    3  6503.807479  6789.946587  7195.796841
    4  6646.981390  6970.501840  7308.359237

To learn more about direct forecasting and how to create features, check out our courses:

.. figure::  ../../images/fetsf.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Time Series Forecasting

.. figure::  ../../images/fwml.png
   :width: 300
   :figclass: align-center
   :align: right
   :target: https:

   Forecasting with Machine Learning

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/pipeline/Pipeline.rst
================================================
.. _pipeline:

.. currentmodule:: feature_engine.pipeline

Pipeline
========
:class:`Pipeline` facilitates the chaining together of multiple estimators into a unified sequence. This proves beneficial
as data processing frequently involves a predefined series of actions, such as feature selection, normalization, and
training a machine learning model.

Feature-engine's :class:`Pipeline` is different from scikit-learn's Pipeline in that our :class:`Pipeline` supports
transformers that remove rows from the dataset, like `DropMissingData`, `OutlierTrimmer`, `LagFeatures` and `WindowFeatures`.

When observations are removed from the training data set, :class:`Pipeline` invokes the method `transform_x_y`
available in these transformers, to adjust the target variable to the remaining rows.

The Pipeline serves various functions in this context:

**Simplicity and encapsulation:**

You need only call the `fit` and `predict` functions once on your data to fit an entire sequence of estimators.

**Hyperparameter Optimization:**

Grid search and random search can be performed over hyperparameters of all estimators in the pipeline simultaneously.

**Safety**

Using a pipeline prevent the leakage of statistics from test data into the trained model during cross-validation, by
ensuring that the same data is used to fit the transformers and predictors.

Pipeline functions
------------------

Calling the `fit` function on the pipeline, is the same as calling `fit` on each individual estimator sequentially,
transforming the input data and forwarding it to the subsequent step.

The pipeline will have all the methods present in the final estimator within it. For instance, if the last estimator is
a classifier, the Pipeline can function as a classifier. Similarly, if the last estimator is a transformer, the pipeline inherits this functionality as well.

Setting up a Pipeline
---------------------

The :class:`Pipeline` is constructed utilizing a list of (key, value) pairs, wherein the key represents the desired
name for the step, and the value denotes an estimator or a transformer object.

In the following example, we set up a :class:`Pipeline` that drops missing data, then replaces categories with ordinal
numbers, and finally fits a Lasso regression model.

.. code:: python

    import numpy as np
    import pandas as pd
    from feature_engine.imputation import DropMissingData
    from feature_engine.encoding import OrdinalEncoder
    from feature_engine.pipeline import Pipeline

    from sklearn.linear_model import Lasso

    X = pd.DataFrame(
        dict(
            x1=[2, 1, 1, 0, np.nan],
            x2=["a", np.nan, "b", np.nan, "a"],
        )
    )
    y = pd.Series([1, 2, 3, 4, 5])

    pipe = Pipeline(
        [
            ("drop", DropMissingData()),
            ("enc", OrdinalEncoder(encoding_method="arbitrary")),
            ("lasso", Lasso(random_state=10)),
        ]
    )
    # predict
    pipe.fit(X, y)
    preds_pipe = pipe.predict(X)
    preds_pipe

In the output we see the predictions made by the pipeline:

.. code:: python

    array([2., 2.])

Accessing Pipeline steps
------------------------

The :class:`Pipeline`'s estimators are stored as a list within the `steps` attribute. We can use slicing notation to
obtain a subset or partial pipeline within the Pipeline. This functionality is useful for executing specific
transformations or their inverses selectively.

For example, this notation extracts the first step of the pipeline:

.. code:: python

   pipe[:1]

.. code:: python

   Pipeline(steps=[('drop', DropMissingData())])

This notation extracts the first **two** steps of the pipeline:

.. code:: python

   pipe[:2]

.. code:: python

   Pipeline(steps=[('drop', DropMissingData()),
                ('enc', OrdinalEncoder(encoding_method='arbitrary'))])

This notation extracts the last step of the pipeline:

.. code:: python

   pipe[-1:]

.. code:: python

   Pipeline(steps=[('lasso', Lasso(random_state=10))])

We can also select specific steps of the pipeline to check their attributes. For example,
we can check the coefficients of the Lasso algorithm as follows:

.. code:: python

    pipe.named_steps["lasso"].coef_

And we see the coefficients:

.. code:: python

    array([-0.,  0.])

There was no relationship between the target and the variables, so it's fine to obtain
these coefficients.

Let's instead check the ordinal encoder mappings for the categorical variables:

.. code:: python

    pipe.named_steps["enc"].encoder_dict_

We see the integers used to replace each category:

.. code:: python

    {'x2': {'a': 0, 'b': 1}}

Finding feature names in a Pipeline
-----------------------------------

The :class:`Pipeline` includes a `get_feature_names_out()` method, similar to other transformers. By employing
pipeline slicing, you can obtain the feature names entering each step.

Let's set up a Pipeline that adds new features to the dataset to make this more interesting:

.. code:: python

    import numpy as np
    import pandas as pd
    from feature_engine.imputation import DropMissingData
    from feature_engine.encoding import OneHotEncoder
    from feature_engine.pipeline import Pipeline

    from sklearn.linear_model import Lasso

    X = pd.DataFrame(
        dict(
            x1=[2, 1, 1, 0, np.nan],
            x2=["a", np.nan, "b", np.nan, "a"],
        )
    )
    y = pd.Series([1, 2, 3, 4, 5])

    pipe = Pipeline(
        [
            ("drop", DropMissingData()),
            ("enc", OneHotEncoder()),
            ("lasso", Lasso(random_state=10)),
        ]
    )
    pipe.fit(X, y)

In the first step of the pipeline, no features are added, we just drop rows with `nan`. So if we execute
`get_feature_names_out()` we should see just the 2 variables from the input dataframe:

.. code:: python

    pipe[:1].get_feature_names_out()

.. code:: python

    ['x1', 'x2']

In the second step, we add binary variables for each category of x2, so x2 should disappear, and in its place, we
should see the binary variables:

.. code:: python

    pipe[:2].get_feature_names_out()

.. code:: python

    ['x1', 'x2_a', 'x2_b']

The last step is an estimator, that is, a machine learning model. Estimators don't support the method
`get_feature_names_out()`. So if we apply this method to the entire pipeline, we'll get an error.

Accessing nested parameters
---------------------------

We can re-define, or re-set the parameters of the transformers and estimators within the pipeline. This is done under
the hood by the Grid search and random search. But in case you need to change a parameter in a step of the
:class:`Pipeline`, this is how you do it:

.. code:: python

    pipe.set_params(lasso__alpha=10)

Here, we changed the alpha of the lasso regression algorithm to 10.

Best use: Dropping rows during data preprocessing
-------------------------------------------------

Feature-engine's :class:`Pipeline` was designed to support transformers that remove rows from the dataset, like
`DropMissingData`, `OutlierTrimmer`, `LagFeatures` and `WindowFeatures`.

We saw earlier in this page how to use :class:`Pipeline` with `DropMissingData`. Let's now take a look at how to
combine :class:`Pipeline` with `LagFeatures` and `WindowFeaures` to do multiple step forecasting.

We start by making imports:

.. code:: python

    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd

    from sklearn.linear_model import Lasso
    from sklearn.metrics import root_mean_squared_error
    from sklearn.multioutput import MultiOutputRegressor

    from feature_engine.timeseries.forecasting import (
        LagFeatures,
        WindowFeatures,
    )
    from feature_engine.pipeline import Pipeline

We'll use the Australia electricity demand dataset described here:

Godahewa, Rakshitha, Bergmeir, Christoph, Webb, Geoff, Hyndman, Rob, & Montero-Manso, Pablo. (2021). Australian
Electricity Demand Dataset (Version 1) [Data set]. Zenodo. https:

.. code:: python

    url = "https://raw.githubusercontent.com/tidyverts/tsibbledata/master/data-raw/vic_elec/VIC2015/demand.csv"
    df = pd.read_csv(url)

    df.drop(columns=["Industrial"], inplace=True)

    # Convert the integer Date to an actual date with datetime type
    df["date"] = df["Date"].apply(
        lambda x: pd.Timestamp("1899-12-30") + pd.Timedelta(x, unit="days")
    )

    # Create a timestamp from the integer Period representing 30 minute intervals
    df["date_time"] = df["date"] + \
        pd.to_timedelta((df["Period"] - 1) * 30, unit="m")

    df.dropna(inplace=True)

    # Rename columns
    df = df[["date_time", "OperationalLessIndustrial"]]

    df.columns = ["date_time", "demand"]

    # Resample to hourly
    df = (
        df.set_index("date_time")
        .resample("h")
        .agg({"demand": "sum"})
    )

    print(df.head())

Here, we see the first rows of data:

.. code:: python

                              demand
    date_time
    2002-01-01 00:00:00  6919.366092
    2002-01-01 01:00:00  7165.974188
    2002-01-01 02:00:00  6406.542994
    2002-01-01 03:00:00  5815.537828
    2002-01-01 04:00:00  5497.732922

We'll predict the next 6 hours of energy demand. We'll use direct forecasting. Hence, we need to create 6 target
variables, one for each step in the horizon:

.. code:: python

    horizon = 6
    y = pd.DataFrame(index=df.index)
    for h in range(horizon):
        y[f"h_{h}"] = df.shift(periods=-h, freq="h")
    y.dropna(inplace=True)
    df = df.loc[y.index]
    print(y.head())

This is our target variable:

.. code:: python

                                 h_0          h_1          h_2          h_3  \
    date_time
    2002-01-01 00:00:00  6919.366092  7165.974188  6406.542994  5815.537828
    2002-01-01 01:00:00  7165.974188  6406.542994  5815.537828  5497.732922
    2002-01-01 02:00:00  6406.542994  5815.537828  5497.732922  5385.851060
    2002-01-01 03:00:00  5815.537828  5497.732922  5385.851060  5574.731890
    2002-01-01 04:00:00  5497.732922  5385.851060  5574.731890  5457.770634

                                 h_4          h_5
    date_time
    2002-01-01 00:00:00  5497.732922  5385.851060
    2002-01-01 01:00:00  5385.851060  5574.731890
    2002-01-01 02:00:00  5574.731890  5457.770634
    2002-01-01 03:00:00  5457.770634  5698.152000
    2002-01-01 04:00:00  5698.152000  5938.337614

Next, we split the data into a training set and a test set:

.. code:: python

    end_train = '2014-12-31 23:59:59'
    X_train = df.loc[:end_train]
    y_train = y.loc[:end_train]

    begin_test = '2014-12-31 17:59:59'
    X_test  = df.loc[begin_test:]
    y_test = y.loc[begin_test:]

Next, we set up `LagFeatures` and `WindowFeatures` to create features from lags and windows:

.. code:: python

    lagf = LagFeatures(
        variables=["demand"],
        periods=[1, 2, 3, 4, 5, 6],
        missing_values="ignore",
        drop_na=True,
    )

    winf = WindowFeatures(
        variables=["demand"],
        window=["3h"],
        freq="1h",
        functions=["mean"],
        missing_values="ignore",
        drop_original=True,
        drop_na=True,
    )

We wrap the lasso regression within the multioutput regressor to predict multiple targets:

.. code:: python

    lasso = MultiOutputRegressor(Lasso(random_state=0, max_iter=10))

Now, we assemble the steps in the :class:`Pipeline` and fit it to the training data:

.. code:: python

    pipe = Pipeline(
        [
            ("lagf", lagf),
            ("winf", winf),
            ("lasso", lasso),
        ]
    ).set_output(transform="pandas")

    pipe.fit(X_train, y_train)

We can obtain the datasets with the predictors and the targets like this:

.. code:: python

    Xt, yt = pipe[:-1].transform_x_y(X_test, y_test)

    X_test.shape, y_test.shape, Xt.shape, yt.shape

We see that the :class:`Pipeline` has dropped some rows during the transformation and re-adjusted the target.
The rows that were dropped were those necessary to create the first lags.

.. code:: python

    ((1417, 1), (1417, 6), (1410, 7), (1410, 6))

We can examine the predictors training set, to make sure we are passing the right variables
to the regression model:

.. code:: python

    print(Xt.head())

We see the input features:

.. code:: python

                         demand_lag_1  demand_lag_2  demand_lag_3  demand_lag_4  \
    date_time
    2015-01-01 01:00:00   7804.086240   8352.992140   7571.301440   7516.472988
    2015-01-01 02:00:00   7174.339984   7804.086240   8352.992140   7571.301440
    2015-01-01 03:00:00   6654.283364   7174.339984   7804.086240   8352.992140
    2015-01-01 04:00:00   6429.598010   6654.283364   7174.339984   7804.086240
    2015-01-01 05:00:00   6412.785284   6429.598010   6654.283364   7174.339984

                         demand_lag_5  demand_lag_6  demand_window_3h_mean
    date_time
    2015-01-01 01:00:00   7801.201802   7818.461408            7804.086240
    2015-01-01 02:00:00   7516.472988   7801.201802            7489.213112
    2015-01-01 03:00:00   7571.301440   7516.472988            7210.903196
    2015-01-01 04:00:00   8352.992140   7571.301440            6752.740453
    2015-01-01 05:00:00   7804.086240   8352.992140            6498.888886

Now, we can make forecasts for the test set:

.. code:: python

    forecast = pipe.predict(X_test)

    forecasts = pd.DataFrame(
        pipe.predict(X_test),
        index=Xt.loc[end_train:].index,
        columns=[f"step_{i+1}" for i in range(6)]

    )

    print(forecasts.head())

We see the 6 hr ahead energy demand prediction for each hour:

.. code:: python

                             step_1       step_2       step_3       step_4  \
    date_time
    2015-01-01 01:00:00  7810.769000  7890.897914  8123.247406  8374.365708
    2015-01-01 02:00:00  7049.673468  7234.890108  7586.593627  7889.608312
    2015-01-01 03:00:00  6723.246357  7046.660134  7429.115933  7740.984091
    2015-01-01 04:00:00  6639.543752  6962.661308  7343.941881  7616.240318
    2015-01-01 05:00:00  6634.279747  6949.262247  7287.866893  7633.157948

                              step_5       step_6
    date_time
    2015-01-01 01:00:00  8569.220349  8738.027713
    2015-01-01 02:00:00  8116.631154  8270.579148
    2015-01-01 03:00:00  7937.918837  8170.531420
    2015-01-01 04:00:00  7884.815566  8197.598425
    2015-01-01 05:00:00  7979.920512  8321.363714

To learn more about direct forecasting and how to create features, check out our courses:

.. figure::  ../../images/fetsf.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Time Series Forecasting

.. figure::  ../../images/fwml.png
   :width: 300
   :figclass: align-center
   :align: right
   :target: https:

   Forecasting with Machine Learning

|
|
|
|
|
|
|
|
|
|

Hyperparameter optimization
---------------------------

We can optimize the hyperparameters of the transformers and the estimators from a pipeline simultaneously.

We'll start by loading the titanic dataset:

.. code:: python

    from feature_engine.datasets import load_titanic
    from feature_engine.encoding import OneHotEncoder
    from feature_engine.outliers import OutlierTrimmer
    from feature_engine.pipeline import Pipeline

    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import train_test_split, GridSearchCV
    from sklearn.preprocessing import StandardScaler

    X, y = load_titanic(
        return_X_y_frame=True,
        predictors_only=True,
        handle_missing=True,
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=0,
    )

    print(X_train.head())

We see the first 5 rows from the training set below:

.. code:: python

          pclass     sex        age  sibsp  parch     fare    cabin embarked
    501        2  female  13.000000      0      1  19.5000  Missing        S
    588        2  female   4.000000      1      1  23.0000  Missing        S
    402        2  female  30.000000      1      0  13.8583  Missing        C
    1193       3    male  29.881135      0      0   7.7250  Missing        Q
    686        3  female  22.000000      0      0   7.7250  Missing        Q

Now, we set up a Pipeline:

.. code:: python

    pipe = Pipeline(
        [
            ("outliers", OutlierTrimmer(variables=["age", "fare"])),
            ("enc", OneHotEncoder()),
            ("scaler", StandardScaler()),
            ("logit", LogisticRegression(random_state=10)),
        ]
    )

We establish the hyperparameter space to search:

.. code:: python

    param_grid={
        'logit__C': [0.1, 10.],
        'enc__top_categories': [None, 5],
        'outliers__capping_method': ["mad", 'iqr']
    }

We do the grid search:

.. code:: python

    grid = GridSearchCV(
        pipe,
        param_grid=param_grid,
        cv=2,
        refit=False,
    )

    grid.fit(X_train, y_train)

And we can see the best hyperparameters for each step:

.. code:: python

    grid.best_params_

.. code:: python

    {'enc__top_categories': None,
     'logit__C': 0.1,
     'outliers__capping_method': 'iqr'}

And the best accuracy obtained with these hyperparameters:

.. code:: python

    grid.best_score_

.. code:: python

    0.7843822843822843

Additional resources
--------------------

To learn more about feature engineering and data preprocessing, including missing data imputation, outlier removal or
capping, variable transformation and encoding, check out our online course and book:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/preprocessing/index.rst
================================================
.. -*- mode: rst -*-

Preprocessing
=============

Feature-engine's preprocessing transformers apply general data pre-processing
and transformation procedures.

.. toctree::
   :maxdepth: 1

   MatchCategories
   MatchVariables

================================================
FILE: docs/user_guide/preprocessing/MatchCategories.rst
================================================
.. _match_categories:

.. currentmodule:: feature_engine.preprocessing

MatchCategories
===============

:class:`MatchCategories()` ensures that categorical variables are encoded as pandas
'categorical' dtype instead of generic python 'object' or other dtypes.

Under the hood, 'categorical' dtype is a representation that maps each
category to an integer, thus providing a more memory-efficient object
structure than, for example, 'str', and allowing faster grouping, mapping, and similar
operations on the resulting object.

:class:`MatchCategories()` remembers the encodings or levels that represent each
category, and can thus can be used to ensure that the correct encoding gets
applied when passing categorical data to modeling packages that support this
dtype, or to prevent unseen categories from reaching a further transformer
or estimator in a pipeline, for example.

Let's explore this with an example. First we load the Titanic dataset and split it into
a train and a test sets:

.. code:: python

    from feature_engine.preprocessing import MatchCategories
    from feature_engine.datasets import load_titanic

    # Load dataset
    data = load_titanic(
        predictors_only=True,
        handle_missing=True,
        cabin="letter_only",
    )

    data['pclass'] = data['pclass'].astype('O')

    # Split test and train
    train = data.iloc[0:1000, :]
    test = data.iloc[1000:, :]

Now, we set up :class:`MatchCategories()` and fit it to the train set.

.. code:: python

    # set up the transformer
    match_categories = MatchCategories(missing_values="ignore")

    # learn the mapping of categories to integers in the train set
    match_categories.fit(train)

:class:`MatchCategories()` stores the mappings from the train set in its attribute:

.. code:: python

    # the transformer stores the mappings for categorical variables
    match_categories.category_dict_

.. code:: python

    {'pclass': Int64Index([1, 2, 3], dtype='int64'),
     'sex': Index(['female', 'male'], dtype='object'),
     'cabin': Index(['A', 'B', 'C', 'D', 'E', 'F', 'M', 'T'], dtype='object'),
     'embarked': Index(['C', 'Missing', 'Q', 'S'], dtype='object')}

If we transform the test dataframe using the same `match_categories` object,
categorical variables will be converted to a 'category' dtype with the same
numeration (mapping from categories to integers) that was applied to the train
dataset:

.. code:: python

    # encoding that would be gotten from the train set
    train.embarked.unique()

.. code:: python

    array(['S', 'C', 'Missing', 'Q'], dtype=object)

.. code:: python
    
    # encoding that would be gotten from the test set
    test.embarked.unique()

.. code:: python

    array(['Q', 'S', 'C'], dtype=object)

.. code:: python
    
    # with 'match_categories', the encoding remains the same
    match_categories.transform(train).embarked.cat.categories

.. code:: python

    Index(['C', 'Missing', 'Q', 'S'], dtype='object')

.. code:: python

    # this will have the same encoding as the train set
    match_categories.transform(test).embarked.cat.categories

.. code:: python

    Index(['C', 'Missing', 'Q', 'S'], dtype='object')

If some category was not present in the training data, it will not mapped
to any integer and will thus not get encoded. This behavior can be modified through the
parameter `errors`:

.. code:: python

    # categories present in the train data
    train.cabin.unique()

.. code:: python

    array(['B', 'C', 'E', 'D', 'A', 'M', 'T', 'F'], dtype=object)

.. code:: python
    
    # categories present in the test data - 'G' is new
    test.cabin.unique()

.. code:: python

    array(['M', 'F', 'E', 'G'], dtype=object)

.. code:: python

    match_categories.transform(train).cabin.unique()

.. code:: python

    ['B', 'C', 'E', 'D', 'A', 'M', 'T', 'F']
    Categories (8, object): ['A', 'B', 'C', 'D', 'E', 'F', 'M', 'T']

.. code:: python
    
    # unseen category 'G' will not get mapped to any integer
    match_categories.transform(test).cabin.unique()

.. code:: python

    ['M', 'F', 'E', NaN]
    Categories (8, object): ['A', 'B', 'C', 'D', 'E', 'F', 'M', 'T']

When to use the transformer
^^^^^^^^^^^^^^^^^^^^^^^^^^^

This transformer is useful when creating custom transformers for categorical columns,
as well as when passing categorical columns to modeling packages which support them
natively but leave the variable casting to the user, such as ``lightgbm`` or ``glum``.

================================================
FILE: docs/user_guide/preprocessing/MatchVariables.rst
================================================
.. _match_variables:

.. currentmodule:: feature_engine.preprocessing

MatchVariables
==============

:class:`MatchVariables()` ensures that the columns in the test set are identical to those
in the train set.

If the test set contains additional columns, they are dropped. Alternatively, if the
test set lacks columns that were present in the train set, they will be added with a
value determined by the user, for example np.nan. :class:`MatchVariables()` will also
return the variables in the order seen in the train set.

Let's explore this with an example. First we load the Titanic dataset and split it into
a train and a test set:

.. code:: python

    from feature_engine.preprocessing import MatchVariables
    from feature_engine.datasets import load_titanic

    # Load dataset
    data = load_titanic(
        predictors_only=True,
        cabin="letter_only",
    )

    data['pclass'] = data['pclass'].astype('O')

    # Split test and train
    train = data.iloc[0:1000, :]
    test = data.iloc[1000:, :]

Now, we set up :class:`MatchVariables()` and fit it to the train set.

.. code:: python

    # set up the transformer
    match_cols = MatchVariables(missing_values="ignore")

    # learn the variables in the train set
    match_cols.fit(train)

:class:`MatchVariables()` stores the variables from the train set in its attribute:

.. code:: python

    # the transformer stores the input variables
    match_cols.feature_names_in_

.. code:: python

    ['pclass',
     'survived',
     'sex',
     'age',
     'sibsp',
     'parch',
     'fare',
     'cabin',
     'embarked']

Now, we drop some columns in the test set.

.. code:: python

    # Let's drop some columns in the test set for the demo
    test_t = test.drop(["sex", "age"], axis=1)

    test_t.head()

.. code:: python

         pclass  survived  sibsp  parch     fare cabin embarked
    1000      3         1      0      0   7.7500     n        Q
    1001      3         1      2      0  23.2500     n        Q
    1002      3         1      2      0  23.2500     n        Q
    1003      3         1      2      0  23.2500     n        Q
    1004      3         1      0      0   7.7875     n        Q

If we transform the dataframe with the dropped columns using :class:`MatchVariables()`,
we see that the new dataframe contains all the variables, and those that were missing
are now back in the data, with np.nan values as default.

.. code:: python

    # the transformer adds the columns back
    test_tt = match_cols.transform(test_t)

    test_tt.head()

.. code:: python

    The following variables are added to the DataFrame: ['age', 'sex']
         pclass  survived  sex  age  sibsp  parch     fare cabin embarked
    1000      3         1  NaN  NaN      0      0   7.7500     n        Q
    1001      3         1  NaN  NaN      2      0  23.2500     n        Q
    1002      3         1  NaN  NaN      2      0  23.2500     n        Q
    1003      3         1  NaN  NaN      2      0  23.2500     n        Q
    1004      3         1  NaN  NaN      0      0   7.7875     n        Q

Note how the missing columns were added back to the transformed test set, with
missing values, in the position (i.e., order) in which they were in the train set.

Similarly, if the test set contained additional columns, those would be removed. To
test that, let's add some extra columns to the test set:

.. code:: python

    # let's add some columns for the demo
    test_t[['var_a', 'var_b']] = 0

    test_t.head()

.. code:: python

         pclass  survived  sibsp  parch     fare cabin embarked  var_a  var_b
    1000      3         1      0      0   7.7500     n        Q      0      0
    1001      3         1      2      0  23.2500     n        Q      0      0
    1002      3         1      2      0  23.2500     n        Q      0      0
    1003      3         1      2      0  23.2500     n        Q      0      0
    1004      3         1      0      0   7.7875     n        Q      0      0

And now, we transform the data with :class:`MatchVariables()`:

.. code:: python

    test_tt = match_cols.transform(test_t)

    test_tt.head()

.. code:: python

    The following variables are added to the DataFrame: ['age', 'sex']
    The following variables are dropped from the DataFrame: ['var_b', 'var_a']
         pclass  survived  sex  age  sibsp  parch     fare cabin embarked
    1000      3         1  NaN  NaN      0      0   7.7500     n        Q
    1001      3         1  NaN  NaN      2      0  23.2500     n        Q
    1002      3         1  NaN  NaN      2      0  23.2500     n        Q
    1003      3         1  NaN  NaN      2      0  23.2500     n        Q
    1004      3         1  NaN  NaN      0      0   7.7875     n        Q

Now, the transformer simultaneously added the missing columns with NA as values and
removed the additional columns from the resulting dataset.

However, if we look closely, the dtypes for the `sex` variable do not match. This could
cause issues if other transformations depend upon having the correct dtypes.

.. code:: python

    train.sex.dtype

.. code:: python

    dtype('O')

.. code:: python

    test_tt.sex.dtype

.. code:: python

    dtype('float64')

Set the `match_dtypes` parameter to `True` in order to align the dtypes as well.

.. code:: python

    match_cols_and_dtypes = MatchVariables(missing_values="ignore", match_dtypes=True)
    match_cols_and_dtypes.fit(train)

    test_ttt = match_cols_and_dtypes.transform(test_t)

.. code:: python

    The following variables are added to the DataFrame: ['sex', 'age']
    The following variables are dropped from the DataFrame: ['var_b', 'var_a']
    The sex dtype is changing from  float64 to object

Now the dtype matches.

.. code:: python

    test_ttt.sex.dtype

.. code:: python

    dtype('O')

By default, :class:`MatchVariables()` will print out messages indicating which variables
were added, removed and altered. We can switch off the messages through the parameter `verbose`.

When to use the transformer
^^^^^^^^^^^^^^^^^^^^^^^^^^^

These transformer is useful in "predict then optimize type of problems". In such cases,
a machine learning model is trained on a certain dataset, with certain input features.
Then, test sets are "post-processed" according to scenarios that want to be modelled.
For example, "what would have happened if the customer received an email campaign"?
where the variable "receive_campaign" would be turned from 0 -> 1.

While creating these modelling datasets, a lot of meta data e.g., "scenario number",
"time scenario was generated", etc, could be added to the data. Then we need to pass
these data over to the model to obtain the modelled prediction.

:class:`MatchVariables()` provides an easy an elegant way to remove the additional metadeta,
while returning datasets with the input features in the correct order, allowing the
different scenarios to be modelled directly inside a machine learning pipeline.

More details
^^^^^^^^^^^^

You can also find a similar implementation of the example shown in this page in the
following Jupyter notebook:

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/preprocessing/MatchVariables.ipynb>`_

All notebooks can be found in a `dedicated repository <https://github.com/feature-engine/feature-engine-examples>`_.

================================================
FILE: docs/user_guide/scaling/index.rst
================================================
.. -*- mode: rst -*-
.. _scaling_user_guide:

.. currentmodule:: feature_engine.scaling

Scaling
=======

`Feature scaling <https://www.blog.trainindata.com/feature-scaling-in-machine-learning/>`_
is the process of transforming the range of numerical features so that they fit within a
specific scale, usually to improve the performance and training stability of machine learning
models.

Scaling helps to normalize the input data, ensuring that each feature contributes proportionately
to the final result, particularly in algorithms that are sensitive to the range of the data,
such as gradient descent-based models (e.g., linear regression, logistic regression, neural networks)
and distance-based models (e.g., K-nearest neighbors, clustering).

Feature-engine's scalers replace the variables' values by the scaled ones. In this page, we
discuss the importance of scaling numerical features, and then introduce the various
scaling techniques supported by Feature-engine.

Importance of scaling
---------------------

Scaling is crucial in machine learning as it ensures that features contribute equally to model
training, preventing bias toward variables with larger ranges. Properly scaled data enhances the
performance of algorithms sensitive to the magnitude of input values, such as gradient descent
and distance-based methods. Additionally, scaling can improve convergence speed and overall model
accuracy, leading to more reliable predictions.

When apply scaling
------------------

- **Training:** Most machine learning algorithms require data to be scaled before training,
  especially linear models, neural networks, and distance-based models.

- **Feature Engineering:** Scaling can be essential for certain feature engineering techniques,
  like polynomial features.

- **Resampling:** Some oversampling methods like SMOTE and many of the undersampling methods
  clean data based on KNN algorithms, which are distance based models.

When Scaling Is Not Necessary
-----------------------------

Not all algorithms require scaling. For example, tree-based algorithms (like Decision Trees,
Random Forests, Gradient Boosting) are generally invariant to scaling because they split data
based on the order of values, not the magnitude.

Scalers
-------

.. toctree::
   :maxdepth: 1

   MeanNormalizationScaler

================================================
FILE: docs/user_guide/scaling/MeanNormalizationScaler.rst
================================================
.. _mean_normalization_scaler:

.. currentmodule:: feature_engine.scaling

MeanNormalizationScaler
=======================

:class:`MeanNormalizationScaler()` scales variables using mean normalization. With mean normalization,
we center the distribution around 0, and rescale the distribution to the variable's value range,
so that its values vary between -1 and 1. This is accomplished by subtracting the mean of the feature
and then dividing by its range (i.e., the difference between the maximum and minimum values).

The :class:`MeanNormalizationScaler()` only works with non-constant numerical variables.
If the variable is constant, the scaler will raise an error.

Python example
--------------

We'll show how to use :class:`MeanNormalizationScaler()` through a toy dataset. Let's create
a toy dataset:

.. code:: python

    import pandas as pd
    from feature_engine.scaling import MeanNormalizationScaler

    df = pd.DataFrame.from_dict(
        {
            "Name": ["tom", "nick", "krish", "jack"],
            "City": ["London", "Manchester", "Liverpool", "Bristol"],
            "Age": [20, 21, 19, 18],
            "Height": [1.80, 1.77, 1.90, 2.00],
            "Marks": [0.9, 0.8, 0.7, 0.6],
            "dob": pd.date_range("2020-02-24", periods=4, freq="min"),
        })

    print(df)

The dataset looks like this:

.. code:: python

        Name        City  Age  Height  Marks                 dob
    0    tom      London   20    1.80    0.9 2020-02-24 00:00:00
    1   nick  Manchester   21    1.77    0.8 2020-02-24 00:01:00
    2  krish   Liverpool   19    1.90    0.7 2020-02-24 00:02:00
    3   jack     Bristol   18    2.00    0.6 2020-02-24 00:03:00

We see that the only numerical features in this dataset are **Age**, **Marks**, and **Height**. We want
to scale them using mean normalization.

First, let's make a list with the variable names:

.. code:: python

    vars = [
      'Age',
      'Marks',
      'Height',
    ]

Now, let's set up :class:`MeanNormalizationScaler()`:

.. code:: python

    # set up the scaler
    scaler = MeanNormalizationScaler(variables = vars)

    # fit the scaler
    scaler.fit(df)
    
The scaler learns the mean of every column in *vars* and their respective range.
Note that we can access these values in the following way:

.. code:: python

    # access the parameters learned by the scaler
    print(f'Means: {scaler.mean_}')
    print(f'Ranges: {scaler.range_}')

We see the features' mean and value ranges in the following output:

.. code:: python

    Means: {'Age': 19.5, 'Marks': 0.7500000000000001, 'Height': 1.8675000000000002}
    Ranges: {'Age': 3.0, 'Marks': 0.30000000000000004, 'Height': 0.22999999999999998}

We can now go ahead and scale the variables:

.. code:: python

    # scale the data
    df = scaler.transform(df)
    print(df)

In the following output, we can see the scaled variables:

.. code:: python

        Name        City       Age    Height     Marks                 dob
    0    tom      London  0.166667 -0.293478  0.500000 2020-02-24 00:00:00
    1   nick  Manchester  0.500000 -0.423913  0.166667 2020-02-24 00:01:00
    2  krish   Liverpool -0.166667  0.141304 -0.166667 2020-02-24 00:02:00
    3   jack     Bristol -0.500000  0.576087 -0.500000 2020-02-24 00:03:00

We can restore the data to itsoriginal values using the inverse transformation:

.. code:: python

    # inverse transform the dataframe
    df = scaler.inverse_transform(df)
    print(df)

In the following data, we see the scaled variables returned to their oridinal representation:

.. code:: python

        Name        City  Age  Height  Marks                 dob
    0    tom      London   20    1.80    0.9 2020-02-24 00:00:00
    1   nick  Manchester   21    1.77    0.8 2020-02-24 00:01:00
    2  krish   Liverpool   19    1.90    0.7 2020-02-24 00:02:00
    3   jack     Bristol   18    2.00    0.6 2020-02-24 00:03:00

Additional resources
--------------------

For more details about this and other feature engineering methods check out
these resources:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/selection/DropConstantFeatures.rst
================================================
.. _drop_constant:

.. currentmodule:: feature_engine.selection

DropConstantFeatures
====================

Constant features are variables that show zero variability, or, in other words, have the
same value in all rows. A key step towards training a machine learning model is to identify
and remove constant features.

Features with no or low variability rarely constitute useful predictors. Hence, removing
them right at the beginning of the data science project is a good way of simplifying your
dataset and subsequent data preprocessing pipelines.

Filter methods are selection algorithms that select or remove features based solely on
their characteristics. In this light, removing constant features could be considered part
of the filter group of selection algorithms.

In Python, we can find constant features by using pandas `std` or `unique` methods, and then
remove them with `drop`.

With Scikit-learn, we can find and remove constant variables with `VarianceThreshold` to quickly
reduce the number of features. `VarianceThreshold` is part of `sklearn.feature_selection`'s API.

`VarianceThreshold`, however, would only work with numerical variables. Hence, we could only
evaluate categorical variables after encoding them, which requires a prior step of data
preprocessing just to remove redundant variables.

Feature-engine introduces :class:`DropConstantFeatures()` to find and remove constant and
quasi-constant features from a dataframe. :class:`DropConstantFeatures()` works with numerical,
categorical, or datetime variables. It is therefore more versatile than Scikit-learn’s transformer
because it allows us to drop all duplicate variables without the need for prior data transformations.

By default, :class:`DropConstantFeatures()` drops constant variables. We also have the option
to drop quasi-constant features, which are those that show mostly constant values and some other
values in a very small percentage of rows.

Because :class:`DropConstantFeatures()` works with numerical and categorical variables alike,
it offers a straightforward way of reducing the feature subset.

Be mindful, though, that depending on the context, quasi-constant variables could be useful.

**Example**

Let’s see how to use :class:`DropConstantFeatures()` by using the Titanic dataset. This dataset
does not contain constant or quasi-constant variables, so for the sake of the demonstration,
we will consider quasi-constant those features that show the same value in more than 70% of
the rows.

We first load the data and separate it into a training set and a test set:

.. code:: python

    from sklearn.model_selection import train_test_split
    from feature_engine.datasets import load_titanic
    from feature_engine.selection import DropConstantFeatures

    X, y = load_titanic(
        return_X_y_frame=True,
        handle_missing=True,
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=0,
    )

Now, we set up the :class:`DropConstantFeatures()` to remove features that show the same
value in more than 70% of the observations. We do this through the parameter `tol`. The
default value for this parameter is zero, in which case it will remove constant features.

.. code:: python

    # set up the transformer
    transformer = DropConstantFeatures(tol=0.7)

With `fit()` the transformer finds the variables to drop:

.. code:: python

    # fit the transformer
    transformer.fit(X_train)

The variables to drop are stored in the attribute `features_to_drop_`:

.. code:: python

    transformer.features_to_drop_

.. code:: python

    ['parch', 'cabin', 'embarked', 'body']

We can check that the variables `parch` and `embarked` show the same value in more than 70% of
the observations as follows:

.. code:: python

    X_train['embarked'].value_counts(normalize = True)

.. code:: python

    S          0.711790
    C          0.195415
    Q          0.090611
    Missing    0.002183
    Name: embarked, dtype: float64

Based on the previous results, 71% of the passengers embarked in S.

Let's now evaluate `parch`:

.. code:: python

    X_train['parch'].value_counts(normalize = True)

.. code:: python

    0    0.771834
    1    0.125546
    2    0.086245
    3    0.005459
    4    0.004367
    5    0.003275
    6    0.002183
    9    0.001092
    Name: parch, dtype: float64

Based on the previous results, 77% of the passengers had 0 parent or child. Because of this,
these features were deemed quasi-constant and will be removed in the next step.

We can also identify quasi-constant variables as follows:

.. code:: python

    import pandas

    X_train["embarked"].value_counts(normalize=True).plot.bar()

After executing the previous code, we observe the following plot, with more than 70% of
passengers embarking in S:

.. figure::  ../../images/quasiconstant.png
   :align:   center

With `transform()`, we drop the quasi-constant variables from the dataset:

.. code:: python

    train_t = transformer.transform(X_train)
    test_t = transformer.transform(X_test)

    print(train_t.head())

We see the resulting dataframe below:

.. code:: python

          pclass                               name     sex        age  sibsp  \
    501        2  Mellinger, Miss. Madeleine Violet  female  13.000000      0
    588        2                  Wells, Miss. Joan  female   4.000000      1
    402        2     Duran y More, Miss. Florentina  female  30.000000      1
    1193       3                 Scanlan, Mr. James    male  29.881135      0
    686        3       Bradley, Miss. Bridget Delia  female  22.000000      0

                 ticket     fare     boat  \
    501          250644  19.5000       14
    588           29103  23.0000       14
    402   SC/PARIS 2148  13.8583       12
    1193          36209   7.7250  Missing
    686          334914   7.7250       13

                                                  home.dest
    501                            England / Bennington, VT
    588                                Cornwall / Akron, OH
    402                     Barcelona, Spain / Havana, Cuba
    1193                                            Missing
    686   Kingwilliamstown, Co Cork, Ireland Glens Falls...

Like sklearn, Feature-engine transformers have the `fit_transform` method that allows us
to find and remove constant or quasi-constant variables in a single line of code for convenience.

Like sklearn as well, `DropConstantFeatures()` has the `get_support()` method, which returns
a vector with values `True` for features that will be retained and `False` for those that
will be dropped.

.. code:: python

    transformer.get_support()

.. code:: python

    [True, True, True, True, True, False, True, True, False, False,
     True, False, True]

This and other feature selection methods may not necessarily avoid overfitting, but they
contribute to simplifying our machine learning pipelines and creating more interpretable
machine learning models.

Additional resources
--------------------

In this Kaggle kernel we use :class:`DropConstantFeatures()` together with other feature
selection algorithms and then train a Logistic regression estimator:

- `Kaggle kernel <https://www.kaggle.com/solegalli/feature-selection-with-feature-engine>`_

For more details about this and other feature selection methods check out these resources:

.. figure::  ../../images/fsml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/fsmlbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection in Machine Learning

|
|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/selection/DropCorrelatedFeatures.rst
================================================
.. _drop_correlated:

.. currentmodule:: feature_engine.selection

DropCorrelatedFeatures
======================

The :class:`DropCorrelatedFeatures()` finds and removes correlated variables from a dataframe.
Correlation is calculated with `pandas.corr()`. All correlation methods supported by `pandas.corr()`
can be used in the selection, including Spearman, Kendall, or Spearman. You can also pass a
bespoke correlation function, provided it returns a value between -1 and 1.

Features are removed on first found first removed basis, without any further insight. That is,
the first feature will be retained an all subsequent features that are correlated with this, will
be removed.

The transformer will examine all numerical variables automatically. Note that you could pass a
dataframe with categorical and datetime variables, and these will be ignored automatically.
Alternatively, you can pass a list with the variables you wish to evaluate.

**Example**

Let's create a toy dataframe where 4 of the features are correlated:

.. code:: python

    import pandas as pd
    from sklearn.datasets import make_classification
    from feature_engine.selection import DropCorrelatedFeatures

    # make dataframe with some correlated variables
    def make_data():
        X, y = make_classification(n_samples=1000,
                               n_features=12,
                               n_redundant=4,
                               n_clusters_per_class=1,
                               weights=[0.50],
                               class_sep=2,
                               random_state=1)

        # trasform arrays into pandas df and series
        colnames = ['var_'+str(i) for i in range(12)]
        X = pd.DataFrame(X, columns =colnames)
        return X

    X = make_data()

Now, we set up :class:`DropCorrelatedFeatures()` to find and remove variables which
(absolute) correlation coefficient is bigger than 0.8:

.. code:: python

    tr = DropCorrelatedFeatures(variables=None, method='pearson', threshold=0.8)

With `fit()` the transformer finds the correlated variables and with `transform()` it drops
them from the dataset:

.. code:: python

    Xt = tr.fit_transform(X)

The correlated feature groups are stored in the transformer's attributes:

.. code:: python

    tr.correlated_feature_sets_

.. code:: python

    [{'var_0', 'var_8'}, {'var_4', 'var_6', 'var_7', 'var_9'}]

We can identify from each group which feature will be retained and which ones removed
by inspecting the dictionary:

.. code:: python

    tr.correlated_feature_dict_

In the dictionary below we see that from the first correlated group, `var_0` is a key,
hence it will be retained, whereas `var_8` is a value, which means that it is correlated
to `var_0` and will therefore be removed.

.. code:: python

   {'var_0': {'var_8'}, 'var_4': {'var_6', 'var_7', 'var_9'}}

Similarly, `var_4` is a key and will be retained, whereas the variables 6, 7 and 8 were
found correlated to `var_4` and will therefore be removed.

The features that will be removed from the dataset are stored in a different attribute
as well:

..  code:: python

    tr.features_to_drop_

.. code:: python

    ['var_8', 'var_6', 'var_7', 'var_9']

If we now go ahead and print the transformed data, we see that the correlated features
have been removed.

.. code:: python

    print(print(Xt.head()))

.. code:: python

              var_0     var_1     var_2     var_3     var_4     var_5    var_10  \
    0  1.471061 -2.376400 -0.247208  1.210290 -3.247521  0.091527  2.070526
    1  1.819196  1.969326 -0.126894  0.034598 -2.910112 -0.186802  1.184820
    2  1.625024  1.499174  0.334123 -2.233844 -3.399345 -0.313881 -0.066448
    3  1.939212  0.075341  1.627132  0.943132 -4.783124 -0.468041  0.713558
    4  1.579307  0.372213  0.338141  0.951526 -3.199285  0.729005  0.398790

         var_11
    0 -1.989335
    1 -1.309524
    2 -0.852703
    3  0.484649
    4 -0.186530

Additional resources
--------------------

In this notebook, we show how to use :class:`DropCorrelatedFeatures()` with a different
relation metric:

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/selection/Drop-Correlated-Features.ipynb>`_

All notebooks can be found in a `dedicated repository <https://github.com/feature-engine/feature-engine-examples>`_.

For more details about this and other feature selection methods check out these resources:

.. figure::  ../../images/fsml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/fsmlbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection in Machine Learning

|
|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/selection/DropDuplicateFeatures.rst
================================================
.. _drop_duplicate:

.. currentmodule:: feature_engine.selection

DropDuplicateFeatures
=====================

Duplicate features are columns in a dataset that are identical, or, in other words, that
contain exactly the same values. Duplicate features can be introduced accidentally, either
through poor data management processes or during data manipulation.

For example, duplicated new records can be created by one-hot encoding a categorical
variable or by adding missing data indicators. We can also accidentally generate duplicate
records when we merge different data sources that show some variable overlap.

Checking for and removing duplicate features is a standard procedure in any data analysis
workflow that helps us reduce the dimension of the dataset quickly and ensure data quality.
In Python, we can find duplicate values in an attribute table very easily with Pandas.
Dropping those duplicate features, however, requires a few more lines of code.

Feature-engine aims to accelerate the process of data validation by finding and removing
duplicate features with the :class:`DropDuplicateFeatures()` class, which is part of the
selection API.

:class:`DropDuplicateFeatures()` does exactly that; it finds and removes duplicated variables
from a dataframe. DropDuplicateFeatures() will automatically evaluate all variables, or
alternatively, you can pass a list with the variables you wish to have examined. And it
works with numerical and categorical features alike.

So let’s see how to set up :class:`DropDuplicateFeatures()`.

**Example**

In this demo, we will use the Titanic dataset and introduce a few duplicated features
manually:

.. code:: python

    import pandas as pd
    from sklearn.model_selection import train_test_split
    from feature_engine.datasets import load_titanic
    from feature_engine.selection import DropDuplicateFeatures

    data = load_titanic(
        handle_missing=True,
        predictors_only=True,
    )

    # Lets duplicate some columns
    data = pd.concat([data, data[['sex', 'age', 'sibsp']]], axis=1)
    data.columns = ['pclass', 'survived', 'sex', 'age',
                    'sibsp', 'parch', 'fare','cabin', 'embarked',
                    'sex_dup', 'age_dup', 'sibsp_dup']

We then split the data into a training and a testing set:

.. code:: python

    # Separate into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(
        data.drop(['survived'], axis=1),
        data['survived'],
        test_size=0.3,
        random_state=0,
    )

    print(X_train.head())

Below we see the resulting data:

.. code:: python

          pclass     sex        age  sibsp  parch     fare    cabin embarked  \
    501        2  female  13.000000      0      1  19.5000  Missing        S
    588        2  female   4.000000      1      1  23.0000  Missing        S
    402        2  female  30.000000      1      0  13.8583  Missing        C
    1193       3    male  29.881135      0      0   7.7250  Missing        Q
    686        3  female  22.000000      0      0   7.7250  Missing        Q

         sex_dup    age_dup  sibsp_dup
    501   female  13.000000          0
    588   female   4.000000          1
    402   female  30.000000          1
    1193    male  29.881135          0
    686   female  22.000000          0

As expected, the variables `sex` and `sex_dup` have duplicate field values throughout all
the rows. The same is true for the variables `age` and `age_dup`.

Now, we set up :class:`DropDuplicateFeatures()` to find the duplicate features:

.. code:: python

    transformer = DropDuplicateFeatures()

With `fit()` the transformer finds the duplicated features:

.. code:: python

    transformer.fit(X_train)

The features that are duplicated and will be removed are stored in the `features_to_drop_`
attribute:

..  code:: python

    transformer.features_to_drop_

.. code:: python

    {'age_dup', 'sex_dup', 'sibsp_dup'}

With `transform()` we remove the duplicated variables:

.. code:: python

    train_t = transformer.transform(X_train)
    test_t = transformer.transform(X_test)

We can go ahead and check the variables in the transformed dataset, and we will see that
the duplicated features are not there any more:

.. code:: python

    train_t.columns

.. code:: python

    Index(['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'cabin', 'embarked'], dtype='object')

The transformer also stores the groups of duplicated features, which is useful for data
analysis and validation.

.. code:: python

    transformer.duplicated_feature_sets_

.. code:: python

    [{'sex', 'sex_dup'}, {'age', 'age_dup'}, {'sibsp', 'sibsp_dup'}]

Additional resources
--------------------

In this Kaggle kernel we use :class:`DropDuplicateFeatures()` in a pipeline with other
feature selection algorithms:

- `Kaggle kernel <https://www.kaggle.com/solegalli/feature-selection-with-feature-engine>`_

For more details about this and other feature selection methods check out these resources:

.. figure::  ../../images/fsml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/fsmlbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection in Machine Learning

|
|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/selection/DropFeatures.rst
================================================
.. _drop_features:

.. currentmodule:: feature_engine.selection

DropFeatures
=============

The :class:`DropFeatures()` drops a list of variables indicated by the user from the original
dataframe. The user can pass a single variable as a string or list of variables to be
dropped.

:class:`DropFeatures()` offers similar functionality to `pandas.dataframe.drop <https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html>`_,
but the difference is that :class:`DropFeatures()` can be integrated into a Scikit-learn
pipeline.

**When is this transformer useful?**

Sometimes, we create new variables combining other variables in the dataset, for
example, we obtain the variable `age` by subtracting `date_of_application` from
`date_of_birth`. After we obtained our new variable, we do not need the date
variables in the dataset any more. Thus, we can add :class:`DropFeatures()` in the Pipeline
to have these removed.

**Example**

Let's see how to use :class:`DropFeatures()` in an example with the Titanic dataset. We
first load the data and separate it into train and test:

.. code:: python

    from sklearn.model_selection import train_test_split
    from feature_engine.datasets import load_titanic
    from feature_engine.selection import DropFeatures

    X, y = load_titanic(
        return_X_y_frame=True,
        handle_missing=True,
    )

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=0,
    )

    print(X_train.head())

Now, we go ahead and print the dataset column names:

.. code:: python

    X_train.columns

.. code:: python

    Index(['pclass', 'name', 'sex', 'age', 'sibsp', 'parch', 'ticket', 'fare',
           'cabin', 'embarked', 'boat', 'body', 'home.dest'],
          dtype='object')

Now, with :class:`DropFeatures()` we can very easily drop a group of variables. Below
we set up the transformer to drop a list of 6 variables:

.. code:: python

    # set up the transformer
    transformer = DropFeatures(
        features_to_drop=['sibsp', 'parch', 'ticket', 'fare', 'body', 'home.dest']
    )

    # fit the transformer
    transformer.fit(X_train)

With `fit()` this transformer does not learn any parameter. We can go ahead and remove
the variables as follows:

.. code:: python

    train_t = transformer.transform(X_train)
    test_t = transformer.transform(X_test)

And now, if we print the variable names of the transformed dataset, we see that it has
been reduced:

.. code:: python

    train_t.columns

.. code:: python

    Index(['pclass', 'name', 'sex', 'age', 'cabin', 'embarked', 'boat'], dtype='object')

Additional resources
--------------------

In this Kaggle kernel we feature 3 different end-to-end machine learning pipelines using
:class:`DropFeatures()`:

- `Kaggle Kernel <https://www.kaggle.com/solegalli/feature-engineering-and-model-stacking>`_

All notebooks can be found in a `dedicated repository <https://github.com/feature-engine/feature-engine-examples>`_.

For more details about this and other feature selection methods check out these resources:

.. figure::  ../../images/fsml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/fsmlbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection in Machine Learning

|
|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/selection/DropHighPSIFeatures.rst
================================================
.. _psi_selection:

.. currentmodule:: feature_engine.selection

DropHighPSIFeatures
===================

The :class:`DropHighPSIFeatures()` finds and removes features with changes in their
distribution, i.e. "unstable values", from a pandas dataframe.
The stability of the distribution is computed using the **Population Stability
Index (PSI)** and all features having a PSI value above a given threshold are removed.

Unstable features may introduce an additional bias in a model if the training population
significantly differs from the population in production. Removing features for
which a shift in the distribution is suspected leads to
more robust models and therefore to better performance. In the field of Credit Risk
modelling, eliminating features with high PSI is common practice and usually required by the
Regulator.

Population Stability Index - PSI
--------------------------------

The PSI is a measure of how much a population has changed in time or how different the distributions
are between two different population samples.

To determine the PSI, continuous features are sorted into discrete intervals, the
fraction of observations per interval is then determined, and finally those values
are compared between the 2 groups, or as we call them in Feature-engine, between the
basis and test sets, to obtain the PSI.

In other words, the PSI is computed as follows:

- Define the intervals into which the observations will be sorted.
- Sort the feature values into those intervals.
- Determine the fraction of observations within each interval.
- Compute the PSI.

The PSI is determined as:

.. math::

    PSI = \sum_{i=1}^n (test_i - basis_i) . ln(\frac{test_i}{basis_i})

where `basis` and `test` are the "reference" and "evaluation" datasets, respectively, and `i`
refers to the interval.

In other words, the PSI determines the difference in the proportion of observations in each
interval, between the reference (aka, original) and test datasets.

In the PSI equation, `n` is the total number of intervals.

Important
~~~~~~~~~

When working with the PSI it is worth highlighting the following:

- The number of bins used to define the distributions has an impact on the PSI values.
- The PSI is a suitable metric for numerical features (i.e., either continuous or with high cardinality).
- For categorical or discrete features, the change in distributions is better assessed with Chi-squared.

he PSI is symmetric. That means that switching the order of the basis and test dataframes
in the PSI calculation will lead to identical values. However, in this implementation, the interval
limits used to calculate the PSI are inferred from the basis dataframe. Hence, switching the
order of the dataframes will lead to different interval limits, which in turn may result in
different PSI values.

Threshold
~~~~~~~~~

Different thresholds can be used to assess the magnitude of the distribution shift according
to the PSI value. The most commonly used thresholds are:

- Below 10%, the variable has not experienced a significant shift.
- Above 25%, the variable has experienced a major shift.
- Between those two values, the shift is intermediate.
- 'auto': the threshold will be calculated based on the size of the base and target datasets and the number of bins.

When 'auto', the threshold is calculated using the chi2 approximation, proposed by B. Yurdakul:

.. math::

    threshold = \chi^2_{(q,B-1)} . (\frac{1}{N} + \frac{1}{M})

where q is the percentile, B is the number of bins, N is the size of basis dataset, N is the size of test dataset.

In our implementation, we are using the 99.9th percentile.

As mentioned above, the number of bins has an impact on PSI value, because with a higher number of bins it is easier to find divergence in data 
and vice versa. The same could be said about dataset size - the more data we have, the harder it is to find the difference (if the shift is not drastic).
This formula tries to catch these relationships and adjust threshold to correctly detect feature drift.

Procedure
---------

To compute the PSI, the :class:`DropHighPSIFeatures()` splits the input dataset in
two: a basis data set (aka the reference data) and a test set. The basis data set is assumed to contain
the expected or original feature distributions. The test set will be assessed
against the basis data set.

In the next step, the interval boundaries are determined based on the features in the basis
or reference data. These intervals can be determined to be of equal with, or equal number
of observations.

Next, :class:`DropHighPSIFeatures()` sorts each of the variable values into those intervals, both in the
basis and test datasets, and then determines the proportion (percentage) of observations
within each interval.

Finally, the PSI is determined as indicated in the previous paragraph for each feature.
With the PSI value per feature, :class:`DropHighPSIFeatures()` can now select the features that are unstable and
drop them, based on a threshold.

Splitting the data
------------------

:class:`DropHighPSIFeatures()` allows us to determine how much a feature distribution has
changed in time, or how much it differs between 2 groups.

If we want to evaluate the distribution change in time, we can use a datetime variable as splitting
reference and provide a datetime cut-off as split point.

If we want to compare the distribution change between 2 groups, :class:`DropHighPSIFeatures()`
offers 3 different approaches to split the input dataframe:

- Based on proportion of observations.
- Based on proportions of unique observations.
- Using a cut-off value.

Proportion of observations
~~~~~~~~~~~~~~~~~~~~~~~~~~

Splitting by proportion of observations will result in a certain proportion of observations
allocated to either the reference and test datasets. For example, if we set `split_frac=0.75`,
then 75% and 25% of the observations will be put into the reference and test data, respectively.

If we select this method, we can pass a variable in the parameter `split_col` or leave it to None.

Note that the data split is not done at random, but instead guided by the values in the reference
variable indicated in `split_col`. Under the hood, the reference variable indicated in `split_col`
is ordered, and the percentage of observations is determined with NumPy quantile. This means
that the observations with smaller values in `split_col` will land in the reference dataset,
and those with bigger values will go to the test set.

If the rows in your dataset are sorted in time, this could be a good default option to split the
dataframe in 2 and compute the PSI. This will for example be the case if your data set contains
daily (or any other frequency) sales information on a company's products.

Proportions of unique observations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If we split based on proportion of unique observations, it is important that we indicate which
column we want to use as reference in the `split_col` parameter, to make a meaningful split. If
we leave this to None, :class:`DropHighPSIFeatures()` will use the dataframe index as reference.
This makes sense only if the index in the dataframe has meaningful values.

:class:`DropHighPSIFeatures()` will first identify the unique values of the variable in
`split_col`. Then it will put a certain proportion of those values into the reference
dataset and the remaining to the test dataset. The proportion is indicated in the parameter
`split_frac`.

Under the hood, :class:`DropHighPSIFeatures()` will sort the unique values of the reference
variable, and then use NumPy quantiles to determine the fraction that should be allocated to the
reference and test sets. Thus, it is important to consider that the order of the unique values
matters in the split.

This split makes sense when we have for example unique customer identifiers and multiple rows
per customer in the dataset. We want to make sure that all rows belonging to the same customer
are allocated either in the reference or test data, but the same customer cannot be in both
data sets. This way of splitting the data will also ensure that we have a certain percentage,
indicated in `split_frac` of customers in either data set after the split.

Thus, if `split_frac=0.6` and `split_distinct=True`, :class:`DropHighPSIFeatures()` will send
the first 60% of customers to the reference data set, and the remaining 40% to the test set. And it will
ensure that rows belonging to the same customer are just in one of the 2 data sets.

Using a cut-off value
~~~~~~~~~~~~~~~~~~~~~

We have the option to pass a reference variable to use to split the dataframe using `split_col` and
also a cut-off value in the `cut_off` parameter. The cut-off value can be a number, integer or float,
a date or a list of values.

If we pass a datetime column in `split_col` and a datetime value in the `cut_off`, we can split the
data in a temporal manner. Observations collected before the time indicated will be sent to the reference
dataframe, and the remaining to the test set.

If we pass a list of values in the `cut_off` all observations which values are included in the
list will go into the reference data set, and the remaining to the test set. This split is useful
if we have a categorical variable indicating a portfolio from which the observations have been collected.
For example, if we set `split_col='portfolio'` and `cut_off=['port_1', 'port_2']`, all observations
that belong to the first and second portfolio will be sent to the reference data set, and the observations
from other portfolios to the test set.

Finally, if we pass a number to `cut_off`, all observations which value in the variable indicated
in `split_col` is <= cut-off, will be sent to the reference data set, alternatively to the test set.
This can be useful for example when dates are defined as integer (for example 20200411) or when
using an ordinal customer segmentation to split the dataframe (1: retail customers, 2: private
banking customers, 3: SME and 4: Wholesale).

split_col
~~~~~~~~~

To split the data set, we recommend that you indicate which column you want to use as
reference in the `split_col` parameter. If you don't, the split will be done based on the
values of the dataframe index. This might be a good option if the index contains meaningful
values or if splitting just based on `split_frac`.

Examples
--------

The versatility of the class lies in the different options to split the input dataframe
in a reference or basis data set with the "expected" distributions, and a test set which
will be evaluated against the reference.

After splitting the data, :class:`DropHighPSIFeatures()` goes ahead and compares the
feature distributions in both data sets by computing the PSI.

To illustrate how to best use :class:`DropHighPSIFeatures()` depending on your data, we
provide various examples illustrating the different possibilities.

Case 1: split data based on proportions (split_frac)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In this case, :class:`DropHighPSIFeatures()` will split the dataset in 2, based on the
indicated proportion. The proportion is indicated in the `split_frac` parameter. You have
the option to select a variable in `split_col` or leave it to None. In the latter, the
dataframe index will be used to split.

Let's first create a toy dataframe containing 5 random variables and 1 variable
with a shift in its distribution (*var_3* in this case).

.. code:: python

    import pandas as pd
    import seaborn as sns

    from sklearn.datasets import make_classification
    from feature_engine.selection import DropHighPSIFeatures

    # Create a dataframe with 500 observations and 6 random variables
    X, y = make_classification(
        n_samples=500,
        n_features=6,
        random_state=0
    )

    colnames = ["var_" + str(i) for i in range(6)]
    X = pd.DataFrame(X, columns=colnames)

    # Add a column with a shift.
    X['var_3'][250:] = X['var_3'][250:] + 1

The default approach in :class:`DropHighPSIFeatures()` is to split the
input dataframe `X` in two equally sized data sets. You can adjust the proportions by changing
the value in the `split_frac` parameter.

For example, let's split the input dataframe into a reference data set containing 60% of
the observations and a test set containing 40% of the observations.

.. code:: python

    # Remove the features with high PSI values using a 60-40 split.

    transformer = DropHighPSIFeatures(split_frac=0.6)
    transformer.fit(X)

The value of `split_frac` tells :class:`DropHighPSIFeatures()` to split X according to a
60% - 40% ratio. The `fit()` method performs the split of the dataframe and the calculation
of the PSI.

Because we created random variables, these features will have low PSI values (i.e., no
distribution change). However, we manually added a distribution shift in the variable *var_3*
and therefore expect the PSI for this particular feature to be above the
0.25 PSI threshold.

The PSI values are accessible through the `psi_values_` attribute:

.. code:: python

    transformer.psi_values_

The analysis of the PSI values below shows that only feature 3 (called `var_3`)
has a PSI above the 0.25 threshold (default value) and will be removed
by the `transform` method.

.. code:: python

    {'var_0': 0.07405459925568803,
    'var_1': 0.09124093185820083,
    'var_2': 0.16985790067687764,
    'var_3': 1.342485289730313,
    'var_4': 0.0743442762545251,
    'var_5': 0.06809060587241555}

From the output, we see that the PSI value for *var_0* is around 7%. This means
that, when comparing the first 300 and the last 200 observations of the dataframe,
there is only a small difference in the distribution of the *var_0* feature.
A similar conclusion applies to *var_1, var_2, var_4* and *var_5*.
Looking at the PSI value for *var_3*, we see that it exceeds by far the 0.25
threshold. We can then conclude the population of this feature has shifted and
it is wise not to include it in the feature set for modelling.

The cut-off value used to split the dataframe is stored in the `cut_off_` attribute:

.. code:: python

    transformer.cut_off_

This yields the following answer

.. code:: python

    299.4

The value of 299.4 means that observations with index from 0 to 299 are used
to define the basis data set. This corresponds to 60% (300 / 500) of the original dataframe
(X).
The value of 299.4 may seem strange because it is not one of the value present in (the
(index of) the dataframe. Intuitively, we would expect the cut_off to be an integer
in the present case. However, the cut_off is computed using quantiles and the quantiles
are computed using extrapolation.

Splitting with proportions will order the index or the reference column first, and then
determine the data that will go into each dataframe. In other words, the order of the index
or the variable indicated in `split_col` matters. Observations with the lowest values will
be sent to the basis dataframe and the ones with the highest values to the test set.

The `features_to_drop_` attribute provides the list with the features to
be dropped when executing the `transform` method.

The command

.. code:: python

    transformer.features_to_drop_

Yields the following result:

.. code:: python

    ['var_3']

That the *var_3* feature is dropped during the procedure is illustrated when
looking at the columns from the `X_transformed` dataframe.

.. code:: python

    X_transformed = transformer.transform(X)

    X_transformed.columns

    Index(['var_0', 'var_1', 'var_2', 'var_4', 'var_5'], dtype='object')

:class:`DropHighPSIFeatures()` also contains a `fit_transform` method that combines
the `fit` and the `transform` methods.

The difference in distribution between a non-shifted and
a shifted distribution is clearly visible when plotting the cumulative density
function.

For the shifted variable:

.. code:: python

    X['above_cut_off'] = X.index > transformer.cut_off_
    sns.ecdfplot(data=X, x='var_3', hue='above_cut_off')

and a non-shifted variable (for example *var_1*)

.. code:: python

    sns.ecdfplot(data=X, x='var_1', hue='above_cut_off')

.. image:: ../../images/PSI_distribution_case1.png

Case 2: split data based on variable (numerical cut_off)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the previous example, we wanted to split the input dataframe in 2 datasets, with the
reference dataset containing 60% of the observations. We let :class:`DropHighPSIFeatures()`
find the cut-off to achieve this.

We can instead, provide ourselves the numerical cut-off that determines which observations will
go to the reference or basis data set, and which to the test set. Using the `cut_off` parameter,
we can define the specific threshold for the split.

A real life example for this case is the use of the customer ID or contract ID
to split the dataframe. These IDs are often increasing in value over time which justifies
their use to assess distribution shifts in the features.

Let's create a toy dataframe representing the customers' characteristics of a
company. This dataset contains six random variables (in
real life this are variables like age or postal code), the seniority of the customer
(i.e. the number of months since the start of the relationship between the
customer and the company) and the customer ID (i.e. the number (integer) used
to identify the customer). Generally the customer ID grows
over time which means that early customers have a lower customer ID than late
customers.

From the definition of the variables, we expect the *seniority* to increase with
the customer ID and therefore to have a high PSI value when comparing early and
late customer,

.. code:: python

    import pandas as pd
    from sklearn.datasets import make_classification
    from feature_engine.selection import DropHighPSIFeatures

    X, y = make_classification(
            n_samples=500,
            n_features=6,
            random_state=0
        )

    colnames = ["var_" + str(i) for i in range(6)]
    X = pd.DataFrame(X, columns=colnames)

    # Let's add a variable for the customer ID
    X['customer_id'] = [customer_id for customer_id in range(1, 501)]

    # Add a column with the seniority... that is related to the customer ID
    X['seniority'] = 100 - X['customer_id']

    transformer = DropHighPSIFeatures(split_col='customer_id', cut_off=250)
    transformer.fit(X)

In this case, :class:`DropHighPSIFeatures()` will allocate in the basis or reference data
set, all observations which values in `customer_id` are <= 250. The test dataframe contains the
remaining observations.

The method `fit()` will determine the PSI values, which are stored in the class:

.. code:: python

    transformer.psi_values_

We see that :class:`DropHighPSIFeatures()` does not provide any PSI value for
the `customer_id` feature, because this variable was used as a reference to split the data.

.. code:: python

    {'var_0': 0.07385590683974477,
    'var_1': 0.061155637727757485,
    'var_2': 0.1736694458621651,
    'var_3': 0.044965387331530465,
    'var_4': 0.0904519893659045,
    'var_5': 0.027545195437270797,
    'seniority': 7.8688986006052035}

.. code:: python

    transformer.features_to_drop_

Gives

.. code:: python

    ['seniority']

Executing the dataframe transformation leads to the exclusion of the *seniority*
feature but not to the exclusion of the *customer_id*.

.. code:: python

    X_transformed = transformer.transform(X)

    X_transformed.columns

    Index(['var_0', 'var_1', 'var_2', 'var_3', 'var_4', 'var_5', 'customer_id'], dtype='object')

Case 3: split data based on time (date as cut_off)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:class:`DropHighPSIFeatures()` can handle different types of `split_col`
variables. The following case illustrates how it works with a date variable. In fact,
we often want to determine if the distribution of a feature changes in time, for example
after a certain event like the start of the Covid-19 pandemic.

This is how to do it. Let's create a toy dataframe with 6 random numerical variables
and two date variables. One will be use to specific the split of the dataframe
while the second one is expected to have a high PSI value.

.. code:: python

    import pandas as pd
    from datetime import date
    from sklearn.datasets import make_classification
    from feature_engine.selection import DropHighPSIFeatures

    X, y = make_classification(
            n_samples=1000,
            n_features=6,
            random_state=0
        )

    colnames = ["var_" + str(i) for i in range(6)]
    X = pd.DataFrame(X, columns=colnames)

    # Add two time variables to the dataframe
    X['time'] = [date(year, 1, 1) for year in range(1000, 2000)]
    X['century'] = X['time'].apply(lambda x: ((x.year - 1)

    # Let's shuffle the dataframe and reset the index to remove the correlation
    # between the index and the time variables.

    X = X.sample(frac=1).reset_index(drop=True)

Dropping features with high PSI values comparing two periods of time is done simply
by providing the name of the column with the date and a cut-off date.
In the example below the PSI calculations
will be done comparing the periods up to the French revolution and after.

.. code:: python

    transformer = DropHighPSIFeatures(split_col='time', cut_off=date(1789, 7, 14))
    transformer.fit(X)

**Important**: if the date variable is in pandas or NumPy datetime format, you may need
to pass the cut_off value as `pd.to_datetime(1789-07-14)`.

The PSI values shows the *century* variables in unstable as its value is above
the 0.25 threshold.

.. code:: python

    transformer.psi_values_

    {'var_0': 0.0181623637463045,
    'var_1': 0.10595496570984747,
    'var_2': 0.05425659114295842,
    'var_3': 0.09720689210928271,
    'var_4': 0.07917647542638032,
    'var_5': 0.10122468631060424,
    'century': 8.272395772368412}

The class has correctly identified the feature to be dropped.

.. code:: python

    transformer.features_to_drop_

    ['century']

And the transform method correctly removes the feature.

.. code:: python

    X_transformed = transformer.transform(X)

    X_transformed.columns

    Index(['var_0', 'var_1', 'var_2', 'var_3', 'var_4', 'var_5', 'time'], dtype='object')

The difference in distribution between a non-shifted and
a shifted distribution is clearly visible when plotting the cumulative density
function for each of the group.

We can plot the cumulative distribution of the shifted variable like this:

.. code:: python

    X['above_cut_off'] = X.time > pd.to_datetime(transformer.cut_off_)
    sns.ecdfplot(data=X, x='century', hue='above_cut_off')

and the distribution of a non-shifted variable, for example *var_2*, like this:

.. code:: python

    sns.ecdfplot(data=X, x='var_2', hue='above_cut_off')

And below we can compare both plots:

.. image:: ../../images/PSI_distribution_case3.png

Case 4: split data based on a categorical variable (category or list as cut_off)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:class:`DropHighPSIFeatures()` can also split the original dataframe based on
a categorical variable. The cut-off can then be defined in two ways:

- Using a single string.
- Using a list of values.

In the first case, the column with the categorical variable is sorted alphabetically and
the split is determined by the cut-off. We recommend being very careful when using a single
category as cut-off, because alphabetical sorting in combination with a cut-off does
not always provide obvious results. In other words, for this way of splitting the data to
be meaningful, the alphabetical order of the categories in the reference variable should have
an intrinsic meaning.

A better purpose for splitting the data based on a categorical variable would be to pass a
list with the values of the variable that want in the reference dataframe. A real life
example for this case is the computation of the PSI between different customer segments
like 'Retail', 'SME' or 'Wholesale'. In this case, if we indicate ['Retail'] as
cut-off, observations for Retail will be sent to the basis data set, and those for 'SME'
and 'Wholesale' will be added to the test set.

Split passing a category value
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Let's show how to set up the transformer in this case. The example data set
contains 6 randoms variables, a categorical variable with the labels of the
different categories and 2 category related features.

.. code:: python

    import pandas as pd
    import seaborn as sns

    from sklearn.datasets import make_classification
    from feature_engine.selection import DropHighPSIFeatures

    X, y = make_classification(
        n_samples=1000,
        n_features=6,
        random_state=0
    )

    colnames = ["var_" + str(i) for i in range(6)]
    X = pd.DataFrame(X, columns=colnames)

    # Add a categorical column
    X['group'] = ["A", "B", "C", "D", "E"] * 200

    # And two category related features
    X['group_means'] = X.group.map({"A": 1, "B": 2, "C": 0, "D": 1.5, "E": 2.5})
    X['shifted_feature'] = X['group_means'] + X['var_2']

We can define a simple cut-off value (for example the letter C). In this case, observations
with values that come before C, alphabetically, will be allocated to the reference data set.

.. code:: python

    transformer = DropHighPSIFeatures(split_col='group', cut_off='C')
    X_transformed = transformer.fit_transform(X)

The PSI values are provided in the `psi_values_` attribute.

.. code:: python

    transformer.psi_values_

    {'var_0': 0.06485778974895254,
    'var_1': 0.03605540598761757,
    'var_2': 0.040632784917352296,
    'var_3': 0.023845405645510645,
    'var_4': 0.028007185972248064,
    'var_5': 0.07009152672971862,
    'group_means': 6.601444547497699,
    'shifted_feature': 0.48428009522119164}

From these values we see that the last 2 features should be removed. We can corroborate
that in the `features_to_drop_` attribute:

.. code:: python

    transformer.features_to_drop_

    ['group_means', 'shifted_feature']

And these columns are removed from the original dataframe by the transform
method that, in the present case, has been applied through the fit_transform
method a couple of block cells above.

.. code:: python

    X_transformed.columns

    Index(['var_0', 'var_1', 'var_2', 'var_3', 'var_4', 'var_5', 'group'], dtype='object')

Split passing a list of categories
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Instead of passing a category value, we can instead pass a list of values to the `cut_off`.
Using the same data set let's set up the :class:`DropHighPSIFeatures()` to split
the dataframe according to the list ['A', 'C', 'E'] for the categorical variable *group*.

In this case, the PSI's will be computed by comparing two dataframes: the first one
containing only the values A, C and E for the *group* variable and the second one containing
only the values B and D.

.. code:: python

    trans = DropHighPSIFeatures(split_col='group', cut_off=['A', 'C', 'E'])
    X_no_drift = trans.fit_transform(X)

.. code:: python

    trans.psi_values_

    'var_0': 0.04322345673014104,
    'var_1': 0.03534439253617049,
    'var_2': 0.05220272785661243,
    'var_3': 0.04550964862452317,
    'var_4': 0.04492720670343145,
    'var_5': 0.044886435640028144,
    'group_means': 6.601444547497699,
    'shifted_features': 0.3683642099948127}

Here again, the object will remove the *group_means* and the *shifted_features* columns
from the dataframe.

.. code:: python

    trans.features_to_drop_

    ['group_means', 'shifted_features']

And these columns are removed from the original dataframe by the transform
method that has been applied through the `fit_transform` method.

.. code:: python

    X_transformed.columns

    Index(['var_0', 'var_1', 'var_2', 'var_3', 'var_4', 'var_5', 'group'], dtype='object')

In the following plots, we can compare the distribution of a feature with high PSI and
one with low PSI, in the different categories of the categorical variable.

With this code we plot the cumulative distribution of a feature which distribution is
different among the different categories of the variable:

.. code:: python

    sns.ecdfplot(data=X, x='shifted_feature', hue='group')

With this code we plot the cumulative distribution of a feature which distribution is
the same across the different categories of the categorical variable:

.. code:: python

    sns.ecdfplot(data=X, x='var_0', hue='group')

And below we can compare the plots of both features:

.. image:: ../../images/PSI_distribution_case4.png

Case 5: split data based on unique values (split_distinct)
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

A variant to the previous example is the use of the `split_distinct` functionality.
In this case, the split is not done based on the number observations from
`split_col` but from the number of distinct values in the reference variable indicated
in `split_col`.

A real life example for this case is when dealing with groups of different sizes
like customers income classes ('1000', '2000', '3000', '4000', ...).
Split_distinct allows to control the numbers of classes in the basis and test
dataframes regardless of the number of observations in each class.

This case is illustrated in the toy data for this case. The data set contains
6 random variable and 1 income variable that is larger for one of the 6 group
defined (the F group).

.. code:: python

    import numpy as np
    import pandas as pd
    import seaborn as sns

    from sklearn.datasets import make_classification
    from feature_engine.selection import DropHighPSIFeatures

    X, y = make_classification(
        n_samples=1000,
        n_features=6,
        random_state=0
    )

    colnames = ["var_" + str(i) for i in range(6)]
    X = pd.DataFrame(X, columns=colnames)

    # Add a categorical column
    X['group'] = ["A", "B", "C", "D", "E"] * 100 + ["F"] * 500

    # And an income variable that is category dependent.
    np.random.seed(0)
    X['income'] = np.random.uniform(1000, 2000, 500).tolist() +
                  np.random.uniform(1250, 2250, 500).tolist()

    # Shuffle the dataframe to make the dataset more real life case.
    X = X.sample(frac=1).reset_index(drop=True)

The `group` column contains 500 observations in the (A, B, C, D, E)
group and 500 in the (F) group.

When we pass `split_distinct=True` when initializing
the `DropHighPSIFeatures` object, the two dataframes used to compute the
PSI will contain the same number of **unique** values in the `group`
column (i.e., one dataframe will contain 300 rows associated to groups A, B and C
while the other will contain 700 rows associated to groups D, E and F).

.. code:: python

    transformer = DropHighPSIFeatures(split_col='group', split_distinct=True)
    transformer.fit(X)

    transformer.psi_values_

This yields the following PSI values:

.. code:: python

    {'var_0': 0.014825303242393804,
    'var_1': 0.03818316821350485,
    'var_2': 0.029635981271458896,
    'var_3': 0.021700399485890084,
    'var_4': 0.061194837255216114,
    'var_5': 0.04119583769297253,
    'income': 0.46191580731264914}

And we can find the feature that will be dropped, income, here:

.. code:: python

    transformer.features_to_drop_

        ['income']

The former feature will be removed from the dataset when calling the `transform()` method.

.. code:: python

    X_transformed = transformer.transform(X)

    X_transformed.columns

    Index(['var_0', 'var_1', 'var_2', 'var_3', 'var_4', 'var_5', 'group'], dtype='object')

The difference in distribution between a non-shifted and
a shifted distribution is clearly visible when plotting the cumulative density
function for each of the group.

For the shifted variable (income):

.. code:: python

    sns.ecdfplot(data=X, x='income', hue='group')

and a non-shifted variable (for example *var_4*)

.. code:: python

    sns.ecdfplot(data=X, x="var_4", hue="group")

.. image:: ../../images/PSI_distribution_case5.png

Additional resources
--------------------

In this notebook, we show how to use :class:`DropHighPSIFeatures` on a real dataset and
give more detail about the underlying base and reference sub-dataframes used to
determine the PSI.

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/selection/Drop-High-PSI-Features.ipynb>`_

All notebooks can be found in a `dedicated repository <https://github.com/feature-engine/feature-engine-examples>`_.

For more details about this and other feature selection methods check out these resources:

.. figure::  ../../images/fsml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/fsmlbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection in Machine Learning

|
|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/selection/index.rst
================================================
.. -*- mode: rst -*-
.. _selection_user_guide:

.. currentmodule:: feature_engine.selection

Feature Selection
=================

Feature-engine's feature selection transformers identify features with low predictive
performance and drop them from the dataset. Most of the feature selection algorithms
supported by Feature-engine are not yet available in other libraries. These algorithms
have been gathered from data science competitions or used in the industry.

Selection Mechanism Overview
----------------------------

Feature-engine’s transformers select features based on different strategies.

The first strategy evaluates the features intrinsic characteristics, like their distributions.
For example, we can remove constant or quasi-constant features. Or we can remove features
whose distribution in unstable in time by using the Population Stability Index.

A second strategy consists in determining the relationships between features. Among these,
we can remove features that are duplicated or correlated.

We can also select features based on their relationship with the target. To assess this,
we can replace the feature values by the target mean, or calculate the information value.

Some feature selection procedures involve training machine learning models. We can assess
features individually, or collectively, through various algorithms, as shown in the
following diagram:

.. figure::  ../../images/selectionChart.png
   :align:   center

   Selection mechanisms - Overview

Algorithms that select features based on their performance within a group of variables, will
normally train a model with all the features, and then remove or add or shuffle a feature and
re-evaluate the model performance.

These methods are normally geared towards improving the overall performance of the final
machine learning model as well as reducing the feature space.

Selectors Characteristics Overview
----------------------------------

Some Feature-engine's selectors work with categorical variables off-the-shelf and/or allow
missing data in the variables. These gives you the opportunity to quickly screen features
before jumping into any feature engineering.

In the following tables, we highlight the main Feature-engine selectors characteristics:

Selection based on feature characteristics
------------------------------------------

============================================ ======================= ============= ====================================================================================
    Transformer                                Categorical variables   Allows NA	    Description
============================================ ======================= ============= ====================================================================================
:class:`DropFeatures()`                         √	                      √	            Drops arbitrary features determined by user
:class:`DropConstantFeatures()`  	            √	                      √	            Drops constant and quasi-constant features
:class:`DropDuplicateFeatures()`                √	                      √             Drops features that are duplicated
:class:`DropCorrelatedFeatures()`               ×	                      √	            Drops features that are correlated
:class:`SmartCorrelatedSelection()`	            ×	                      √	            From a correlated feature group drops the less useful features
:class:`MRMR()`                  	            √	                      ×	            Selects features based on the MRMR framework
============================================ ======================= ============= ====================================================================================

Methods that determine duplication or the number of unique values, can work with both
numerical and categorical variables and support missing data as well.

Selection procedures based on correlation work only with numerical variables but allow
missing data.

Selection based on a machine learning model
-------------------------------------------

============================================ ======================= ============= ====================================================================================
    Transformer                                Categorical variables   Allows NA	    Description
============================================ ======================= ============= ====================================================================================
:class:`SelectBySingleFeaturePerformance()`	    ×	                      ×	            Selects features based on single feature model performance
:class:`RecursiveFeatureElimination()`          ×                         ×             Removes features recursively by evaluating model performance
:class:`RecursiveFeatureAddition()`             ×                         ×             Adds features recursively by evaluating model performance
============================================ ======================= ============= ====================================================================================

Selection procedures that require training a machine learning model from Scikit-learn
require numerical variables without missing data.

Selection methods commonly used in finance
------------------------------------------

============================================ ======================= ============= ====================================================================================
    Transformer                                Categorical variables   Allows NA	    Description
============================================ ======================= ============= ====================================================================================
:class:`DropHighPSIFeatures()`	                ×	                      √	            Drops features with high Population Stability Index
:class:`SelectByInformationValue()`	            √                         x             Drops features with low information value
============================================ ======================= ============= ====================================================================================

:class:`DropHighPSIFeatures()` allows to remove features with changes in their distribution. This is done by
splitting the input dataframe in two parts and comparing the distribution of each feature in the two
parts. The metric used to assess distribution shift is the Population Stability Index (PSI). Removing
unstable features may lead to more robust models. In fields like Credit Risk Modelling, the Regulator
often requires the PSI of the final feature set to be below are given threshold.

Alternative feature selection methods
-------------------------------------

============================================ ======================= ============= ====================================================================================
    Transformer                                Categorical variables   Allows NA	    Description
============================================ ======================= ============= ====================================================================================
:class:`SelectByShuffling()`	                ×	                      ×	            Selects features if shuffling their values causes a drop in model performance
:class:`SelectByTargetMeanPerformance()`        √                         ×             Using the target mean as performance proxy, selects high performing features
:class:`ProbeFeatureSelection()`                ×                         ×             Selects features whose importance is greater than those of random variables
============================================ ======================= ============= ====================================================================================

The :class:`SelectByTargetMeanPerformance()` uses the target mean value as proxy for prediction,
replacing categories or variable intervals by these values and then determining a performance metric.
Thus, it is suitable for both categorical and numerical variables. In its current implementation,
it does not support missing data.

The :class:`ProbeFeatureSelection()` introduces random variables to the dataset, then creates a model and derives
the feature importance. It selects all variables whose importance is grater than the mean importance of the random
features.

Throughout the rest of user guide, you will find more details about each of the feature selection procedures.

Feature Selection Algorithms
----------------------------

Click below to find more details on how to use each one of the transformers.

.. toctree::
   :maxdepth: 1

   DropFeatures
   DropConstantFeatures
   DropDuplicateFeatures
   DropCorrelatedFeatures
   SmartCorrelatedSelection
   SelectBySingleFeaturePerformance
   RecursiveFeatureElimination
   RecursiveFeatureAddition
   SelectByShuffling
   SelectByTargetMeanPerformance
   DropHighPSIFeatures
   SelectByInformationValue
   ProbeFeatureSelection
   MRMR

Additional Resources
--------------------

More details about feature selection can be found in the following resources:

.. figure::  ../../images/fsml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/fsmlbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection in Machine Learning

|
|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/selection/MRMR.rst
================================================
.. _mrmr:

.. currentmodule:: feature_engine.selection

MRMR - Minimum Redundancy Maximum Relevance
===========================================

:class:`MRMR()` selects features based on the Maximum Relevance Minimum Redundancy framework. In
this framework, features with a strong relationship with the target (high relevance), but weak
relationship with other predictor features (low redundancy) are favored and hence selected.

The MRMR algorithm obtains a measure of relevance and a measure of redundancy, and then it assigns
an importance score to each feature based on the difference or ratio between relevance and
redundancy. After that, it selects the features with the highest scores.

MRMR was first described in bioinformatics as a method to select features for microarray gene expression data,
and then expanded and popularized by Uber in the context of marketing models.

MRMR - Mechanism
----------------

The MRMR feature selection algorithm works as follows:

1. Determine the relevance of all predictor variables and select the feature with highest relevance.
2. Determine the redundancy between the remaining features and the selected in step 1.
3. Calculate the importance score as the ratio or difference between relevance and redundancy and select the feature with the maximum value: add it to the selected features.
4. Determine the mean redundancy between the remaining features and the features selected so far.
5. Calculate the importance score and select the feature with the maximum value: add it to the selected features.
6. Repeat 4 and 5 until the desired number of selected features is reached.

MRMR is an iterative algorithm. At each iteration, it determines the mean redundancy between the
remaining features and the features that were selected in previous rounds. With the redundancy,
it obtains a new measure of importance, and then it selects the feature with highest importance.

Note that you need to define the number of features to select.

Relevance
----------

:class:`MRMR()` has 3 strategies to determine feature relevance. To determine the relationship
of each feature with the target variable, :class:`MRMR()` obtains:

1. The F-statistic, which is derived from ANOVA if the target is discrete or correlation if the target is categorical.
2. The mutual information.
3. The importance derived from random forests.

The relevance is used to select the first feature and then to calculate the MRMR values at
each of the subsequent iterations.

F-statistic
~~~~~~~~~~~

The F-statistic determines the degree of linear association between the features and the target.
If the target is categorical, the F-statistic is calculated using Scikit-learn's `f_classif`
function. If the target is continuous, the F-statistic is determined using `f_regression`.

Note that in both cases, these statistic is useful when the features are continuous. For discrete
features, other tests should be used, like chi-square, which at the moment is not implemented.
So if your datasets contain both numerical and categorical variables, try the mutual information
or the importance derived from random forests instead.

Mutual information
~~~~~~~~~~~~~~~~~~

The mutual information is a measure that quantifies how much we know about one variable, by
examining the values of a second variable. In other words, it measures the non-linear association
between features. Higher values indicate stronger associations.

:class:`MRMR()` uses scikit-learn's `mutual_info_classif` to determine the association between
features and a discrete target, or `mutual_info_regression`, to calculate the association between
features and a continuous target.

The mutual information is calculated differently for continuous and discrete variables, so it is
important to flag categorical and discrete through the `discrete_features` parameter.

Random Forests
~~~~~~~~~~~~~~

Random forests can automatically assign a measure of relevance, by computing the degree of impurity
reduction returned by each feature at each node of the tree, and then across the forest. Hence, the importance derived
from random forests offers a good approximation of the relationship between features and target.

Note however that if the features are highly correlated, the importance derived from random forests
will be diluted.

Redundancy
----------

Redundant features are those that are highly correlated, or show high dependency with other features in
the dataset.

:class:`MRMR()` has 2 strategies to determine the relationship of the variables to other variables in
the dataset: Pearson's correlation coefficient or mutual information.

Correlation
~~~~~~~~~~~

To determine each features's redundancy, :class:`MRMR()` obtains Pearson's correlation
coefficient between each feature and the features selected in previous rounds. Next, it
takes the average of the absolute value of the coefficients.

Note that correlation assumes that all features are continuous, so this metric may returned biased
results for categorical and discrete variables.

Mutual information
~~~~~~~~~~~~~~~~~~

To determine each features's redundancy, :class:`MRMR()` caclulates the mutual information
between each feature and the features selected in former iterations, and then takes the average.

MRMR
----

The MRMR method obtains a measure of feature importance by comparing its relevance to the target
and its redundancy with other, previously selected features.

High feature importance is obtained when the relevance is high and the redundancy is low.

A value of MRMR can be obtained through the difference or the ratio between relevance and
redundancy.

The following schemes are supported by :class:`MRMR()`:

.. csv-table::
    :header: Method, Relevance, Redundance, Scheme

    'MID', Mutual information, Mutual information, Difference,
    'MIQ', Mutual information, Mutual information, Ratio,
    'FCD', F-Statistic, Correlation, Difference,
    'FCQ', F-Statistic, Correlation, Ratio,
    'RFCQ', Random Forests, Correlation, Ratio,

Feature selection
-----------------

:class:`MRMR()` selects as many features as indicated in the parameter `'max_features'`.
If left to `None`, :class:`MRMR()` will select 20% of the features seen in the training
dataset used during fit.

Note that the number of features to select is arbitrary.

MRMR is fast to compute if using the F-statistic and correlation. However, these statistical
values are suitable to establish linear relationships.

To detect non-linear relationships, using the variant that determines relevance through
random forests derived feature importance might be better. Mutual information inherently
detects linear and non-linear associations, but for continuous features it takes longer to
compute, impacting the speed of selection of MRMR.

Python examples
---------------

Let's see how to implement :class:`MRMR()`. We'll start by using Scikit-learn's breast cancer
dataset. The target variable is binary, representing malignant or benign tumors. All
predictor variables are continuous.

Let's import the required libraries and classes:

.. code:: python

    import pandas as pd
    import matplotlib.pyplot as plt
    from sklearn.datasets import load_breast_cancer
    from sklearn.model_selection import train_test_split
    from feature_engine.selection import MRMR

Let's now load the cancer diagnostic data and display its top rows:

.. code:: python

    X, y = load_breast_cancer(return_X_y=True, as_frame=True)
    y = y.map({0:1, 1:0})
    print(X.head())

In the following output, we see the top 5 rows of the dataset:

.. code:: python

       mean radius  mean texture  mean perimeter  mean area  mean smoothness  \
    0        17.99         10.38          122.80     1001.0          0.11840
    1        20.57         17.77          132.90     1326.0          0.08474
    2        19.69         21.25          130.00     1203.0          0.10960
    3        11.42         20.38           77.58      386.1          0.14250
    4        20.29         14.34          135.10     1297.0          0.10030

       mean compactness  mean concavity  mean concave points  mean symmetry  \
    0           0.27760          0.3001              0.14710         0.2419
    1           0.07864          0.0869              0.07017         0.1812
    2           0.15990          0.1974              0.12790         0.2069
    3           0.28390          0.2414              0.10520         0.2597
    4           0.13280          0.1980              0.10430         0.1809

       mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \
    0                 0.07871  ...         25.38          17.33           184.60
    1                 0.05667  ...         24.99          23.41           158.80
    2                 0.05999  ...         23.57          25.53           152.50
    3                 0.09744  ...         14.91          26.50            98.87
    4                 0.05883  ...         22.54          16.67           152.20

       worst area  worst smoothness  worst compactness  worst concavity  \
    0      2019.0            0.1622             0.6656           0.7119
    1      1956.0            0.1238             0.1866           0.2416
    2      1709.0            0.1444             0.4245           0.4504
    3       567.7            0.2098             0.8663           0.6869
    4      1575.0            0.1374             0.2050           0.4000

       worst concave points  worst symmetry  worst fractal dimension
    0                0.2654          0.4601                  0.11890
    1                0.1860          0.2750                  0.08902
    2                0.2430          0.3613                  0.08758
    3                0.2575          0.6638                  0.17300
    4                0.1625          0.2364                  0.07678

    [5 rows x 30 columns]

Let's now split the data into train and test sets:

.. code:: python

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=0)

The F-Statistic framework: linear associations
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In this example, we want :class:`MRMR()` to determine feature relevance by using the F-statistic
obtained with ANOVA, and the redundancy by examining the Pearson's correlation coefficient
among features. The importance is obtained as the ratio between relevance and redundancy.

Let's set up :class:`MRMR()`:

.. code:: python

    sel = MRMR(method="FCQ", regression=False)
    sel.fit(X, y)

With `fit()`, :class:`MRMR()` computed the relevance, redundancy, MRMR or feature importance
and determined which features should be selected. Note that this is an iterative process.

We can print out the relevance as follows:

.. code:: python

    sel.relevance_

In the following output, we see an array with the F-statistic obtained from ANOVA:

.. code:: python

    array([6.46981021e+02, 1.18096059e+02, 6.97235272e+02, 5.73060747e+02,
           8.36511234e+01, 3.13233079e+02, 5.33793126e+02, 8.61676020e+02,
           6.95274435e+01, 9.34592949e-02, 2.68840327e+02, 3.90947023e-02,
           2.53897392e+02, 2.43651586e+02, 2.55796780e+00, 5.32473391e+01,
           3.90144816e+01, 1.13262760e+02, 2.41174067e-02, 3.46827476e+00,
           8.60781707e+02, 1.49596905e+02, 8.97944219e+02, 6.61600206e+02,
           1.22472880e+02, 3.04341063e+02, 4.36691939e+02, 9.64385393e+02,
           1.18860232e+02, 6.64439606e+01])

We can instead create a bar plot with the relevance:

.. code:: python

    pd.Series(sel.relevance_, index=sel.variables_).sort_values(
        ascending=False).plot.bar(figsize=(15, 4))
    plt.title("Relevance")
    plt.show()

In the following image, we see the F-statistic per feature:

.. figure::  ../../images/f_statistic.png
   :align:   center

We can see the subset of features that will be removed as follows:

.. code:: python

    sel.features_to_drop_

In the following output we see the features that were not selected:

.. code:: python

    ['mean radius',
     'mean texture',
     'mean area',
     'mean smoothness',
     'mean compactness',
     'mean concavity',
     'mean symmetry',
     'mean fractal dimension',
     'radius error',
     'texture error',
     'perimeter error',
     'area error',
     'smoothness error',
     'compactness error',
     'concavity error',
     'concave points error',
     'symmetry error',
     'fractal dimension error',
     'worst texture',
     'worst smoothness',
     'worst compactness',
     'worst concavity',
     'worst symmetry',
     'worst fractal dimension']

Finally, we can go ahead and retain the selected features like this:

.. code:: python

    Xtr = sel.transform(X_test)
    print(Xtr.head())

In the following output we see the test set with a reduced number of features:

.. code:: python

         mean perimeter  mean concave points  worst radius  worst perimeter  \
    512           88.64              0.08172         16.41           113.30
    457           84.10              0.02068         14.35            91.29
    439           89.59              0.02652         14.91            96.53
    298           91.22              0.01374         16.22           105.80
    37            82.61              0.02923         13.30            84.46

         worst area  worst concave points
    512       844.4               0.20510
    457       632.9               0.06005
    439       688.9               0.08216
    298       819.7               0.07530
    37        545.9               0.05013

In the final dataset we only have the "relevant features". And by relevant, we mean those
with high association with the target, and low association with other features.

Since we left the parameter `'max_features'` as `None, :class:`MRMR()` selected 20% of the
features in the training set. The training set contained 30 features, so 6 features remain
after applying MRMR.

Using random forests
~~~~~~~~~~~~~~~~~~~~

When we have categorical or discrete variables, or want to examine non-linear associations, we
can determine the relevance using the random forests derived feature importance.

:class:`MRMR()` will train a random forest using grid search over a hyperparameter grid that
can be specified by the user. The redundancy is determined using Pearson's
correlation coefficient.

In a similar way, the MRMR feature selection algorithm will compute the feature importance as the
ratio between the random forest importance and Pearson's correlation coefficient.

Lets, set up :class:`MRMR()` to use a random forests classifier for the relevance. Note that we
need to specify a cross-validation scheme, a performance metric, and we have the option to pass
a grid with hyperparameters to optimize:

.. code:: python

    sel = MRMR(
        method="RFCQ",
        scoring="roc_auc",
        param_grid = {"n_estimators": [5, 50, 500], "max_depth":[1,2,3]},
        cv=3,
        regression=False,
        random_state=42,
    )

    sel.fit(X, y)

We can now go ahead and plot the relevance:

.. code:: python

    pd.Series(sel.relevance_, index=sel.variables_).sort_values(
        ascending=False).plot.bar(figsize=(15, 4))
    plt.title("Relevance")
    plt.show()

In the following image we see the relationship between features and the target derived from
random forests:

.. figure::  ../../images/rfimportancemrmr.png
   :align:   center

We can now go ahead and select relevant features using the `transform()` method.

.. code:: python

    Xtr = sel.transform(X_test)
    print(Xtr.head())

In the following output we see the test set with a reduced number of features:

.. code:: python

         mean concave points  fractal dimension error  worst radius  \
    512              0.08172                 0.004005         16.41
    457              0.02068                 0.001828         14.35
    439              0.02652                 0.002104         14.91
    298              0.01374                 0.001957         16.22
    37               0.02923                 0.001777         13.30

         worst perimeter  worst area  worst concave points
    512           113.30       844.4               0.20510
    457            91.29       632.9               0.06005
    439            96.53       688.9               0.08216
    298           105.80       819.7               0.07530
    37             84.46       545.9               0.05013

In the final dataset we only have the relevant features. Again, we selected 20% of the
features in the training set.

Mutual information
~~~~~~~~~~~~~~~~~~

If we have non-linear associations and / or categorical or discrete variables, a better
option is to obtain the relevance and redundancy utilizing mutual information.

The mutual information is calculated differently for numerical and categorical variables,
so it is best to flag discrete features with a boolean array.

For this demo, we'll use the California housing dataset to predict house prices, and we'll
treat 3 variables as discrete. Let's load the data:

.. code:: python

    from sklearn.datasets import fetch_california_housing

    X, y = fetch_california_housing(return_X_y=True, as_frame=True)
    X[['AveRooms', 'AveBedrms', 'AveOccup']] = X[['AveRooms', 'AveBedrms', 'AveOccup']].astype(int)

    print(X.head())

In the following output, we see the first 5 rows of the dataset:

.. code:: python

       MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \
    0  8.3252      41.0         6          1       322.0         2     37.88
    1  8.3014      21.0         6          0      2401.0         2     37.86
    2  7.2574      52.0         8          1       496.0         2     37.85
    3  5.6431      52.0         5          1       558.0         2     37.85
    4  3.8462      52.0         6          1       565.0         2     37.85

       Longitude
    0    -122.23
    1    -122.22
    2    -122.24
    3    -122.25
    4    -122.25

Now, we'll set up :class:`MRMR()` to use mutual information to determine both redundancy and relevance,
and the importance score as the ratio (or quotient, hence the Q in MIQ) between the two.

Note the boolean vector with `True` in the position of the categorical variables  'AveRooms', 'AveBedrms' and 'AveOccup':

.. code:: python

    sel = MRMR(
        variables = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup'],
        method="MIQ",
        max_features=4,
        discrete_features=[False, False, True, True, False, True],
        regression=True,
        random_state=42,
    )

    sel.fit(X,y)

To select features we use `transform()`:

.. code:: python

    Xtr = sel.transform(X)
    print(Xtr.head())

Note that the variables latitude and longitude, which were not part of the set of features
examined by MRMR are retained in the transformed dataset:

.. code:: python

       MedInc  HouseAge  AveBedrms  AveOccup  Latitude  Longitude
    0  8.3252      41.0          1         2     37.88    -122.23
    1  8.3014      21.0          0         2     37.86    -122.22
    2  7.2574      52.0          1         2     37.85    -122.24
    3  5.6431      52.0          1         2     37.85    -122.25
    4  3.8462      52.0          1         2     37.85    -122.25

For compatibility with Scikit-learn, :class:`MRMR()` also supports the
method `get_support()`:

.. code:: python

    sel.get_support()

which returns the following output:

.. code:: python

    [True, True, False, True, False, True, True, True]

Considerations
--------------

The maximum relevance minimum redundancy feature selection method is fast, and therefore allows
scrutinizing fairly big datasets. Computing the F-statistic is fast. That's one of the reasons
that made it gain popularity.

Additional resources
--------------------

For more details about this and other feature selection methods check out these resources:

.. figure::  ../../images/fsml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/fsmlbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection in Machine Learning

|
|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/selection/ProbeFeatureSelection.rst
================================================
.. _probe_features:

.. py:currentmodule:: feature_engine.selection

ProbeFeatureSelection
=====================

:class:`ProbeFeatureSelection()` adds one or more random variables to the dataframe. Next
it derives the feature importance for each variable, including the probe features. Finally,
it removes those features whose importance is lower than the probes.

Deriving feature importance
---------------------------

:class:`ProbeFeatureSelection()` has 2 strategies to derive feature importance.

In the `collective` strategy, :class:`ProbeFeatureSelection()` trains one machine learning
model using all the variables plus the probe features, and then derives the feature importance
from the fitted model. This feature importance is given by the coefficients of
linear models or the feature importance derived from tree-based algorithms.

In the `individual feature` strategy, :class:`ProbeFeatureSelection()` trains one machine
learning model per feature and per probe, and then, the feature importance is given by the
performance of that single feature model. Here, the importance is given by any performance
metric chosen by you.

Both strategies have advantages and limitations. If the features are correlated, the
feature importance value returned by the coefficients of a linear model, or derived from
a decision tree, will appear to be smaller than if the feature was used to train a model
individually. Hence, potentially important features might be lost to the probes due to these
seemingly low importance values resulting from correlation.

On the other hand, training models using individual features, does not allow to detect
feature interactions and does not remove redundant variables.

In addition, keep in mind that the importance derived tree-based models is biased towards
features with high cardinality. Hence, continuous features will seem to be more important
than discrete variables. If your features are discrete and your probes continuous,
you could be removing important features accidentally.

Selecting features
------------------

After assigning a value of feature importance to each feature, including the probes,
:class:`ProbeFeatureSelection()` will select those variables whose importance is greater
than:

- the mean importance of all probes
- the maximum importance of all probes
- the mean plus 3 times the standard deviation of the importance of the probes

The threshold for feature selection can be controlled through the parameter `threshold`
when setting up the transformer.

Feature selection process
-------------------------

This is how :class:`ProbeFeatureSelection()` selects features using the `collective`
strategy:

1. Add 1 or more random features to the dataset
2. Train a machine learning model using all features including the random ones
3. Derive feature importance from the fitted model
4. Take the average (or maximum or mean+std) importance of the random features
5. Select features whose importance is greater than the importance of the random variables (step 4)

This is how :class:`ProbeFeatureSelection()` selects features using the `individual feature`
strategy:

1. Add 1 or more random features to the dataset
2. Train a machine learning per feature and per probe
3. Determine the feature importance as the performance of the single feature model
4. Take the average (or maximum or mean+std) importance of the random features
5. Select features whose importance is greater than the importance of the random variables (step 4)

Rationale of probe feature selection
------------------------------------

One of the primary goals of feature selection is to remove noise from the dataset. A
randomly generated variable, i.e., probe feature, inherently possesses a high level of
noise. Consequently, any variable with less importance than a probe feature is assumed
to be noise and can be discarded from the dataset.

Distribution of the probe features
----------------------------------

When initiating the :class:`ProbeFeatureSelection()` class, you have the option to select
which distribution is to be assumed to create the probe feature(s), as well as the number of
probe features to create.

The possible distributions are 'normal', 'binary', 'uniform', 'discrete_uniform',
'poisson', or 'all'. 'all' creates `n_probe` features per each of the aforementioned
distributions. So, if you selected 'all' and are creating 2 probe features, you will have
2 probes for each distribution.

The distribution matters. Tree-based models tend to give more importance to highly cardinal
features. Hence, probes created from a uniform or normal distribution will display a greater
importance than probes extracted from a binomial, poisson or discrete uniform distributions
when using these models.

Python examples
---------------

Let's see how to use this transformer to select variables from UC Irvine's Breast Cancer
Wisconsin (Diagnostic) dataset, which can be found `here`_. We will use Scikit-learn to load
the dataset. This dataset concerns breast cancer diagnoses. The target variable is binary, i.e.,
malignant or benign. The data is solely comprised of numerical data.

.. _here: https:

Let's import the required libraries and classes:

.. code:: python

    import matplotlib.pyplot as plt
    import pandas as pd
    from sklearn.datasets import load_breast_cancer
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split
    from feature_engine.selection import ProbeFeatureSelection

Let's now load the cancer diagnostic data:

.. code:: python

    cancer_X, cancer_y = load_breast_cancer(return_X_y=True, as_frame=True)

Let's check the shape of `cancer_X`:

.. code:: python

    print(cancer_X.shape)

We see that the dataset is comprised of 569 observations and 30 features:

.. code:: python

    (569, 30)

Let's now split the data into train and test sets:

.. code:: python

    # separate train and test sets
    X_train, X_test, y_train, y_test = train_test_split(
        cancer_X,
        cancer_y,
        test_size=0.2,
        random_state=3
    )

    X_train.shape, X_test.shape

We see the size of the datasets below. Note that there are 30 features in both the
training and test sets.

.. code:: python

    ((455, 30), (114, 30))

Now, we set up :class:`ProbeFeatureSelection()` to select features using the `collective`
strategy.

We will pass  `RandomForestClassifier()` as the :code:`estimator`. We will use `precision`
as the :code:`scoring` parameter and `5` as :code:`cv` parameter, both parameters to be
used in the cross validation.

In this example, we will introduce just 1 random feature with a normal distribution. Thus,
we pass `1` for the :code:`n_probes` parameter and `normal` as the :code:`distribution`.

.. code:: python

    sel = ProbeFeatureSelection(
        estimator=RandomForestClassifier(),
        variables=None,
        scoring="precision",
        n_probes=1,
        distribution="normal",
        cv=5,
        random_state=150,
        confirm_variables=False
    )

    sel.fit(X_train, y_train)

With :code:`fit()`, the transformer:

- creates `n_probes` number of probe features using provided distribution(s)
- uses cross-validation to fit the provided estimator
- calculates the feature importance score for each variable, including probe features
- if there are multiple probe features, the transformer calculates the average importance score
- identifies features to drop because their importance scores are less than that of the probe feature(s)

Analysing the probes
~~~~~~~~~~~~~~~~~~~~

In the attribute :code:`probe_features`, we find the pseudo-randomly generated variable(s):

.. code:: python

    sel.probe_features_.head()

.. code:: python

           gaussian_probe_0
    0         -0.694150
    1          1.171840
    2          1.074892
    3          1.698733
    4          0.498702

We can go ahead and display a histogram of the probe feature:

.. code:: python

    sel.probe_features_.hist(bins=30)

As we can see, it shows a normal distribution:

.. figure::  ../../images/probe_feature_normal.png
   :align:   center

|

Analysing the feature importance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The attribute :code:`feature_importances_` shows each variable's feature importance:

.. code:: python

    sel.feature_importances_.head()

These are the importance for the first 5 features:

.. code:: python

    mean radius        0.058463
    mean texture       0.011953
    mean perimeter     0.069516
    mean area          0.050947
    mean smoothness    0.004974
    dtype: float64

At the end of the series, we see the importance of the probe feature:

.. code:: python

    sel.feature_importances_.tail()

These are the importance of the last 5 features including the probe:

.. code:: python

    worst concavity            0.037844
    worst concave points       0.102769
    worst symmetry             0.011587
    worst fractal dimension    0.007456
    gaussian_probe_0           0.003783
    dtype: float64

In the attribute :code:`feature_importances_std_` we find the standard deviation of the
feature importance, which we can use for data analysis:

.. code:: python

    sel.feature_importances_std_.head()

These are the standard deviations for the first 5 features:

.. code:: python

    mean radius        0.013648
    mean texture       0.002571
    mean perimeter     0.025189
    mean area          0.010173
    mean smoothness    0.001650
    dtype: float64

We can go ahead and plot bar plots with the feature importance and the standard deviation:

.. code:: python

    r = pd.concat([
        sel.feature_importances_,
        sel.feature_importances_std_
    ], axis=1)

    r.columns = ["mean", "std"]

    r.sort_values("mean", ascending=False)["mean"].plot.bar(
        yerr=[r['std'], r['std']], subplots=True, figsize=(15,6)
    )
    plt.title("Feature importance derived from the random forests")
    plt.ylabel("Feature importance")
    plt.show()

In the following image, we see the importance of each feature, including the probe:

.. figure::  ../../images/probe-importance-std.png
   :align:   center

|

Selected features
~~~~~~~~~~~~~~~~~

In the attribute :code:`features_to_drop_`, we find the variables that were not selected:

.. code:: python

    sel.features_to_drop_

These are the variables that will be removed from the dataframe:

.. code:: python

    ['mean symmetry',
     'mean fractal dimension',
     'texture error',
     'smoothness error',
     'concave points error',
     'fractal dimension error']

We see that the :code:`features_to_drop_` have feature importance scores that are less
than the probe feature's score:

.. code:: python

    sel.feature_importances_.loc[sel.features_to_drop_+["gaussian_probe_0"]]

The previous command returns the following output:

.. code:: python

    mean symmetry              0.003698
    mean fractal dimension     0.003455
    texture error              0.003595
    smoothness error           0.003333
    concave points error       0.003548
    fractal dimension error    0.003576
    gaussian_probe_0           0.003783

Dropping features from the data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

With :code:`transform()`, we can go ahead and drop the six features with feature importance score
smaller than `gaussian_probe_0` variable:

.. code:: python

    Xtr = sel.transform(X_test)

    Xtr.shape

The final shape of the data after removing the features:

.. code:: python

    (114, 24)

Getting the name of the resulting features
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

And, finally, we can also obtain the names of the features in the final transformed dataset:

.. code:: python

    sel.get_feature_names_out()

In the following output we see the name of the features that will be present in the
transformed datasets:

.. code:: python

    ['mean radius',
     'mean texture',
     'mean perimeter',
     'mean area',
     'mean smoothness',
     'mean compactness',
     'mean concavity',
     'mean concave points',
     'radius error',
     'perimeter error',
     'area error',
     'compactness error',
     'concavity error',
     'symmetry error',
     'worst radius',
     'worst texture',
     'worst perimeter',
     'worst area',
     'worst smoothness',
     'worst compactness',
     'worst concavity',
     'worst concave points',
     'worst symmetry',
     'worst fractal dimension']

For compatibility with Scikit-learn selection transformers, :class:`ProbeFeatureSelection()`
also supports the method `get_support()`:

.. code:: python

    sel.get_support()

which returns the following output:

.. code:: python

    [True, True, True, True, True, True, True, True, False, False, True, False, True,
     True, False, True, True, False, True, False, True, True, True, True, True, True,
     True, True, True, True]

Using several probe features
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Let's now repeat the selection process, but using more than 1 probe feature.

.. code:: python

    sel = ProbeFeatureSelection(
        estimator=RandomForestClassifier(),
        variables=None,
        scoring="precision",
        n_probes=1,
        distribution="all",
        cv=5,
        random_state=150,
        confirm_variables=False
    )

    sel.fit(X_train, y_train)

Let's display the random features that the transformer created:

.. code:: python

    sel.probe_features_.head()

Here we find some example values of the probe features:

.. code:: python

       gaussian_probe_0  binary_probe_0  uniform_probe_0  \
    0         -0.694150               1         0.983610
    1          1.171840               1         0.765628
    2          1.074892               1         0.991439
    3          1.698733               0         0.668574
    4          0.498702               0         0.192840

       discrete_uniform_probe_0  poisson_probe_0
    0                         2                8
    1                         3                3
    2                         0                7
    3                         8                2
    4                         3               13

Let's go ahead and plot histograms:

.. code:: python

    sel.probe_features_.hist(bins=30, figsize=(10,10))
    plt.show()

In the histograms we recognise the 5 well defined distributions:

.. figure::  ../../images/probe_features.png
   :align:   center

Let's display the importance of the random features

.. code:: python

    sel.feature_importances_.tail()

.. code:: python

    gaussian_probe_0            0.004600
    binary_probe_0              0.000366
    uniform_probe_0             0.002541
    discrete_uniform_probe_0    0.001124
    poisson_probe_0             0.001759
    dtype: float64

We see that the binary feature has an extremely low importance, hence, when we take the
average, the value is so small, that no feature will be dropped (remember random forests
favouring highly cardinal features?):

.. code:: python

    sel.features_to_drop_

The previous command returns and empty list:

.. code:: python

    []

It is important to select a suitable probe feature distribution when trying to remove variables.
If most variables are continuous, introduce features with normal and uniform distributions.
If you have one hot encoded features or sparse matrices, binary features might be a better
option.

Changing the probe importance threshold
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We can make the selection process more aggressive by using the maximum of the probe features
or the mean plus 3 times the standard deviation as threshold to select features.

In the following example, we'll use the same random forest and the same probe features,
but this time, we'll select features whose importance is greater than the mean plus 3 times
the standard deviation of the probes:

.. code:: python

    sel = ProbeFeatureSelection(
        estimator=RandomForestClassifier(),
        variables=None,
        scoring="precision",
        n_probes=1,
        distribution="all",
        threshold = "mean_plus_std",
        cv=5,
        random_state=150,
        confirm_variables=False
    )

    sel.fit(X_train, y_train)

We now inspect the variables that will be removed:

.. code:: python

    sel.features_to_drop_

We see that now, several variables will be removed from the dataset:

.. code:: python

    ['mean smoothness',
     'mean symmetry',
     'mean fractal dimension',
     'texture error',
     'smoothness error',
     'compactness error',
     'concave points error',
     'symmetry error',
     'fractal dimension error']

Using the individual feature strategy
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We will now select features by training a random forest per feature and using the roc-auc
obtained from that model as a measure of feature importance:

.. code:: python

    sel = ProbeFeatureSelection(
        estimator=RandomForestClassifier(n_estimators=5, random_state=1),
        variables=None,
        collective=False,
        scoring="roc_auc",
        n_probes=1,
        distribution="all",
        cv=5,
        random_state=150,
        confirm_variables=False
    )

    sel.fit(X_train, y_train)

We can now go ahead and plot the feature importance, including that of the probes:

.. code:: python

    r = pd.concat([
        sel.feature_importances_,
        sel.feature_importances_std_
    ], axis=1)

    r.columns = ["mean", "std"]

    r.sort_values("mean", ascending=False)["mean"].plot.bar(
        yerr=[r['std'], r['std']], subplots=True, figsize=(15,6)
    )
    plt.title("Feature importance derived from single feature models")
    plt.ylabel("Feature importance - roc-auc")
    plt.show()

In the following image we see the feature importance, including the probes:

.. figure::  ../../images/single_feature_probes_imp.png
   :align:   center

When assessed individually, each feature seems to have a greater importance. Note that
many of the features return roc-auc that are not significantly different from the probes
(error bars overlaps). So, even if the transformer would not drop those features, we
could decide to discard them after analysis of this plot.

Alternatively, we can set the threshold to be more aggressive and drop features whose
importance is smaller than the mean plus three times the standard deviation of the
importance of the probes, as follows:

.. code:: python

    sel = ProbeFeatureSelection(
        estimator=RandomForestClassifier(n_estimators=5, random_state=1),
        variables=None,
        collective=False,
        scoring="roc_auc",
        n_probes=1,
        distribution="all",
        threshold = "mean_plus_std",
        cv=5,
        random_state=150,
        confirm_variables=False
    ).fit(X_train, y_train)

Additional resources
--------------------

More info about this method can be found in these resources:

- `Kaggle Tips for Feature Engineering and Selection <https://www.youtube.com/watch?v=RtqtM1UJfZc&t=3150s>`_, by Gilberto Titericz.
- `Feature Selection: Beyond feature importance? <https://www.kdnuggets.com/2019/10/feature-selection-beyond-feature-importance.html>`_, KDDNuggets.

For more details about this and other feature selection methods check out these resources:

.. figure::  ../../images/fsml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/fsmlbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection in Machine Learning

|
|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/selection/RecursiveFeatureAddition.rst
================================================
.. _recursive_addition:

.. currentmodule:: feature_engine.selection

RecursiveFeatureAddition
========================

:class:`RecursiveFeatureAddition` implements recursive feature addition (RFA), which is
a forward feature selection process.

This method starts by training a machine learning model using the entire set of variables
and then derives the feature importance from this model. The feature importance is given by
the coefficients of the linear models (`coef_` attribute) or the feature importance derived
from decision tree-based models (`feature_importances_` attribute).

If the machine learning model does not have the attributes `coef_` or `feature_importances_`,
then the initial feature importance is determined by feature permutation.

In the next step, :class:`RecursiveFeatureAddition` trains a model only using the feature
with the highest importance and stores this model's performance.

Then, :class:`RecursiveFeatureAddition` adds the second most important feature, trains a
new machine learning model, and determines its performance. If the performance increases
beyond a threshold (compared to the previous model with just 1 feature), then the second
feature is deemed important and will be kept. Otherwise, it is removed.

:class:`RecursiveFeatureAddition` proceeds to evaluate the next most important feature
by adding it to the feature set, training a new machine learning model, obtaining its performance,
determining the performance change, and so on, until all features are evaluated.

Note that the feature importance derived from the initial machine learning model is used
just to rank features and thus determine the order in which the features will be added.
But whether to retain a feature is determined based on the increase in the performance of
the model after the feature addition.

Parameters
----------

:class:`RecursiveFeatureAddition` has 2 parameters that need to be determined somewhat arbitrarily by
the user: the first one is the machine learning model which performance will be evaluated. The
second is the threshold in the performance increase that needs to occur, to keep a feature.

RFA is not machine learning model agnostic. This means that the feature selection depends on
the model, and different models may have different subsets of optimal features. Thus, it is
recommended that you use the machine learning model that you finally intend to build.

Regarding the threshold, this parameter needs a bit of hand tuning. Higher thresholds will
return fewer features.

Python example
--------------

Let's see how to use this transformer with the diabetes dataset that comes in Scikit-learn.
First, we load the data:

.. code:: python

    import matplotlib.pyplot as plt
    import pandas as pd
    from sklearn.datasets import load_diabetes
    from sklearn.linear_model import LinearRegression
    from feature_engine.selection import RecursiveFeatureAddition

    # load dataset
    X, y = load_diabetes(return_X_y=True, as_frame=True)

    print(X.head())

In the following output we see the diabetes dataset:

.. code:: python

            age       sex       bmi        bp        s1        s2        s3  \
    0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401
    1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412
    2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356
    3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038
    4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142

             s4        s5        s6
    0 -0.002592  0.019907 -0.017646
    1 -0.039493 -0.068332 -0.092204
    2 -0.002592  0.002861 -0.025930
    3  0.034309  0.022688 -0.009362
    4 -0.002592 -0.031988 -0.046641

Now, we set up :class:`RecursiveFeatureAddition` to select features based on the r2
returned by a Linear Regression model, using 3 fold cross-validation. In this case,
we leave the parameter `threshold` to the default value which is 0.01.

.. code:: python

    # initialize linear regression estimator
    linear_model = LinearRegression()

    # initialize feature selector
    tr = RecursiveFeatureAddition(estimator=linear_model, scoring="r2", cv=3)

With `fit()` the model finds the most useful features, that is, features that when added,
caused an increase in model performance bigger than 0.01. With `transform()`, the transformer
removes the features from the dataset.

.. code:: python

    Xt = tr.fit_transform(X, y)
    print(Xt.head())

Only 4 features were deemed important by recursive feature addition with linear regression:

.. code:: python

            bmi        bp        s1        s5
    0  0.061696  0.021872 -0.044223  0.019907
    1 -0.051474 -0.026328 -0.008449 -0.068332
    2  0.044451 -0.005670 -0.045599  0.002861
    3 -0.011595 -0.036656  0.012191  0.022688
    4 -0.036385  0.021872  0.003935 -0.031988

:class:`RecursiveFeatureAddition` stores the performance of the model trained using all
the features in its attribute:

.. code:: python

    # get the initial linear model performance, using all features
    tr.initial_model_performance_

In the following output we see the performance of the linear regression trained on the
entire dataset:

.. code:: python

    0.488702767247119

Evaluating feature importance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The coefficients of the linear regression are used to determine the initial feature importance
score, which is used to sort the features before applying the recursive addition process. We
can check out the feature importance as follows:

.. code:: python

    tr.feature_importances_

In the following output we see the feature importance derived from the linear model:

.. code:: python

    s1     750.023872
    s5     741.471337
    bmi    522.330165
    s2     436.671584
    bp     322.091802
    sex    238.619526
    s4     182.174834
    s3     113.965992
    s6      64.768417
    age     41.418041
    dtype: float64

The feature importance is obtained using cross-validation, so :class:`RecursiveFeatureAddition`
also stores the standard deviation of the feature importance:

.. code:: python

    tr.feature_importances_std_

In the following output we see the standard deviation of the feature importance:

.. code:: python

    age     18.217152
    sex     68.354719
    bmi     86.030698
    bp      57.110383
    s1     329.375819
    s2     299.756998
    s3      72.805496
    s4      47.925822
    s5     117.829949
    s6      42.754774
    dtype: float64

The selection procedure is based on whether adding a feature increases the performance of
a model compared to the same model without that feature. We can check out the performance
changes as follows:

..  code:: python

    # Get the performance drift of each feature
    tr.performance_drifts_

In the following output we see the changes in performance returned by adding each feature:

..  code:: python

    {'s1': 0,
     's5': 0.28371458794131676,
     'bmi': 0.1377714799388745,
     's2': 0.0023327265047610735,
     'bp': 0.018759914615172735,
     'sex': 0.0027996354657459643,
     's4': 0.002695149440021638,
     's3': 0.002683934134630306,
     's6': 0.000304067408860742,
     'age': -0.007387230783454768}

We can also check out the standard deviation of the performance drift:

..  code:: python

    # Get the performance drift of each feature
    tr.performance_drifts_std_

In the following output we see the standard deviation of the changes in performance
returned by adding each feature:

..  code:: python

    {'s1': 0,
     's5': 0.029336910701570382,
     'bmi': 0.01752426732750277,
     's2': 0.020525965661877265,
     'bp': 0.017326401244547558,
     'sex': 0.00867675077259389,
     's4': 0.024234566449074676,
     's3': 0.023391851139598106,
     's6': 0.016865740401721313,
     'age': 0.02042081611218045}

We can now plot the performance change with the standard deviation to identify importance
features:

..  code:: python

    r = pd.concat([
        pd.Series(tr.performance_drifts_),
        pd.Series(tr.performance_drifts_std_)
    ], axis=1
    )
    r.columns = ['mean', 'std']

    r['mean'].plot.bar(yerr=[r['std'], r['std']], subplots=True)

    plt.title("Performance drift elicited by adding features")
    plt.ylabel('Mean performance drift')
    plt.xlabel('Features')
    plt.show()

In the following image we see the change in performance resulting from adding each feature
to a model:

.. figure::  ../../images/rfa_perf_drifts.png

For comparison, we can plot the feature importance derived from the linear regression
together with the standard deviation:

..  code:: python

    r = pd.concat([
        tr.feature_importances_,
        tr.feature_importances_std_,
    ], axis=1
    )
    r.columns = ['mean', 'std']

    r['mean'].plot.bar(yerr=[r['std'], r['std']], subplots=True)

    plt.title("Feature importance derived from the linear regression")
    plt.ylabel('Coefficients value')
    plt.xlabel('Features')
    plt.show()

In the following image we see the feature importance determined by the coefficients of
the linear regression:

.. figure::  ../../images/rfa_linreg_imp.png

We see that both plots coincide in that `s1` and `s5` are the most important features.
However, note that from the feature importance plot we'd think that `s2` and `bp`
are important (their coefficient value is relatively big), however, adding them to a
model that already contains `s1`, `s5` and `bmi`, doesn't result in an increase in model performance.
This suggests that there might be correlation between `s2` or `bp` and some of the most
important features (`s1`, `s5` and `bmi`).

Checking out the eliminated features
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:class:`RecursiveFeatureAddition` stores the features that will be dropped based
on the given threshold:

..  code:: python

    # the features to drop
    tr.features_to_drop_

These features were not deemed important by the RFA process:

..  code:: python

    ['age', 'sex', 's2', 's3', 's4', 's6']

:class:`RecursiveFeatureAddition` also has the `get_support()` method that works exactly
like that of Scikit-learn's feature selection classes:

..  code:: python

    tr.get_support()

The output contains True for the features that are selected and False for those that will
be dropped:

..  code:: python

    [False, False, True, True, True, False, False, False, True, False]

And that's it! You now now how to select features by recursively adding them to a dataset.

Additional resources
--------------------

For more details about this and other feature selection methods check out these resources:

.. figure::  ../../images/fsml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/fsmlbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection in Machine Learning

|
|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/selection/RecursiveFeatureElimination.rst
================================================
.. _recursive_elimination:

.. currentmodule:: feature_engine.selection

RecursiveFeatureElimination
============================

:class:`RecursiveFeatureElimination` implements recursive feature elimination. Recursive
feature elimination (RFE) is a backward feature selection process.

In Feature-engine's implementation of RFE, a feature will be kept or removed based on the
resulting change in model performance resulting of adding that feature to a
machine learning. This differs from Scikit-learn's implementation of
`RFE <https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html>`_
where a feature will be kept or removed based on the feature importance derived from a
machine learning model via it's coefficients parameters or 'feature_importances_` attribute.

Feature-engine's implementation of RFE begins by training a model on the entire set of variables,
and storing its performance value. From this same model, :class:`RecursiveFeatureElimination`
derives the feature importance through the `coef_` or `feature_importances_` attributes, depending
if it is a linear model or a tree-based algorithm. These feature importance value is used
to sort the features by increasing performance, to determine the order in which the features
will be recursively removed. The least important features are removed first.

**New**: If the machine learning model does not have the attributes `coef_` or `feature_importances_`,
then the initial feature importance is determined by feature permutation.

In the next step, :class:`RecursiveFeatureElimination` removes the least important feature
and trains a new machine learning model using the remaining variables. If the performance of
this model is worse than the performance from the previus model, then, the feature is kept
(because eliminating the feature caused a drop in model performance) otherwise, it removed.

:class:`RecursiveFeatureElimination` removes now the second least important feature, trains a new model,
compares its performance to the previous model, determines if it should remove or retain the feature,
and moves on to the next variable until it evaluates all the features in the dataset.

Note that, in Feature-engine's implementation of RFE, the feature importance is used
just to rank features and thus determine the order in which the features will be eliminated.
But whether to retain a feature is determined based on the decrease in the performance of the
model after the feature elimination.

By recursively eliminating features, RFE attempts to eliminate dependencies and
collinearity that may exist in the model.

Parameters
----------

:class:`RecursiveFeatureElimination` has 2 parameters that need to be determined somewhat arbitrarily by
the user: the first one is the machine learning model which performance will be evaluated. The
second is the threshold in the performance drop that needs to occur to remove a feature.

RFE is not machine learning model agnostic, this means that the feature selection depends on
the model, and different models may have different subsets of optimal features. Thus, it is
recommended that you use the machine learning model that you finally intend to build.

Regarding the threshold, this parameter needs a bit of hand tuning. Higher thresholds will
return fewer features.

Python example
--------------

Let's see how to use this transformer with the diabetes dataset that comes in Scikit-learn.
First, we load the data:

.. code:: python

    import matplotlib.pyplot as plt
    import pandas as pd
    from sklearn.datasets import load_diabetes
    from sklearn.linear_model import LinearRegression
    from feature_engine.selection import RecursiveFeatureElimination

    # load dataset
    X, y = load_diabetes(return_X_y=True, as_frame=True)

    print(X.head())

In the following output we see the diabetes dataset:

.. code:: python

            age       sex       bmi        bp        s1        s2        s3  \
    0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401
    1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412
    2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356
    3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038
    4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142

             s4        s5        s6
    0 -0.002592  0.019907 -0.017646
    1 -0.039493 -0.068332 -0.092204
    2 -0.002592  0.002861 -0.025930
    3  0.034309  0.022688 -0.009362
    4 -0.002592 -0.031988 -0.046641

Now, we set up :class:`RecursiveFeatureElimination` to select features based on the r2
returned by a Linear Regression model, using 3 fold cross-validation. In this case,
we leave the parameter `threshold` to the default value which is 0.01.

.. code:: python

    # initialize linear regresion estimator
    linear_model = LinearRegression()

    # initialize feature selector
    tr = RecursiveFeatureElimination(estimator=linear_model, scoring="r2", cv=3)

With `fit()` the model finds the most useful features, that is, features that when removed
cause a drop in model performance bigger than 0.01. With `transform()`, the transformer
removes the features from the dataset.

.. code:: python

    Xt = tr.fit_transform(X, y)
    print(Xt.head())

Six features were deemed important by recursive feature elimination with linear regression:

.. code:: python

            sex       bmi        bp        s1        s2        s5
    0  0.050680  0.061696  0.021872 -0.044223 -0.034821  0.019907
    1 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163 -0.068332
    2  0.050680  0.044451 -0.005670 -0.045599 -0.034194  0.002861
    3 -0.044642 -0.011595 -0.036656  0.012191  0.024991  0.022688
    4 -0.044642 -0.036385  0.021872  0.003935  0.015596 -0.031988

:class:`RecursiveFeatureElimination` stores the performance of the model trained using all
the features in its attribute:

.. code:: python

    # get the initial linear model performance, using all features
    tr.initial_model_performance_

In the following output we see the performance of the linear regression trained on the
entire dataset:

.. code:: python

    0.488702767247119

Evaluating feature importance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The coefficients of the linear regression are used to determine the initial feature importance
score, which is used to sort the features before applying the recursive elimination process. We
can check out the feature importance as follows:

.. code:: python

    tr.feature_importances_

In the following output we see the feature importance derived from the linear model:

.. code:: python

    age     41.418041
    s6      64.768417
    s3     113.965992
    s4     182.174834
    sex    238.619526
    bp     322.091802
    s2     436.671584
    bmi    522.330165
    s5     741.471337
    s1     750.023872
    dtype: float64

The feature importance is obtained using cross-validation, so :class:`RecursiveFeatureElimination`
also stores the standard deviation of the feature importance:

.. code:: python

    tr.feature_importances_std_

In the following output we see the standard deviation of the feature importance:

.. code:: python

    age     18.217152
    sex     68.354719
    bmi     86.030698
    bp      57.110383
    s1     329.375819
    s2     299.756998
    s3      72.805496
    s4      47.925822
    s5     117.829949
    s6      42.754774
    dtype: float64

The selection procedure is based on whether removing a feature decreases the performance of
a model compared to the same model with that feature. We can check out the performance
changes as follows:

..  code:: python

    # Get the performance drift of each feature
    tr.performance_drifts_

In the following output we see the changes in performance returned by removing each feature:

..  code:: python

    {'age': -0.0032800993162502845,
     's6': -0.00028194870232089997,
     's3': -0.0006751427734088544,
     's4': 0.00013890056776355575,
     'sex': 0.01195652626644067,
     'bp': 0.02863360798239445,
     's2': 0.012639242239088355,
     'bmi': 0.06630359039334816,
     's5': 0.10937354113435072,
     's1': 0.024318355833473526}

We can also check out the standard deviation of the performance drift:

..  code:: python

    # Get the performance drift of each feature
    tr.performance_drifts_std_

In the following output we see the standard deviation of the changes in performance
returned by eliminating each feature:

..  code:: python

    {'age': 0.013642261032787014,
     's6': 0.01678934235354838,
     's3': 0.01685859860738229,
     's4': 0.017977817100713972,
     'sex': 0.025202392033518706,
     'bp': 0.00841776123355417,
     's2': 0.008676750772593812,
     'bmi': 0.042463565656018436,
     's5': 0.046779680487815146,
     's1': 0.01621466049786452}

We can now plot the performance change with the standard deviation to identify importance
features:

..  code:: python

    r = pd.concat([
        pd.Series(tr.performance_drifts_),
        pd.Series(tr.performance_drifts_std_)
    ], axis=1
    )
    r.columns = ['mean', 'std']

    r['mean'].plot.bar(yerr=[r['std'], r['std']], subplots=True)

    plt.title("Performance drift elicited by adding features")
    plt.ylabel('Mean performance drift')
    plt.xlabel('Features')
    plt.show()

In the following image we see the change in performance resulting from removing each feature
from a model:

.. figure::  ../../images/rfe_perf_drift.png

For comparison, we can plot the feature importance derived from the linear regression
together with the standard deviation:

..  code:: python

    r = pd.concat([
        tr.feature_importances_,
        tr.feature_importances_std_,
    ], axis=1
    )
    r.columns = ['mean', 'std']

    r['mean'].plot.bar(yerr=[r['std'], r['std']], subplots=True)

    plt.title("Feature importance derived from the linear regression")
    plt.ylabel('Coefficients value')
    plt.xlabel('Features')
    plt.show()

In the following image we see the feature importance determined by the coefficients of
the linear regression:

.. figure::  ../../images/rfa_linreg_imp.png

By comparing the performance in both plots, we can begin to understand which features
are important, and which ones could show some correlation to other variables in the data.
If a feature has a relatively big coefficient, but removing it does not change the model
performance, then, it might be correlated to another variable in the data.

Checking out the eliminated features
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:class:`RecursiveFeatureElimination` also stores the features that will be dropped based
on the given threshold.

..  code:: python

    # the features to remove
    tr.features_to_drop_

These features were not deemed important by the RFE process:

..  code:: python

    ['age', 's3', 's4', 's6']

:class:`RecursiveFeatureElimination`  also has the `get_support()` method that works exactly
like that of Scikit-learn's feature selection classes:

..  code:: python

    tr.get_support()

The output contains True for the features that are selected and False for those that will
be dropped:

..  code:: python

    [False, True, True, True, True, True, False, False, True, False]

And that's it! You now now how to select features by recursively removing them to a dataset.

Additional resources
--------------------

More details on recursive feature elimination in this article:

- `Recursive feature elimination with Python <https://www.blog.trainindata.com/recursive-feature-elimination-with-python/>`_

For more details about this and other feature selection methods check out these resources:

For more details about this and other feature selection methods check out these resources:

.. figure::  ../../images/fsml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/fsmlbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection in Machine Learning

|
|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/selection/SelectByInformationValue.rst
================================================
.. _information_value:

.. currentmodule:: feature_engine.selection

SelectByInformationValue
========================

:class:`SelectByInformationValue()` selects features based on whether the feature's information value score is
greater than the threshold passed by the user.

The IV is calculated as:

.. math::

   IV = ∑ (fraction of positive cases - fraction of negative cases) * WoE

where:

- the fraction of positive cases is the proportion of observations of class 1, from the total class 1 observations.
- the fraction of negative cases is the proportion of observations of class 0, from the total class 0 observations.
- WoE is the weight of the evidence.

The WoE is calculated as:

.. math::

   WoE = ln(fraction of positive cases / fraction of negative cases)

Information value (IV) is used to assess a feature's predictive power of a binary-class dependent
variable. To derive a feature's IV, the weight of evidence (WoE) must first be calculated for each
unique category or bin that comprises the feature. If a category or bin contains a large percentage
of true or positive labels compared to the percentage of false or negative labels, then that category
or bin will have a high WoE value.

Once the WoE is derived, :class:`SelectByInformationValue()` calculates the IV for each variable.
A variable's IV is essentially the weighted sum of the individual WoE values for each category or bin
within that variable where the weights incorporate the absolute difference between the
numerator and denominator. This value assesses the feature's predictive power in capturing the binary
dependent variable.

The table below presents a general framework for using IV to determine a variable's predictive power:

.. list-table::
    :widths: 30 30
    :header-rows: 1

    * - Information Value
      - Predictive Power
    * - < 0.02
      - Useless
    * - 0.02 to 0.1
      - Weak
    * - 0.1 to 0.3
      - Medium
    * - 0.3 to 0.5
      - Strong
    * - > 0.5
      - Suspicious, too good to be true

Table taken from `listendata <https://www.listendata.com/2015/03/weight-of-evidence-woe-and-information.html>`_.

Example
-------
Let's see how to use this transformer to select variables from UC Irvine's credit approval data set which can
be found `here`_. This dataset concerns credit card applications. All attribute names and values have been changed
to meaningless symbols to protect confidentiality.

The data is comprised of both numerical and categorical data.

.. _here: https:

Let's import the required libraries and classes:

.. code:: python

    import pandas as pd
    import numpy as np
    from sklearn.model_selection import train_test_split
    from feature_engine.selection import SelectByInformationValue

Let's now load and prepare the credit approval data:

.. code:: python

    # load data
    data = pd.read_csv('crx.data', header=None)

    # name variables
    var_names = ['A' + str(s) for s in range(1,17)]
    data.columns = var_names
    data.rename(columns={'A16': 'target'}, inplace=True)

    # preprocess data
    data = data.replace('?', np.nan)
    data['A2'] = data['A2'].astype('float')
    data['A14'] = data['A14'].astype('float')
    data['target'] = data['target'].map({'+':1, '-':0})

    # drop rows with missing data
    data.dropna(axis=0, inplace=True)

    data.head()

Let's now review the first 5 rows of the dataset:

.. code:: python

      A1     A2     A3 A4 A5 A6 A7    A8 A9 A10  A11 A12 A13    A14  A15  target
    0  b  30.83  0.000  u  g  w  v  1.25  t   t    1   f   g  202.0    0       1
    1  a  58.67  4.460  u  g  q  h  3.04  t   t    6   f   g   43.0  560       1
    2  a  24.50  0.500  u  g  q  h  1.50  t   f    0   f   g  280.0  824       1
    3  b  27.83  1.540  u  g  w  v  3.75  t   t    5   t   g  100.0    3       1
    4  b  20.17  5.625  u  g  w  v  1.71  t   f    0   f   s  120.0    0       1

Let's now split the data into train and test sets:

.. code:: python

    # separate train and test sets
    X_train, X_test, y_train, y_test = train_test_split(
        data.drop(['target'], axis=1),
        data['target'],
        test_size=0.2,
        random_state=0)

    X_train.shape, X_test.shape

We see the size of the datasets below.

.. code:: python

    ((522, 15), (131, 15))

Now, we set up :class:`SelectByInformationValue()`. We will pass six categorical
variables to the parameter :code:`variables`. We will set the parameter :code:`threshold`
to `0.2`. We see from the above mentioned table that an IV score of 0.2 signifies medium
predictive power.

.. code:: python

    sel = SelectByInformationValue(
        variables=['A1', 'A6', 'A9', 'A10', 'A12', 'A13'],
        threshold=0.2,
    )

    sel.fit(X_train, y_train)

With :code:`fit()`, the transformer:

 - calculates the WoE for each variable
 - calculates the the IV for each variable
 - identifies the variables that have an IV score below the threshold

In the attribute :code:`variables_`, we find the variables that were evaluated:

.. code:: python

    ['A1', 'A6', 'A7', 'A9', 'A10', 'A12', 'A13']

In the attribute :code:`features_to_drop_`, we find the variables that were not selected:

.. code:: python

    sel.features_to_drop_

    ['A1', 'A12', 'A13']

The attribute :code:`information_values_` shows the IV scores for each variable.

.. code:: python

   {'A1': 0.0009535686492270659,
    'A6': 0.6006252129425703,
    'A9': 2.9184484098456807,
    'A10': 0.8606638171665587,
    'A12': 0.012251943759377052,
    'A13': 0.04383964979386022}

We see that the transformer correctly selected the features that have an IV score greater
than the :code:`threshold` which was set to 0.2.

The transformer also has the method `get_support` with similar functionality to Scikit-learn's
selectors method. If you execute `sel.get_support()`, you obtain:

.. code:: python

    [False, True, True, True, True, True, True,
     True, True, True, True, False, False, True,
     True]

With :code:`transform()`, we can go ahead and drop the features that do not meet the threshold:

.. code:: python

    Xtr = sel.transform(X_test)

    Xtr.head()

.. code:: python

            A2     A3 A4 A5  A6 A7      A8 A9 A10  A11    A14  A15
    564  42.17   5.04  u  g   q  h  12.750  t   f    0   92.0    0
    519  39.17   1.71  u  g   x  v   0.125  t   t    5  480.0    0
    14   45.83  10.50  u  g   q  v   5.000  t   t    7    0.0    0
    257  20.00   0.00  u  g   d  v   0.500  f   f    0  144.0    0
    88   34.00   4.50  u  g  aa  v   1.000  t   f    0  240.0    0

Note that :code:`Xtr` includes all the numerical features - i.e., A2, A3, A8, A11, and A14 - because
we only evaluated a few of the categorical features.

And, finally, we can also obtain the names of the features in the final transformed dataset:

.. code:: python

    sel.get_feature_names_out()

    ['A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10', 'A11', 'A14', 'A15']

If we want to select from categorical and numerical variables, we can do so as well by
sorting the numerical variables into bins first. Let's sort them into 5 bins of equal-frequency:

.. code:: python

    sel = SelectByInformationValue(
        bins=5,
        strategy="equal_frequency",
        threshold=0.2,
    )

    sel.fit(X_train.drop(["A4", "A5", "A7"], axis=1), y_train)

If we now inspect the information values:

.. code:: python

   sel.information_values_

We see the following:

.. code:: python

    {'A1': 0.0009535686492270659,
     'A2': 0.10319123021570434,
     'A3': 0.2596258749173557,
     'A6': 0.6006252129425703,
     'A8': 0.7291628533346297,
     'A9': 2.9184484098456807,
     'A10': 0.8606638171665587,
     'A11': 1.0634602064399297,
     'A12': 0.012251943759377052,
     'A13': 0.04383964979386022,
     'A14': 0.3316668794040285,
     'A15': 0.6228678069374612}

And if we inspect the features to drop:

.. code:: python

   sel.features_to_drop_

We see the following:

.. code:: python

    ['A1', 'A2', 'A12', 'A13']

Note
----

The WoE is given by a logarithm of a fraction. Thus, if for any category or bin, the fraction of
observations of class 0 is 0, the WoE is not defined, and the transformer will raise an error.

If you encounter this problem try grouping variables into fewer bins if they are numerical,
or grouping rare categories with the RareLabelEncoder if they are categorical.

Additional resources
--------------------

For more details about this and other feature selection methods check out these resources:

.. figure::  ../../images/fsml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/fsmlbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection in Machine Learning

|
|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/selection/SelectByShuffling.rst
================================================
.. _feature_shuffling:

.. currentmodule:: feature_engine.selection

SelectByShuffling
=================

:class:`SelectByShuffling()` selects features whose random value permutation reduces model
performance. If a feature is predictive, shuffling its values across rows will result in
predictions that deviate significantly from the actual outcomes. Conversely, if the
feature is not predictive, altering the order of its values will have little to no impact
on the model's predictions.

Procedure
---------

The algorithm operates as follows:

1. Train a machine learning model using all available features.
2. Establish a baseline performance metric for the model.
3. Shuffle the values of a single feature while keeping all other features unchanged.
4. Use the model from step 1 to generate predictions with the shuffled feature.
5. Measure the model's performance based on these new predictions.
6. If the performance drops beyond a predefined threshold, retain the feature.
7. Repeat steps 3-6 for each feature until all have been evaluated.

Python Example
--------------

Let's see how to use :class:`SelectByShuffling()` with the diabetes dataset that comes
with Scikit-learn. First, we load the data:

.. code:: python

    import pandas as pd
    import matplotlib.pyplot as plt
    from sklearn.datasets import load_diabetes
    from sklearn.linear_model import LinearRegression
    from feature_engine.selection import SelectByShuffling

    X, y = load_diabetes(return_X_y=True, as_frame=True)
    print(X.head())

In the following output, we see the diabetes dataset:

.. code:: python

            age       sex       bmi        bp        s1        s2        s3  \
    0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401
    1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412
    2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356
    3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038
    4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142

             s4        s5        s6
    0 -0.002592  0.019907 -0.017646
    1 -0.039493 -0.068332 -0.092204
    2 -0.002592  0.002861 -0.025930
    3  0.034309  0.022688 -0.009362
    4 -0.002592 -0.031988 -0.046641

Now, we set up a machine learning model. We'll use a linear regression:

.. code:: python

    linear_model = LinearRegression()

Now, we set up :class:`SelectByShuffling()` to select features by shuffling. We'll examine
the change in the `r2` using 3 fold cross-validation.

The parameter `threshold` is left to None, which means that features will be selected if
the performance drop is bigger than the mean drop caused by all features.

.. code:: python

    tr = SelectByShuffling(
        estimator=linear_model,
        scoring="r2",
        cv=3,
        random_state=0,
    )

The `fit`()` method identifies important variables—those whose value permutations lead
to a decline in model performance. The `transform()` method then removes these variables
from the dataset.

.. code:: python

    Xt = tr.fit_transform(X, y)

:class:`SelectByShuffling()` stores the performance of the model trained using all the
features in its attribute:

.. code:: python

    tr.initial_model_performance_

In the following output we see the r2 of the linear regression trained and evaluated on
the entire dataset, without shuffling, using cross-validation.

.. code:: python

    0.488702767247119

In the following sections, we'll explore some of the additional useful data stored by
:class:`SelectByShuffling()`.

Evaluating feature importance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:class:`SelectByShuffling()` stores the change in the model performance caused by shuffling
every feature.

..  code:: python

    tr.performance_drifts_

In the following output, we see the change in the linear regression r2 after shuffling
each feature:

.. code:: python

    {'age': -0.0054698043007869734,
     'sex': 0.03325633986510784,
     'bmi': 0.184158237207512,
     'bp': 0.10089894421748086,
     's1': 0.49324432634948095,
     's2': 0.21163252880660438,
     's3': 0.02006839198785859,
     's4': 0.011098050006761673,
     's5': 0.4828781996541602,
     's6': 0.003963360084439538}

:class:`SelectByShuffling()` stores the standard deviation of the performance change:

.. code:: python

    tr.performance_drifts_std_

In the following output, we see the variability of the change in r2 after feature
shuffling:

.. code:: python

    {'age': 0.012788500580799392,
     'sex': 0.040792331972680645,
     'bmi': 0.042212436355346106,
     'bp': 0.05397012536801143,
     's1': 0.35198797776358015,
     's2': 0.167636042355086,
     's3': 0.03455158514716544,
     's4': 0.007755675852874145,
     's5': 0.1449579162698361,
     's6': 0.011193022434166025}

We can plot the performance change together with the standard deviation to get a better
idea of how shuffling features affect the model performance:

..  code:: python

    r = pd.concat([
        pd.Series(tr.performance_drifts_),
        pd.Series(tr.performance_drifts_std_)
    ], axis=1
    )
    r.columns = ['mean', 'std']

    r['mean'].plot.bar(yerr=[r['std'], r['std']], subplots=True)

    plt.title("Performance drift elicited by shuffling a feature")
    plt.ylabel('Mean performance drift')
    plt.xlabel('Features')
    plt.show()

In the following image we see the change in performance resulting from shuffling each
feature:

.. figure::  ../../images/shuffle-features-std.png

With this set up, features that elicited a mean performance drop greater than the mean
performance of all features, will be removed. If, for any reason, this threshold is too
conservative or too permissive, by analysing the former barplot, you can get a better
idea of how these features affect the predictions of the model, and select a different
threshold.

Checking out the eliminated features
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:class:`SelectByShuffling()` stores the features that will be dropped based on a certain
threshold:

.. code:: python

    tr.features_to_drop_

The following features were deemed as non-important, because their performance drift is
greater than the mean performance drift of all features:

.. code:: python

    ['age', 'sex', 'bp', 's3', 's4', 's6']

If we now print the transformed data, we see that the features above were removed.

..  code:: python

    print(Xt.head())

In the following output, we see the dataframe with the selected features:

..  code:: python

            bmi        s1        s2        s5
    0  0.061696 -0.044223 -0.034821  0.019907
    1 -0.051474 -0.008449 -0.019163 -0.068332
    2  0.044451 -0.045599 -0.034194  0.002861
    3 -0.011595  0.012191  0.024991  0.022688
    4 -0.036385  0.003935  0.015596 -0.031988

Additional resources
--------------------

For more details about this and other feature selection methods check out these resources:

.. figure::  ../../images/fsml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/fsmlbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection in Machine Learning

|
|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/selection/SelectBySingleFeaturePerformance.rst
================================================
.. _single_feat_performance:

.. currentmodule:: feature_engine.selection

SelectBySingleFeaturePerformance
================================

:class:`SelectBySingleFeaturePerformance()` selects features based on the performance of
machine learning models trained on each feature individually. In other words, it
identifies features that demonstrate strong predictive power on their own. The selection
process works as follows:

1. Train a separate machine learning model using only one feature at a time.
2. Evaluate each model using a chosen performance metric.
3. Retain features whose performance exceeds a specified threshold.

If the `threshold` parameter is set to `None`, the algorithm will select features with
performance above the average of all individual features.

Python Example
--------------

Let's see how to use :class:`SelectBySingleFeaturePerformance()` with the diabetes
dataset that comes with Scikit-learn. First, we load the data:

.. code:: python

    import pandas as pd
    import matplotlib.pyplot as plt
    from sklearn.datasets import load_diabetes
    from sklearn.linear_model import LinearRegression
    from feature_engine.selection import SelectBySingleFeaturePerformance

    X, y = load_diabetes(return_X_y=True, as_frame=True)
    print(X.head())

In the following output, we see the diabetes dataset:

.. code:: python

            age       sex       bmi        bp        s1        s2        s3  \
    0  0.038076  0.050680  0.061696  0.021872 -0.044223 -0.034821 -0.043401
    1 -0.001882 -0.044642 -0.051474 -0.026328 -0.008449 -0.019163  0.074412
    2  0.085299  0.050680  0.044451 -0.005670 -0.045599 -0.034194 -0.032356
    3 -0.089063 -0.044642 -0.011595 -0.036656  0.012191  0.024991 -0.036038
    4  0.005383 -0.044642 -0.036385  0.021872  0.003935  0.015596  0.008142

             s4        s5        s6
    0 -0.002592  0.019907 -0.017646
    1 -0.039493 -0.068332 -0.092204
    2 -0.002592  0.002861 -0.025930
    3  0.034309  0.022688 -0.009362
    4 -0.002592 -0.031988 -0.046641

Let's set up :class:`SelectBySingleFeaturePerformance()` to select features based on the
r2 returned by a Linear regression, using 3 fold cross-validation. We want to select features
which r2 > 0.01.

.. code:: python

    # initialize feature selector
    sel = SelectBySingleFeaturePerformance(
            estimator=LinearRegression(), scoring="r2", cv=3, threshold=0.01)

With `fit()` the transformer fits 1 model per feature, determines the performance and
selects the important features:

.. code:: python

    # fit transformer
    sel.fit(X, y)

The features that will be dropped are stored in the following attribute:

.. code:: python

    sel.features_to_drop_

Only one feature will be dropped, because a linear model trained using this feature showed
an r2 smaller than 0. 1:

.. code:: python

    [sex]

Evaluating feature importance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:class:`SelectBySingleFeaturePerformance()` stores the performance of each one of the
single feature models:

..  code:: python

    sel.feature_performance_

In the following output, we see the r2 of every linear regression trained using the
feature in the key of the dictionary:

.. code:: python

    {'age': 0.029231969375784466,
     'sex': -0.003738551760264386,
     'bmi': 0.33662080998769284,
     'bp': 0.19218913007834937,
     's1': 0.037115559827549806,
     's2': 0.017854228256932614,
     's3': 0.1515388617752689,
     's4': 0.1772160996650173,
     's5': 0.31494478799681097,
     's6': 0.13876602125792703}

We can also check out the standard deviation of the r2 as follows:

.. code:: python

    sel.feature_performance_std_

In the following output, we see the standard deviation:

.. code:: python

    {'age': 0.017870583127141664,
     'sex': 0.005465336770744777,
     'bmi': 0.04257342727445452,
     'bp': 0.027318947204928765,
     's1': 0.031397211603399186,
     's2': 0.03224477055466249,
     's3': 0.020243573053986438,
     's4': 0.04782262499458294,
     's5': 0.02473650354444323,
     's6': 0.029051175300521623}

We can plot the performance together with the standard deviation to get a better
idea of the model performance's variability:

..  code:: python

    r = pd.concat([
        pd.Series(sel.feature_performance_),
        pd.Series(sel.feature_performance_std_)
    ], axis=1
    )
    r.columns = ['mean', 'std']

    r['mean'].plot.bar(yerr=[r['std'], r['std']], subplots=True)

    plt.title("Single feature model Performance")
    plt.ylabel('R2')
    plt.xlabel('Features')
    plt.show()

In the following image we see the single feature model performance:

.. figure::  ../../images/single-feature-perf-std.png

With this, we can get a better idea of the relationship between the features and the
target variable, based on a linear regression model.

Checking out the resulting dataframe
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

With `transform()` we go ahead and remove the features from the dataset:

.. code:: python

    # drop variables
    Xt = sel.transform(X)

We can now print the transformed data:

..  code:: python

    print(Xt.head())

In the following output, we see the selected features:

..  code:: python

            age       bmi        bp        s1        s2        s3        s4  \
    0  0.038076  0.061696  0.021872 -0.044223 -0.034821 -0.043401 -0.002592
    1 -0.001882 -0.051474 -0.026328 -0.008449 -0.019163  0.074412 -0.039493
    2  0.085299  0.044451 -0.005670 -0.045599 -0.034194 -0.032356 -0.002592
    3 -0.089063 -0.011595 -0.036656  0.012191  0.024991 -0.036038  0.034309
    4  0.005383 -0.036385  0.021872  0.003935  0.015596  0.008142 -0.002592

             s5        s6
    0  0.019907 -0.017646
    1 -0.068332 -0.092204
    2  0.002861 -0.025930
    3  0.022688 -0.009362
    4 -0.031988 -0.046641

Additional resources
--------------------

Check also:

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/selection/Select-by-Single-Feature-Performance.ipynb>`_

All notebooks can be found in a `dedicated repository <https://github.com/feature-engine/feature-engine-examples>`_.

For more details about this and other feature selection methods check out these resources:

For more details about this and other feature selection methods check out these resources:

.. figure::  ../../images/fsml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/fsmlbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection in Machine Learning

|
|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/selection/SelectByTargetMeanPerformance.rst
================================================
.. _target_mean_selection:

.. currentmodule:: feature_engine.selection

SelectByTargetMeanPerformance
=============================

:class:`SelectByTargetMeanPerformance()` selects features based on a performance metric,
like ROC-AUC or accuracy for classification, or mean squared error and R-squared
for regression.

Performance metrics are obtained by comparing a prediction with the real value of the
target. The closer the values of the prediction to the real target, the better the value
of the performance metric. Typically, these predictions are obtained from machine learning
models.

:class:`SelectByTargetMeanPerformance()` uses a very simple method to obtain "predictions".
It returns the mean target value per category or per interval if the variable is continuous.
With this "prediction", it determines the value of a performance metric of choice for each
feature, by comparing the values of the "predictions" with that of the target.

Procedure
---------

This feature selection idea is very simple; it involves taking the mean of the
responses (target) for each level (category or interval) of the variable, and so amounts to a least
squares fit on a single categorical variable against a response variable, with the
categories in the continuous variables defined by intervals.

Despite its simplicity, the method has a number of advantages:

- Speed: Computing means and intervals is fast, straightforward and efficient.
- Stability with respect to feature magnitude: Extreme values for continuous variables do not skew predictions as they would in many models.
- Comparability between continuous and categorical variables.
- Does not assume linear relationships and hence can identify non-linearities.
- Does not require encoding categorical variables into numbers.

The method has also some limitations. First, the selection of the number of intervals
as well as the threshold is arbitrary. And also, rare categories and very skewed
variables will raise errors when NAN are accidentally introduced during the evaluation.

:class:`SelectByTargetMeanPerformance()` works with cross-validation. It uses the k-1
folds to define the numerical intervals and learn the mean target value per category or
interval. Then, it uses the remaining fold to evaluate the performance of the
feature: that is, in the last fold it sorts numerical variables into the bins, replaces
bins and categories by the learned target estimates, and calculates the performance of
each feature.

Important
---------

:class:`SelectByTargetMeanPerformance()` automatically identifies numerical and
categorical variables. It will select as categorical variables, those cast as object
or categorical, and as numerical variables those of type numeric. Therefore, make sure
that your variables are of the correct data type.

Troubleshooting
---------------

The main problem that you may encounter using this selector is having missing data
introduced in the variables when replacing the categories or the intervals by the
target mean estimates.

Categorical variables
~~~~~~~~~~~~~~~~~~~~~

NAN are introduced in categorical variables when a category present in the kth fold was
not present in the k-1 fold used to calculate the mean target value per category. This
is probably due to the categorical variable having high cardinality (a lot of categories)
or rare categories, that is, categories present in a small fraction of the observations.

If this happens, try reducing the cardinality of the variable, for example by grouping
rare labels into a single group. Check the :ref:`RareLabelEncoder <rarelabel_encoder>`
for more details.

Numerical variables
~~~~~~~~~~~~~~~~~~~

NAN are introduced in numerical variables when an interval present in the kth cross-validation
fold was not present in the k-1 fold used to calculate the mean target value per interval.
This is probably due to the numerical variable being highly skewed, or having few unique values,
for example, if the variable is discrete instead of continuous.

If this happens, check the distribution of the problematic variable and try to identify
the problem. Try using equal-frequency intervals instead of equal-width and also reducing
the number of bins.

If the variable is discrete and has few unique values, another thing you could do is
casting the variable as object, so that the selector evaluates the mean target value
per unique value.

Finally, if a numerical variable is truly continuous and not skewed, check that it is
not accidentally cast as object.

Example
-------

Let's see how to use this method to select variables in the Titanic dataset. This data
has a mix of numerical and categorical variables, then it is a good option to showcase
this selector.

Let's import the required libraries and classes, and prepare the titanic dataset:

.. code:: python

    import numpy as np
    import matplotlib.pyplot as plt
    import pandas as pd
    from sklearn.model_selection import train_test_split

    from feature_engine.datasets import load_titanic
    from feature_engine.encoding import RareLabelEncoder
    from feature_engine.selection import SelectByTargetMeanPerformance

    data = load_titanic(
        handle_missing=True,
        predictors_only=True,
        cabin="letter_only",
    )

    # replace infrequent cabins by N
    data['cabin'] = np.where(data['cabin'].isin(['T', 'G']), 'N', data['cabin'])

    # cap maximum values
    data['parch'] = np.where(data['parch']>3,3,data['parch'])
    data['sibsp'] = np.where(data['sibsp']>3,3,data['sibsp'])

    # cast variables as object to treat as categorical
    data[['pclass','sibsp','parch']] = data[['pclass','sibsp','parch']].astype('O')

    print(data.head())

We can see the first 5 rows of data below:

.. code:: python

      pclass  survived     sex      age sibsp parch      fare cabin embarked
    0      1         1  female  29.0000     0     0  211.3375     B        S
    1      1         1    male   0.9167     1     2  151.5500     C        S
    2      1         0  female   2.0000     1     2  151.5500     C        S
    3      1         0    male  30.0000     1     2  151.5500     C        S
    4      1         0  female  25.0000     1     2  151.5500     C        S

Let's now go ahead and split the data into train and test sets:

.. code:: python

    # separate train and test sets
    X_train, X_test, y_train, y_test = train_test_split(
        data.drop(['survived'], axis=1),
        data['survived'],
        test_size=0.1,
        random_state=0)

    X_train.shape, X_test.shape

We see the sizes of the datasets below:

.. code:: python

    ((1178, 8), (131, 8))

Now, we set up :class:`SelectByTargetMeanPerformance()`. We will examine the roc-auc
using 3 fold cross-validation. We will separate numerical variables into equal-frequency
intervals. And we will retain those variables where the roc-auc is bigger than the mean
ROC-AUC of all features (default functionality).

.. code:: python

    sel = SelectByTargetMeanPerformance(
        variables=None,
        scoring="roc_auc",
        threshold=None,
        bins=3,
        strategy="equal_frequency",
        cv=3,
        regression=False,
    )

    sel.fit(X_train, y_train)

With `fit()` the transformer:

- replaces categories by the target mean
- sorts numerical variables into equal-frequency bins
- replaces bins by the target mean
- calculates the the roc-auc for each transformed variable
- selects features which roc-auc bigger than the average

In the attribute `variables_` we find the variables that were evaluated:

.. code:: python

    sel.variables_

.. code:: python

    ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'cabin', 'embarked']

In the attribute `features_to_drop_` we find the variables that were not selected:

.. code:: python

    sel.features_to_drop_

.. code:: python

    ['age', 'sibsp', 'parch', 'embarked']

Evaluating feature importance
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In the attribute `feature_performance_` we find the ROC-AUC for each feature. Remember
that this is the average ROC-AUC in each cross-validation fold:

.. code:: python

    sel.feature_performance_

In the following output we see the ROC-AUC returned by the target mean encoding of
each variable:

.. code:: python

    {'pclass': 0.668151138112005,
    'sex': 0.764831274819234,
    'age': 0.535490029737471,
    'sibsp': 0.5815934176199077,
    'parch': 0.5721327969642238,
    'fare': 0.6545985745474006,
    'cabin': 0.630092526712033,
    'embarked': 0.5765961846034091}

The mean ROC-AUC of all features is 0.62, we can calculate it as follows:

.. code:: python

    pd.Series(sel.feature_performance_).mean()

    0.6229357428894605

In the attribute `feature_performance_std_` we find the standard deviation of the
ROC-AUC for each feature:

.. code:: python

    sel.feature_performance_std_

Below we see the standard deviation of the ROC-AUC:

.. code:: python

    {'pclass': 0.0062490415569808975,
     'sex': 0.006574623168243345,
     'age': 0.023454310730681827,
     'sibsp': 0.007263903286722272,
     'parch': 0.017865107795851633,
     'fare': 0.01669212962579665,
     'cabin': 0.006868970787685758,
     'embarked': 0.008925910686325774}

We can plot the performance together with the standard deviation to get a better
idea of the feature's importance:

..  code:: python

    r = pd.concat([
        pd.Series(sel.feature_performance_),
        pd.Series(sel.feature_performance_std_)
    ], axis=1
    )
    r.columns = ['mean', 'std']

    r['mean'].plot.bar(yerr=[r['std'], r['std']], subplots=True)

    plt.title("Feature importance")
    plt.ylabel('ROC-AUC')
    plt.xlabel('Features')
    plt.show()

In the following image we see the feature importance:

.. figure::  ../../images/target-mean-sel-std.png

With this, we can get a better idea of the relationship between the features and the
target variable, based on a linear regression model.

Checking out the resulting dataframe
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

With `transform()` we can go ahead and drop the features:

.. code:: python

    Xtr = sel.transform(X_test)

    Xtr.head()

.. code:: python

         pclass     sex     fare cabin
    1139      3    male   7.8958     M
    533       2  female  21.0000     M
    459       2    male  27.0000     M
    1150      3    male  14.5000     M
    393       2    male  31.5000     M

And finally, we can also obtain the names of the features in the final transformed
data:

.. code:: python

    sel.get_feature_names_out()

.. code:: python

    ['pclass', 'sex', 'fare', 'cabin']

Additional resources
--------------------

Check also:

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/selection/Select-by-Target-Mean-Encoding.ipynb>`_

All notebooks can be found in a `dedicated repository <https://github.com/feature-engine/feature-engine-examples>`_.

For more details about this and other feature selection methods check out these resources:
For more details about this and other feature selection methods check out these resources:

.. figure::  ../../images/fsml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/fsmlbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection in Machine Learning

|
|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/selection/SmartCorrelatedSelection.rst
================================================
.. _smart_correlation:

.. currentmodule:: feature_engine.selection

SmartCorrelatedSelection
========================

When dealing with datasets containing numerous features, it's common for more than two
features to exhibit correlations with each other. This correlation might manifest among
three, four, or even more features within the dataset. Consequently, determining which
features to retain and which ones to eliminate becomes a crucial consideration.

Deciding which features to retain from a correlated group involves several strategies,
such us:

1. **Model Performance**: Some features returns model with higher performance than others.

2. **Variability and Cardinality**: Features with higher variability or cardinality often provide more information about the target variable.

3. **Missing Data**: Features with less missing data are generally more reliable and informative.

4. **Correlation with Target**: Features that show a stronger correlation with the target variable are often more predictive and should be prioritized.

We can apply this selection strategies out of the box with the :class:`SmartCorrelatedSelection`.

From a group of correlated variables, the :class:`SmartCorrelatedSelection` will retain
the variable with:

- the highest variance
- the highest cardinality
- the least missing data
- the best performing model (based on a single feature)
- the strongest correlation with the target variable

The remaining features within each correlated group will be dropped.

Features with higher diversity of values (higher variance or cardinality), tend to be more
predictive, whereas features with least missing data, tend to be more useful.

Alternatively, directly training a model using each feature within the group and retaining
the one that trains the best performing model, directly evaluates the influence of the
feature on the target.

Procedure
---------

:class:`SmartCorrelatedSelection` first finds correlated feature groups using any
correlation method supported by `pandas.corr()`, or a user defined function that returns
a value between -1 and 1.

Then, from each group of correlated features, it will try and identify the best candidate
based on the above criteria.

If the criteria is based on model performance, :class:`SmartCorrelatedSelection` will
train a single feature machine learning model, using each one of the features in a correlated
group, calculate the model's performance, and select the feature that returned the highest
performing model. In simpler words, it trains single feature models, and retains the
feature of the highest performing model.

If the criteria is based on variance or cardinality, :class:`SmartCorrelatedSelection` will
determine these attributes for each feature in the group and retain that one with the highest.
Note however, that variability is dominated by the variable's scale. Hence, **variables with
larger scales will dominate the selection procedure**, unless you have a scaled dataset.

If the criteria is based on missing data, :class:`SmartCorrelatedSelection` will determine the
number of NA in each feature from the correlated group and keep the one with less NA.

If the criteria is based on correlation with the target, :class:`SmartCorrelatedSelection`
will calculate the correlation coefficient between each feature and the target, and retain
the one with the highest absolute correlation.

Variance
~~~~~~~~

Let's see how to use :class:`SmartCorrelatedSelection` in a toy example. Let's create a
toy dataframe with 4 correlated features:

.. code:: python

    import pandas as pd
    from sklearn.datasets import make_classification
    from feature_engine.selection import SmartCorrelatedSelection

    # make dataframe with some correlated variables
    def make_data():
        X, y = make_classification(n_samples=1000,
                                   n_features=12,
                                   n_redundant=4,
                                   n_clusters_per_class=1,
                                   weights=[0.50],
                                   class_sep=2,
                                   random_state=1)

        # transform arrays into pandas df and series
        colnames = ['var_'+str(i) for i in range(12)]
        X = pd.DataFrame(X, columns=colnames)
        return X

    X = make_data()

Now, we set up :class:`SmartCorrelatedSelection` to find features groups which (absolute)
correlation coefficient is >0.8. From these groups, we want to retain the feature with
highest variance:

.. code:: python

    # set up the selector
    tr = SmartCorrelatedSelection(
        variables=None,
        method="pearson",
        threshold=0.8,
        missing_values="raise",
        selection_method="variance",
        estimator=None,
    )

With `fit()`, the transformer finds the correlated variables and selects the ones to keep.
With `transform()`, it drops the remaining features in the correlated group from the dataset:

.. code:: python

    Xt = tr.fit_transform(X)

The correlated feature groups are stored in the one of the transformer's attributes:

.. code:: python

    tr.correlated_feature_sets_

In the first group, 4 features are correlated to at least one of them. In the second group,
2 features are correlated.

.. code:: python

    [{'var_4', 'var_6', 'var_7', 'var_9'}, {'var_0', 'var_8'}]

:class:`SmartCorrelatedSelection` picks a feature, and then determines the correlation
of other features in the dataframe to it. Hence, all features in a group will be correlated
to this one feature, but they may or may not be correlated to the other features within
the group, because correlation is not transitive.

This feature that was used in the assessment, was either the one with the higher variance,
higher cardinality or smaller number of missing data. Or, if model performance was selected,
it was the one that came first in alphabetic order.

We can identify from each group which feature will be retained and which ones removed
by inspecting the following attribute:

.. code:: python

    tr.correlated_feature_dict_

In the dictionary below we see that from the first correlated group, `var_7` is a key,
hence it will be retained, whereas variables 4, 6 and  9 are values, which means that
they are correlated to `var_7` and will therefore be removed.

Because we are selecting features based on variability, `var_7` has the higher variability
from the group.

.. code:: python

   {'var_7': {'var_4', 'var_6', 'var_9'}, 'var_8': {'var_0'}}

Similarly, `var_8` is a key and will be retained, whereas the `var_0` is a value, which
means that it was found correlated to `var_8` and will therefore be removed.

We can corroborate that, for example, `var_7` had the highest variability as follows:

.. code:: python

    X[list(tr.correlated_feature_sets_[0])].std()

That command returns the following output, where we see that the variability of `var_7`
is the highest:

.. code:: python

    var_4    1.810273
    var_7    2.159634
    var_9    1.764249
    var_6    2.032947
    dtype: float64

The features that will be removed from the dataset are stored in the following attribute:

..  code:: python

    tr.features_to_drop_

.. code:: python

   ['var_6', 'var_4', 'var_9', 'var_0']

If we now go ahead and print the transformed data, we see that the correlated features
have been removed.

.. code:: python

    print(Xt.head())

.. code:: python

          var_1     var_2     var_3     var_5     var_7     var_8    var_10  \
    0 -2.376400 -0.247208  1.210290  0.091527 -2.230170  2.070483  2.070526
    1  1.969326 -0.126894  0.034598 -0.186802 -1.447490  2.421477  1.184820
    2  1.499174  0.334123 -2.233844 -0.313881 -2.240741  2.263546 -0.066448
    3  0.075341  1.627132  0.943132 -0.468041 -3.534861  2.792500  0.713558
    4  0.372213  0.338141  0.951526  0.729005 -2.053965  2.186741  0.398790

         var_11
    0 -1.989335
    1 -1.309524
    2 -0.852703
    3  0.484649
    4 -0.186530

Performance
~~~~~~~~~~~

Let's now select the feature that returns a machine learning model with the highest
performance, from each group. We'll use a decision tree.

We start by creating a toy dataframe:

.. code:: python

    import pandas as pd
    from sklearn.datasets import make_classification
    from sklearn.tree import DecisionTreeClassifier
    from feature_engine.selection import SmartCorrelatedSelection

    # make dataframe with some correlated variables
    def make_data():
        X, y = make_classification(n_samples=1000,
                                   n_features=12,
                                   n_redundant=4,
                                   n_clusters_per_class=1,
                                   weights=[0.50],
                                   class_sep=2,
                                   random_state=1)

        # transform arrays into pandas df and series
        colnames = ['var_'+str(i) for i in range(12)]
        X = pd.DataFrame(X, columns=colnames)
        return X, y

    X, y = make_data()

Let's now set up the selector:

.. code:: python

    tr = SmartCorrelatedSelection(
        variables=None,
        method="pearson",
        threshold=0.8,
        missing_values="raise",
        selection_method="model_performance",
        estimator=DecisionTreeClassifier(random_state=1),
        scoring='roc_auc',
        cv=3,
    )

Next, we fit the selector to the data. Here, as we are training a model, we also need to
pass the target variable:

.. code:: python

    Xt = tr.fit_transform(X, y)

Let's explore the correlated feature groups:

.. code:: python

    tr.correlated_feature_sets_

We see that the groups of correlated features are slightly different, because in this
cases, the features were assessed in alphabetical order, whereas when we used the variance
the features we sorted based on their standard deviation for the assessment.

.. code:: python

    [{'var_0', 'var_8'}, {'var_4', 'var_6', 'var_7', 'var_9'}]

We can find the feature that will be retained as the key in the following attribute:

.. code:: python

    tr.correlated_feature_dict_

The variables `var_0` and `var_7` will be retained, and the remaining ones will be dropped.

.. code:: python

    {'var_0': {'var_8'}, 'var_7': {'var_4', 'var_6', 'var_9'}}

We find the variables that will be dropped in the following attribute:

.. code:: python

    tr.features_to_drop_

.. code:: python

    ['var_8', 'var_4', 'var_6', 'var_9']

And now we can print the resulting dataframe after the transformation:

.. code:: python

    print(Xt.head())

.. code:: python

          var_0     var_1     var_2     var_3     var_5     var_7    var_10  \
    0  1.471061 -2.376400 -0.247208  1.210290  0.091527 -2.230170  2.070526
    1  1.819196  1.969326 -0.126894  0.034598 -0.186802 -1.447490  1.184820
    2  1.625024  1.499174  0.334123 -2.233844 -0.313881 -2.240741 -0.066448
    3  1.939212  0.075341  1.627132  0.943132 -0.468041 -3.534861  0.713558
    4  1.579307  0.372213  0.338141  0.951526  0.729005 -2.053965  0.398790

         var_11
    0 -1.989335
    1 -1.309524
    2 -0.852703
    3  0.484649
    4 -0.186530

Let's examine other attributes that may be useful. Like with any Scikit-learn transformer
we can obtain the names of the features in the resulting dataframe as follows:

.. code:: python

    tr.get_feature_names_out()

.. code:: python

    ['var_0', 'var_1', 'var_2', 'var_3', 'var_5', 'var_7', 'var_10', 'var_11']

We also find the `get_support` method that flags the features that will be retained from
the dataframe:

.. code:: python

    tr.get_support()

.. code:: python

    [True, True, True, True, False, True, False, True, False, False, True, True]

Correlation with Target
~~~~~~~~~~~~~~~~~~~~~~~

Finally, let's select the feature that shows the strongest correlation with the target
variable from each group.

Let's create another toy dataframe with 4 features and a target variable:

.. code:: python

    import pandas as pd
    from sklearn.datasets import make_classification
    from feature_engine.selection import SmartCorrelatedSelection

    # make dataframe with some correlated variables
    def make_data():
        X, y = make_classification(n_samples=1000,
                                   n_features=12,
                                   n_redundant=4,
                                   n_clusters_per_class=1,
                                   weights=[0.50],
                                   class_sep=2,
                                   random_state=1)

        # transform arrays into pandas df and series
        colnames = ['var_'+str(i) for i in range(12)]
        X = pd.DataFrame(X, columns=colnames)
        y = pd.Series(y)
        return X, y

    X, y = make_data()

Now, we set up :class:`SmartCorrelatedSelection` to find feature groups with absolute
correlation coefficient >0.8 and retain the feature with the strongest correlation with
the target variable:

.. code:: python

    # set up the selector
    tr = SmartCorrelatedSelection(
        variables=None,
        method="pearson",
        threshold=0.8,
        missing_values="raise",
        selection_method="corr_with_target",
        estimator=None,
    )

With `fit()`, the transformer finds the correlated variables and selects the ones to
keep. With `transform()`, it drops the remaining features in the correlated group from
the dataset:

.. code:: python

    Xt = tr.fit_transform(X, y)

Let's explore the correlated feature groups:

.. code:: python

    tr.correlated_feature_sets_

Similarly to the previous examples, we see that the groups of correlated features are
slightly different. Here, what happened is that the features were ordered first based 
on their absolute correlation with the target, before carrying on the search of correlation
with the other features in the dataset. Like this, the first feature of the group is retained, 
which is the one with highest correlation with the target.

.. code:: python

    [{'var_4', 'var_6', 'var_7', 'var_9'}, {'var_0', 'var_8'}]

We can find the features that will be retained as keys in the following dictionary:

.. code:: python

    tr.correlated_feature_dict_

The variables `var_7` and `var_0` will be retained, and the remaining ones will be dropped.

.. code:: python

    {'var_7': {'var_4', 'var_6', 'var_9'}, 'var_0': {'var_8'}}

We can check the correlation of the features with the target variable as follows:

.. code:: python

    print(X.corrwith(y).abs())

.. code:: python

    var_0     0.270913
    var_1     0.088358
    var_2     0.038257
    var_3     0.027320
    var_4     0.838361
    var_5     0.028020
    var_6     0.834925
    var_7     0.916045
    var_8     0.007724
    var_9     0.797149
    var_10    0.006742
    var_11    0.023710
    dtype: float64

We notice that indeed the the `var_0` has a much higher correlation with the target than
the `var_8`, which is why `var_0` was retained:

.. code:: python

    print(X.corrwith(y).abs().loc[['var_0', 'var_8']])

.. code:: python

    var_0    0.270913
    var_8    0.007724
    dtype: float64

The variables that will be dropped are available in the following attribute:

.. code:: python

    tr.features_to_drop_

.. code:: python

    ['var_4', 'var_6', 'var_9', 'var_8']

And now we can print the resulting dataframe after the transformation:

.. code:: python

    print(Xt.head())

.. code:: python

        var_0     var_1     var_2     var_3     var_5     var_7    var_10  \
    0  1.471061 -2.376400 -0.247208  1.210290  0.091527 -2.230170  2.070526
    1  1.819196  1.969326 -0.126894  0.034598 -0.186802 -1.447490  1.184820
    2  1.625024  1.499174  0.334123 -2.233844 -0.313881 -2.240741 -0.066448
    3  1.939212  0.075341  1.627132  0.943132 -0.468041 -3.534861  0.713558
    4  1.579307  0.372213  0.338141  0.951526  0.729005 -2.053965  0.398790

        var_11
    0 -1.989335
    1 -1.309524
    2 -0.852703
    3  0.484649
    4 -0.186530

We can also obtain the names of the features in the resulting dataframe as follows:

.. code:: python

    tr.get_feature_names_out()

.. code:: python

    ['var_0', 'var_1', 'var_2', 'var_3', 'var_5', 'var_7', 'var_10', 'var_11']

We also find the `get_support` method that flags the features that will be retained from
the dataframe:

.. code:: python

    tr.get_support()

.. code:: python

    [True, True, True, True, False, True, False, True, False, False, True, True]

And that's it!

Additional resources
--------------------

In this notebook, we show how to use :class:`SmartCorrelatedSelection` with a different
relation metric:

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/selection/Smart-Correlation-Selection.ipynb>`_

All notebooks can be found in a `dedicated repository <https://github.com/feature-engine/feature-engine-examples>`_.

For more details about this and other feature selection methods check out these resources:

.. figure::  ../../images/fsml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/fsmlbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Feature Selection in Machine Learning

|
|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/timeseries/index.rst
================================================
.. -*- mode: rst -*-
.. _timeseries:

.. currentmodule:: feature_engine.timeseries

Time Series Features
====================

Feature-engine's time series transformers create features from time series data.

.. toctree::
   :maxdepth: 1

   forecasting/index

|
|
|

================================================
FILE: docs/user_guide/timeseries/forecasting/ExpandingWindowFeatures.rst
================================================
.. _expanding_window_features:

.. currentmodule:: feature_engine.timeseries.forecasting

ExpandingWindowFeatures
=======================

Window features are variables created by performing mathematical operations over a window
of past data in a time series.

**Rolling window features** are created by performing aggregations over a **sliding partition**
(or moving window) of past data points of the time series data. The window size in this case is constant.

**Expanding window features** are created by performing aggregations over an **expanding partition**
of past values of the time series. The window size increases as we approach more recent values.

An example of an expanding window feature is the mean value of all the data points prior
to the current row / value. The maximum value of all the rows prior to the current row is
another expanding window feature.

For an expanding window feature to be suitable for forecasting, the window can span
from the start of the data up to, but not including, the first point of forecast.

Expanding window features can be used for forecasting by using traditional machine learning
models, like linear regression.

Expanding window features with pandas
--------------------------------------

In Python, we can create expanding window features by utilizing pandas method `expanding`.
For example, by executing:

.. code:: python

    X[["var_1", "var_2"].expanding(min_periods=3).agg(["max", "mean"])

With the previous command, we create 2 window features for each variable, `var_1` and
`var_2`, by taking the maximum and average value of all observations up to (and including)
a certain row.

If we want to use those features for forecasting using traditional machine learning algorithms,
we would also shift the window forward with pandas method `shift`:

.. code:: python

    X[["var_1", "var_2"].expanding(min_periods=3).agg(["max", "mean"]).shift(period=1)

Expanding window features with Feature-engine
----------------------------------------------

:class:`ExpandingWindowFeatures` adds expanding window features to the dataframe.

Window features are the result of applying an aggregation operation (e.g., mean,
min, max, etc.) to a variable over a window of past data.

When forecasting the future values of a variable, the past values of that variable are
likely to be predictive. To capitalize on the past values of a variable, we can simply
lag features with :class:`LagFeatures`. We can also create features that summarise the
past values into a single quantity utilising :class:`ExpandingWindowFeatures`.

:class:`ExpandingWindowFeatures` works on top of `pandas.expanding`, `pandas.aggregate`
and `pandas.shift`.

:class:`ExpandingWindowFeatures` uses `pandas.aggregate` to perform the mathematical
operations over the expanding window. Therefore, you can use any operation supported
by pandas. For supported aggregation functions, see Expanding Window
`Functions <https://pandas.pydata.org/docs/reference/window.html>`_.

With `pandas.shift`, :class:`ExpandingWindowFeatures` lags the result of the expanding
window operation. This is useful to ensure that only the information known at predict
time is used to compute the window feature. So if at predict time we only know
the value of a feature at the previous time period and before that, then we should lag the
the window feature by 1 period. If at predict time we only know the value of a feature
from 2 weeks ago and before that, then we should lag the window feature column by 2 weeks.
:class:`ExpandingWindowFeatures` uses a default lag of one period.

:class:`ExpandingWindowFeatures` will add the new variables with a representative
name to the original dataframe. It also has the methods `fit()` and `transform()`
that make it compatible with the Scikit-learn's `Pipeline` and cross-validation
functions.

Note that, in the current implementation, :class:`ExpandingWindowFeatures` only works with
dataframes whose index, containing the time series timestamp, contains unique values and no NaN.

Examples
--------

Let's create a toy dataset to demonstrate the functionality of :class:`ExpandingWindowFeatures`.
The dataframe contains 3 numerical variables, a categorical variable, and a datetime
index.

.. code:: python

    import pandas as pd

    X = {"ambient_temp": [31.31, 31.51, 32.15, 32.39, 32.62, 32.5, 32.52, 32.68],
         "module_temp": [49.18, 49.84, 52.35, 50.63, 49.61, 47.01, 46.67, 47.52],
         "irradiation": [0.51, 0.79, 0.65, 0.76, 0.42, 0.49, 0.57, 0.56],
         "color": ["green"] * 4 + ["blue"] * 4,
         }

    X = pd.DataFrame(X)
    X.index = pd.date_range("2020-05-15 12:00:00", periods=8, freq="15min")

    y = pd.Series([1,2,3,4,5,6,7,8])
    y.index = X.index

    X.head()

Below we see the output of our toy dataframe:

.. code:: python

                         ambient_temp  module_temp  irradiation  color
    2020-05-15 12:00:00         31.31        49.18         0.51  green
    2020-05-15 12:15:00         31.51        49.84         0.79  green
    2020-05-15 12:30:00         32.15        52.35         0.65  green
    2020-05-15 12:45:00         32.39        50.63         0.76  green
    2020-05-15 13:00:00         32.62        49.61         0.42   blue

Let's now print out the target:

.. code:: python

    y

Below we see the target variable:

.. code:: python

    2020-05-15 12:00:00    1
    2020-05-15 12:15:00    2
    2020-05-15 12:30:00    3
    2020-05-15 12:45:00    4
    2020-05-15 13:00:00    5
    2020-05-15 13:15:00    6
    2020-05-15 13:30:00    7
    2020-05-15 13:45:00    8
    Freq: 15min, dtype: int64

Now we will create expanding window features from the numerical variables. In `functions`,
we indicate all the operations that we want to perform over those windows. In
our example below, we want to calculate the mean and the standard deviation of the
data within those windows and also find the maximum value within the windows.

.. code:: python

    from feature_engine.timeseries.forecasting import ExpandingWindowFeatures

    win_f = ExpandingWindowFeatures(functions=["mean", "max", "std"])

    X_tr = win_f.fit_transform(X)

    X_tr.head()

We can find the window features on the right side of the dataframe.

.. code:: python

                         ambient_temp  module_temp  irradiation  color  \
    2020-05-15 12:00:00         31.31        49.18         0.51  green
    2020-05-15 12:15:00         31.51        49.84         0.79  green
    2020-05-15 12:30:00         32.15        52.35         0.65  green
    2020-05-15 12:45:00         32.39        50.63         0.76  green
    2020-05-15 13:00:00         32.62        49.61         0.42   blue

                         ambient_temp_expanding_mean  ambient_temp_expanding_max  \
    2020-05-15 12:00:00                          NaN                         NaN
    2020-05-15 12:15:00                    31.310000                       31.31
    2020-05-15 12:30:00                    31.410000                       31.51
    2020-05-15 12:45:00                    31.656667                       32.15
    2020-05-15 13:00:00                    31.840000                       32.39

                         ambient_temp_expanding_std  module_temp_expanding_mean  \
    2020-05-15 12:00:00                         NaN                         NaN
    2020-05-15 12:15:00                         NaN                   49.180000
    2020-05-15 12:30:00                    0.141421                   49.510000
    2020-05-15 12:45:00                    0.438786                   50.456667
    2020-05-15 13:00:00                    0.512640                   50.500000

                         module_temp_expanding_max  module_temp_expanding_std  \
    2020-05-15 12:00:00                        NaN                        NaN
    2020-05-15 12:15:00                      49.18                        NaN
    2020-05-15 12:30:00                      49.84                   0.466690
    2020-05-15 12:45:00                      52.35                   1.672553
    2020-05-15 13:00:00                      52.35                   1.368381

                         irradiation_expanding_mean  irradiation_expanding_max  \
    2020-05-15 12:00:00                         NaN                        NaN
    2020-05-15 12:15:00                      0.5100                       0.51
    2020-05-15 12:30:00                      0.6500                       0.79
    2020-05-15 12:45:00                      0.6500                       0.79
    2020-05-15 13:00:00                      0.6775                       0.79

                         irradiation_expanding_std
    2020-05-15 12:00:00                        NaN
    2020-05-15 12:15:00                        NaN
    2020-05-15 12:30:00                   0.197990
    2020-05-15 12:45:00                   0.140000
    2020-05-15 13:00:00                   0.126853

The variables used as input for the window features are stored in the `variables_`
attribute of the :class:`ExpandingWindowFeatures`.

.. code:: python

    win_f.variables_

.. code:: python

    ['ambient_temp', 'module_temp', 'irradiation']

We can obtain the names of the variables in the returned dataframe using the
`get_feature_names_out()` method:

.. code:: python

    win_f.get_feature_names_out()

.. code:: python

    ['ambient_temp',
     'module_temp',
     'irradiation',
     'color',
     'ambient_temp_expanding_mean',
     'ambient_temp_expanding_max',
     'ambient_temp_expanding_std',
     'module_temp_expanding_mean',
     'module_temp_expanding_max',
     'module_temp_expanding_std',
     'irradiation_expanding_mean',
     'irradiation_expanding_max',
     'irradiation_expanding_std']

Dropping rows with nan
~~~~~~~~~~~~~~~~~~~~~~

When we create window features using expanding windows, we may introduce nan values for
those data points where there isn't enough data in the past to create the windows. We
can automatically drop the rows with nan values in the window features both in the train
set and in the target variable as follows:

.. code:: python

    win_f = ExpandingWindowFeatures(
        functions=["mean", "max", "std"],
        drop_na=True,
    )

    win_f.fit(X)

    X_tr, y_tr = win_f.transform_x_y(X, y)

    X.shape, y.shape, X_tr.shape, y_tr.shape

We see that the resulting dataframe contains less rows than the original dataframe:

.. code:: python

    (8, 4), (8,), (6, 13), (6,))

Imputing rows with nan
~~~~~~~~~~~~~~~~~~~~~~

If instead of removing the row with nan in the expanding window features, we want to impute those
values, we can do so with any of Feature-engine's imputers. Here, we will replace nan with
the median value of the resulting window features, using the `MeanMedianImputer` within
a pipeline:

.. code:: python

    from feature_engine.imputation import MeanMedianImputer
    from feature_engine.pipeline import Pipeline

    win_f = ExpandingWindowFeatures(functions=["mean", "std"])

    pipe = Pipeline([
        ("windows", win_f),
        ("imputer", MeanMedianImputer(imputation_method="median"))
    ])

    X_tr = pipe.fit_transform(X, y)

    print(X_tr.head())

We see the resulting dataframe, where the nan values were replaced with the median:

.. code:: python

                         ambient_temp  module_temp  irradiation  color  \
    2020-05-15 12:00:00         31.31        49.18         0.51  green
    2020-05-15 12:15:00         31.51        49.84         0.79  green
    2020-05-15 12:30:00         32.15        52.35         0.65  green
    2020-05-15 12:45:00         32.39        50.63         0.76  green
    2020-05-15 13:00:00         32.62        49.61         0.42   blue

                         ambient_temp_expanding_mean  ambient_temp_expanding_std  \
    2020-05-15 12:00:00                    31.840000                    0.518740
    2020-05-15 12:15:00                    31.310000                    0.518740
    2020-05-15 12:30:00                    31.410000                    0.141421
    2020-05-15 12:45:00                    31.656667                    0.438786
    2020-05-15 13:00:00                    31.840000                    0.512640

                         module_temp_expanding_mean  module_temp_expanding_std  \
    2020-05-15 12:00:00                   49.770000                   1.520467
    2020-05-15 12:15:00                   49.180000                   1.520467
    2020-05-15 12:30:00                   49.510000                   0.466690
    2020-05-15 12:45:00                   50.456667                   1.672553
    2020-05-15 13:00:00                   50.500000                   1.368381

                         irradiation_expanding_mean  irradiation_expanding_std
    2020-05-15 12:00:00                      0.6260                   0.146424
    2020-05-15 12:15:00                      0.5100                   0.146424
    2020-05-15 12:30:00                      0.6500                   0.197990
    2020-05-15 12:45:00                      0.6500                   0.140000
    2020-05-15 13:00:00                      0.6775                   0.126853

Working with pandas series
~~~~~~~~~~~~~~~~~~~~~~~~~~

If your time series is a pandas Series instead of a pandas Dataframe, you need to
transform it into a dataframe before using :class:`ExpandingWindowFeatures`.

The following is a pandas Series:

.. code:: python

    X['ambient_temp']

.. code:: python

    2020-05-15 12:00:00    31.31
    2020-05-15 12:15:00    31.51
    2020-05-15 12:30:00    32.15
    2020-05-15 12:45:00    32.39
    2020-05-15 13:00:00    32.62
    2020-05-15 13:15:00    32.50
    2020-05-15 13:30:00    32.52
    2020-05-15 13:45:00    32.68
    Freq: 15T, Name: ambient_temp, dtype: float64

We can use :class:`ExpandingWindowFeatures` to create, for example, 2 new expanding window
features by finding the mean and maximum value of a pandas Series if we convert
it to a pandas Dataframe using the method `to_frame()`:

.. code:: python

    win_f = ExpandingWindowFeatures(functions=["mean", "max"])

    X_tr = win_f.fit_transform(X['ambient_temp'].to_frame())

    X_tr.head()

.. code:: python

                         ambient_temp  ambient_temp_expanding_mean  \
    2020-05-15 12:00:00         31.31                          NaN
    2020-05-15 12:15:00         31.51                    31.310000
    2020-05-15 12:30:00         32.15                    31.410000
    2020-05-15 12:45:00         32.39                    31.656667
    2020-05-15 13:00:00         32.62                    31.840000

                         ambient_temp_expanding_max
    2020-05-15 12:00:00                         NaN
    2020-05-15 12:15:00                       31.31
    2020-05-15 12:30:00                       31.51
    2020-05-15 12:45:00                       32.15
    2020-05-15 13:00:00                       32.39

And if we do not want the original values of time series in the returned dataframe, we
just need to remember to drop the original series after the transformation:

.. code:: python

    win_f = ExpandingWindowFeatures(
        functions=["mean", "max"],
        drop_original=True,
    )

    X_tr = win_f.fit_transform(X['ambient_temp'].to_frame())

    X_tr.head()

.. code:: python

                         ambient_temp_expanding_mean  ambient_temp_expanding_max
    2020-05-15 12:00:00                          NaN                         NaN
    2020-05-15 12:15:00                    31.310000                       31.31
    2020-05-15 12:30:00                    31.410000                       31.51
    2020-05-15 12:45:00                    31.656667                       32.15
    2020-05-15 13:00:00                    31.840000                       32.39

Getting the name of the new features
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We can easily obtain the name of the original and new variables with the method
`get_feature_names_out`.

.. code:: python

    win_f = ExpandingWindowFeatures()

    win_f.fit(X)

    win_f.get_feature_names_out()

.. code:: python

    ['ambient_temp',
     'module_temp',
     'irradiation',
     'color',
     'ambient_temp_expanding_mean',
     'module_temp_expanding_mean',
     'irradiation_expanding_mean']

See also
--------

Check out the additional transformers to create rolling window features
(:class:`WindowFeatures`) or lag features, by lagging past values of the time
series data (:class:`LagFeatures`).

Tutorials and courses
---------------------

For tutorials about this and other feature engineering methods for time series forecasting
check out our online courses:

.. figure::  ../../../images/fetsf.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Time Series Forecasting

.. figure::  ../../../images/fwml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Forecasting with Machine Learning

|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

Our courses are suitable for beginners and more advanced data scientists looking to
forecast time series using traditional machine learning models, like linear regression
or gradient boosting machines.

By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/timeseries/forecasting/index.rst
================================================
.. -*- mode: rst -*-
.. _forecasting:

.. currentmodule:: feature_engine.timeseries.forecasting

Forecasting Features
====================
Machine learning is becoming increasingly popular for time series forecasting because of its ability
to model complex relationships and handle large datasets. While traditional forecasting methods like
moving averages, ARIMA (autoregressive integrated moving averages), exponential smoothing, and others,
are effective in identifying trend and seasonality, machine learning algorithms like linear regression,
random forests, or gradient boosted machines, can model complex patterns and incorporate exogenous
features, thereby resulting in accurate predictions.

However, to use machine learning to forecast time series data, we need to extract relevant features
and convert the data into a tabular format, so that it can be framed as a regression problem.

Feature-engine's time series forecasting transformers give us the ability to extract and generate
useful features from time series to use for forecasting. They are built on top of Python libraries
such as Pandas and offer an interface to extract various features from temporal data simultaneously.

Time series forecasting involves learning from historical data observations to predict future values.
Feature-engine's offers various transformers for creating features from the past values.

Lag and Window Features
-----------------------

Trend and seasonality can be captured using lag and window features. In Feature-engine, we have
three transformers to extract these features.

Lag features
~~~~~~~~~~~~~

A lag feature at a given time step represents the value of the time series from a prior time step.
Feature engine's :class:`LagFeatures` implements lag features. These are straightforward to
compute and widely used in time series forecasting tasks. For instance, in sales forecasting,
you might include the sales from the previous day, week,
or even year to predict the sales for a given day. Lag features are also the foundation
to many autoregressive models like moving averages and ARIMA.

While lag features are particularly effective for capturing seasonality, they can also model
short-term trends. Seasonality is well captured by lag features because they carry data at
regular intervals from the past - like daily, weekly, or yearly cycles. For example, a 7-day
lag can capture weekly seasonality, such as sales spikes over the weekend; and a 365-day lag
can capture yearly seasonality, like Christmas holiday sales. Hence a machine learning model
can understand a lot of the time series patterns by using lag features as input.

However, lag features may not capture long-term trends unless they are combined with other features,
such as rolling window or expanding window features.

Window features
~~~~~~~~~~~~~~~

:class:`WindowFeatures`, also known as rolling window features, are used to summarize past behavior over a
fixed time period by computing statistics like mean, standard deviation, min, max, sum, etc. on the
time series variable. For instance, in sales forecasting, calculating the "mean" sales value of the
previous 4 weeks of data is a window feature.

Window features smooth out short-term fluctuations in the data allowing the model to capture trend.
This is somewhat similar to moving averages in traditional time series analysis.

In time series forecasting, we can aggregate data across multiple window features by applying
various mathematical operations across different sized time windows. Moreover, we can also combine
window features with lag features, for instance, by computing rolling statistics on lagged values,
which adds more depth to the feature set. This approach helps us generate a large number of features
and can capture both short-term and long-term patterns in historical data.

To determine which window sizes or lag combinations are useful, you can either perform time series
analyses to identify relevant window sizes, or you can use Feature-engine's
:ref:`feature selection transformers <selection_user_guide>`.
These are used to drop subsets of variables that are uninformative and have low predictive power
which in turn improves model performance. We can set aside validation data while training the
forecasting model to test different configurations and finally stick with the values that result
in minimal forecasting error.

Expanding Window Features
~~~~~~~~~~~~~~~~~~~~~~~~~~

:class:`ExpandingWindowFeatures` are similar to rolling window features but instead
of a fixed-size window,
the window expands incrementally over time, starting from the beginning of the time series.
With each new time step, the window includes all prior observations, retaining a cumulative
summary of the data.

For example, "sum" of all sales values up to a given time step provides insights into the
ongoing trend i.e., whether the sales are increasing, decreasing or don't change with time.
This helps to capture long-term trends in the data and also model cumulative effects over time,
thus improving forecast accuracy.

Expanding window features are helpful in various data science use cases like demand forecasting
for supply chain optimization, stock price prediction etc.

Just like rolling window features, expanding window features can be used with various statistical
methods like mean, sum, standard deviation, min, max, among others. Unlike rolling window, we
don't need to specify a window size as the window is expanded automatically at each time step.

Datetime Features
-----------------

In addition to lag and window features, Feature-engine also offers transformers to extract
other attributes from the time series such as day_of_week, day_of_month, quarter, year, hour,
minute etc. directly from the datetime variable using :obj:`DatetimeFeatures <feature_engine.datetime.DatetimeFeatures>`.
These features are important to identify seasonal patterns, daily trends, especially when certain
time periods have strong correlation with the target variable.

Cyclical Features
------------------

In time series data, certain time-based attributes, such as month_of_year, day_of_week, etc. are
inherently cyclical. For example, after 12th month, the calendar resets to 1st month, and after
the 7th weekday, the calendar resets to 1st weekday. To inform the model of this periodic structure,
Feature-engine allows us to capture this behavior through the
:obj:`CyclicalFeatures <feature_engine.creation.CyclicalFeatures>` transformer.

CyclicalFeatures represents datetime variables using the sine and cosine transformation,
allowing the model to understand the continuity of time-based cycles. This approach overcomes
the limitations of ordinal or label encoding, where the end and beginning of a cycle
(e.g., 12th month to 1st month) would appear distant in the feature space while in reality
they are closer. These features can further enhance model's ability to capture seasonal and
repeating patterns in the timeseries.

Tutorials and courses
---------------------

For tutorials about this and other feature engineering methods for time series forecasting
check out our online courses:

.. figure::  ../../../images/fetsf.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Time Series Forecasting

.. figure::  ../../../images/fwml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Forecasting with Machine Learning

|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

Our courses are suitable for beginners and more advanced data scientists looking to
forecast time series using traditional machine learning models, like linear regression
or gradient boosting machines.

By purchasing them you are supporting Sole, the main developer of Feature-engine.

Forecasting Features Transformers
---------------------------------

.. toctree::
   :maxdepth: 1

   LagFeatures
   WindowFeatures
   ExpandingWindowFeatures

|
|
|

================================================
FILE: docs/user_guide/timeseries/forecasting/LagFeatures.rst
================================================
.. _lag_features:

.. currentmodule:: feature_engine.timeseries.forecasting

LagFeatures
===========

Lag features are commonly used in data science to forecast time series with traditional

machine learning models, like linear regression or random forests. A lag feature is a
feature with information about a prior time step of the time series.

When forecasting the future values of a variable, the past values of that same variable
are likely to be predictive. Past values of other predictive features can also be useful
for our forecast. Thus, in forecasting, it is common practice to create lag features from
time series data and use them as input to machine learning algorithms or forecasting
models.

What is a lag feature?
----------------------

A lag feature is the value of the time series **k** period(s) in the past, where **k** is
the lag and is to be set by the user.  For example, a lag of 1 is a feature that contains
the previous time point value of the time series. A lag of 3 contains the value 3 time
points before, and so on. By varying k, we can create features with multiple lags.

In Python, we can create lag features by using the pandas method `shift`. For example, by
executing `X[my_variable].shift(freq=”1H”, axis=0)`, we create a new feature consisting of
lagged values of `my_variable` by 1 hour.

Feature-engine’s :class:`LagFeatures` automates the creation of lag features from multiple
variables and by using multiple lags. It uses pandas `shift` under the hood, and automatically
concatenates the new features to the input dataframe.

Automating lag feature creation
-------------------------------

There are 2 ways in which we can indicate the lag k using :class:`LagFeatures`. Just like
with pandas `shift`, we can indicate the lag using the parameter `periods`. This parameter
takes integers that indicate the number of rows forward that the features will be lagged.

Alternatively, we can use the parameter `freq`, which takes a string with the period and
frequency, and lags features based on the datetime index. For example, if we pass `freq="1D"`,
the values of the features will be moved 1 day forward.

The :class:`LagFeatures` transformer works very similarly to `pandas.shift`, but unlike
`pandas.shift` we can indicate the lag using either `periods` or `freq` but not both at the
same time. Also, unlike `pandas.shift`, we can only lag features forward.

:class:`LagFeatures` has several advantages over `pandas.shift`:

- First, it can create features with multiple values of k at the same time.
- Second, it adds the features with a name to the original dataframe.
- Third, it has the methods `fit()` and `transform()` that make it compatible with the Scikit-learn's `Pipeline` and cross-validation functions.

Note that, in the current implementation, :class:`LagFeatures` only works with dataframes whose index,
containing the time series timestamp, contains unique values and no NaN.

Examples
--------

Let's create a toy dataset to show how to add lag features with :class:`LagFeatures`.
The dataframe contains 3 numerical variables, a categorical variable, and a datetime
index. We also create an arbitrary target.

.. code:: python

    import pandas as pd

    X = {"ambient_temp": [31.31, 31.51, 32.15, 32.39, 32.62, 32.5, 32.52, 32.68],
         "module_temp": [49.18, 49.84, 52.35, 50.63, 49.61, 47.01, 46.67, 47.52],
         "irradiation": [0.51, 0.79, 0.65, 0.76, 0.42, 0.49, 0.57, 0.56],
         "color": ["green"] * 4 + ["blue"] * 4,
         }

    X = pd.DataFrame(X)
    X.index = pd.date_range("2020-05-15 12:00:00", periods=8, freq="15min")
    y = pd.Series([1, 2, 3, 4, 5, 6, 7, 8])
    y.index = X.index

    X.head()

Below we see the output of our toy dataframe:

.. code:: python

                         ambient_temp  module_temp  irradiation  color
    2020-05-15 12:00:00         31.31        49.18         0.51  green
    2020-05-15 12:15:00         31.51        49.84         0.79  green
    2020-05-15 12:30:00         32.15        52.35         0.65  green
    2020-05-15 12:45:00         32.39        50.63         0.76  green
    2020-05-15 13:00:00         32.62        49.61         0.42   blue

And here we print and show the target variable:

.. code:: python

    y

.. code:: python

    2020-05-15 12:00:00    1
    2020-05-15 12:15:00    2
    2020-05-15 12:30:00    3
    2020-05-15 12:45:00    4
    2020-05-15 13:00:00    5
    2020-05-15 13:15:00    6
    2020-05-15 13:30:00    7
    2020-05-15 13:45:00    8
    Freq: 15min, dtype: int64

Shift a row forward
~~~~~~~~~~~~~~~~~~~

Now we will create lag features by lagging all numerical variables 1 row forward. Note
that :class:`LagFeatures` automatically finds all numerical variables.

.. code:: python

    from feature_engine.timeseries.forecasting import LagFeatures

    lag_f = LagFeatures(periods=1)

    X_tr = lag_f.fit_transform(X)

    X_tr.head()

We can find the lag features on the right side of the dataframe. Note that the values
have been shifted a row forward.

.. code:: python

                         ambient_temp  module_temp  irradiation  color  \
    2020-05-15 12:00:00         31.31        49.18         0.51  green
    2020-05-15 12:15:00         31.51        49.84         0.79  green
    2020-05-15 12:30:00         32.15        52.35         0.65  green
    2020-05-15 12:45:00         32.39        50.63         0.76  green
    2020-05-15 13:00:00         32.62        49.61         0.42   blue

                         ambient_temp_lag_1  module_temp_lag_1  irradiation_lag_1
    2020-05-15 12:00:00                 NaN                NaN                NaN
    2020-05-15 12:15:00               31.31              49.18               0.51
    2020-05-15 12:30:00               31.51              49.84               0.79
    2020-05-15 12:45:00               32.15              52.35               0.65
    2020-05-15 13:00:00               32.39              50.63               0.76

The variables to lag are stored in the `variables_` attribute of the
:class:`LagFeatures`:

.. code:: python

    lag_f.variables_

.. code:: python

    ['ambient_temp', 'module_temp', 'irradiation']

We can obtain the names of the original variables plus the lag features that are the
returned in the transformed dataframe using the `get_feature_names_out()` method:

.. code:: python

    lag_f.get_feature_names_out()

.. code:: python

    ['ambient_temp',
     'module_temp',
     'irradiation',
     'color',
     'ambient_temp_lag_1',
     'module_temp_lag_1',
     'irradiation_lag_1']

When we create lag features, we introduce nan values for the first rows of the training
data set, because there are no past values for those data points. We can impute those
nan values with an arbitrary value as follows:

.. code:: python

    lag_f = LagFeatures(periods=1, fill_value=0)

    X_tr = lag_f.fit_transform(X)

    print(X_tr.head())

We see that the nan values were replaced by 0:

.. code:: python

                         ambient_temp  module_temp  irradiation  color  \
    2020-05-15 12:00:00         31.31        49.18         0.51  green
    2020-05-15 12:15:00         31.51        49.84         0.79  green
    2020-05-15 12:30:00         32.15        52.35         0.65  green
    2020-05-15 12:45:00         32.39        50.63         0.76  green
    2020-05-15 13:00:00         32.62        49.61         0.42   blue

                         ambient_temp_lag_1  module_temp_lag_1  irradiation_lag_1
    2020-05-15 12:00:00                0.00               0.00               0.00
    2020-05-15 12:15:00               31.31              49.18               0.51
    2020-05-15 12:30:00               31.51              49.84               0.79
    2020-05-15 12:45:00               32.15              52.35               0.65
    2020-05-15 13:00:00               32.39              50.63               0.76

Alternatively, we can drop the rows with missing values in the lag features, like this:

.. code:: python

    lag_f = LagFeatures(periods=1, drop_na=True)

    X_tr = lag_f.fit_transform(X)

    print(X_tr.head())

.. code:: python

                         ambient_temp  module_temp  irradiation  color  \
    2020-05-15 12:15:00         31.51        49.84         0.79  green
    2020-05-15 12:30:00         32.15        52.35         0.65  green
    2020-05-15 12:45:00         32.39        50.63         0.76  green
    2020-05-15 13:00:00         32.62        49.61         0.42   blue
    2020-05-15 13:15:00         32.50        47.01         0.49   blue

                         ambient_temp_lag_1  module_temp_lag_1  irradiation_lag_1
    2020-05-15 12:15:00               31.31              49.18               0.51
    2020-05-15 12:30:00               31.51              49.84               0.79
    2020-05-15 12:45:00               32.15              52.35               0.65
    2020-05-15 13:00:00               32.39              50.63               0.76
    2020-05-15 13:15:00               32.62              49.61               0.42

We can also drop the rows with nan in the lag features and then adjust the target
variable like this:

.. code:: python

    X_tr, y_tr = lag_f.transform_x_y(X, y)

    X_tr.shape, y_tr.shape, X.shape, y.shape

We created a lag feature of 1, hence there is only 1 row with nan, which was removed from
train set and target:

.. code:: python

    ((7, 7), (7,), (8, 4), (8,))

Create multiple lag features
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We can create multiple lag features with one transformer by passing the lag periods in
a list.

.. code:: python

    lag_f = LagFeatures(periods=[1, 2])

    X_tr = lag_f.fit_transform(X)

    X_tr.head()

Note how multiple lag features were created for each of the numerical variables and
added at the right side of the dataframe.

.. code:: python

                         ambient_temp  module_temp  irradiation  color  \
    2020-05-15 12:00:00         31.31        49.18         0.51  green
    2020-05-15 12:15:00         31.51        49.84         0.79  green
    2020-05-15 12:30:00         32.15        52.35         0.65  green
    2020-05-15 12:45:00         32.39        50.63         0.76  green
    2020-05-15 13:00:00         32.62        49.61         0.42   blue

                         ambient_temp_lag_1  module_temp_lag_1  irradiation_lag_1  \
    2020-05-15 12:00:00                 NaN                NaN                NaN
    2020-05-15 12:15:00               31.31              49.18               0.51
    2020-05-15 12:30:00               31.51              49.84               0.79
    2020-05-15 12:45:00               32.15              52.35               0.65
    2020-05-15 13:00:00               32.39              50.63               0.76

                         ambient_temp_lag_2  module_temp_lag_2  irradiation_lag_2
    2020-05-15 12:00:00                 NaN                NaN                NaN
    2020-05-15 12:15:00                 NaN                NaN                NaN
    2020-05-15 12:30:00               31.31              49.18               0.51
    2020-05-15 12:45:00               31.51              49.84               0.79
    2020-05-15 13:00:00               32.15              52.35               0.65

We can get the names of features in the resulting dataframe as follows:

.. code:: python

    lag_f.get_feature_names_out()

.. code:: python

    ['ambient_temp',
     'module_temp',
     'irradiation',
     'color',
     'ambient_temp_lag_1',
     'module_temp_lag_1',
     'irradiation_lag_1',
     'ambient_temp_lag_2',
     'module_temp_lag_2',
     'irradiation_lag_2']

We can replace the nan introduced in the lag features as well. In this opportunity,
we'll use a string. Not that this is a suitable solution to train machine learning
algorithms, but the idea here is to showcase :class:`LagFeatures`'s functionality.

.. code:: python

    lag_f = LagFeatures(periods=[1, 2], fill_value='None')

    X_tr = lag_f.fit_transform(X)

    print(X_tr.head())

In this case, we replaced the nan in the lag features with the string None:

.. code:: python

                         ambient_temp  module_temp  irradiation  color  \
    2020-05-15 12:00:00         31.31        49.18         0.51  green
    2020-05-15 12:15:00         31.51        49.84         0.79  green
    2020-05-15 12:30:00         32.15        52.35         0.65  green
    2020-05-15 12:45:00         32.39        50.63         0.76  green
    2020-05-15 13:00:00         32.62        49.61         0.42   blue

                        ambient_temp_lag_1 module_temp_lag_1 irradiation_lag_1  \
    2020-05-15 12:00:00               None              None              None
    2020-05-15 12:15:00              31.31             49.18              0.51
    2020-05-15 12:30:00              31.51             49.84              0.79
    2020-05-15 12:45:00              32.15             52.35              0.65
    2020-05-15 13:00:00              32.39             50.63              0.76

                        ambient_temp_lag_2 module_temp_lag_2 irradiation_lag_2
    2020-05-15 12:00:00               None              None              None
    2020-05-15 12:15:00               None              None              None
    2020-05-15 12:30:00              31.31             49.18              0.51
    2020-05-15 12:45:00              31.51             49.84              0.79
    2020-05-15 13:00:00              32.15             52.35              0.65

Alternatively, we can drop rows containing nan in the lag features and then adjust the
target variable:

.. code:: python

    lag_f = LagFeatures(periods=[1, 2], drop_na=True)

    lag_f.fit(X)

    X_tr, y_tr = lag_f.transform_x_y(X, y)

    X_tr.shape, y_tr.shape, X.shape, y.shape

We see that 2 rows were dropped from train set and target:

.. code:: python

    ((6, 10), (6,), (8, 4), (8,))

Lag features based on datetime
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We can also lag features utilizing information in the timestamp of the dataframe, which
is commonly cast as datetime.

Let's for example create features by lagging 2 of the numerical variables 30 minutes
forward.

.. code:: python

    lag_f = LagFeatures(variables = ["module_temp", "irradiation"], freq="30min")

    X_tr = lag_f.fit_transform(X)

    X_tr.head()

Note that the features were moved forward 30 minutes.

.. code:: python

                         ambient_temp  module_temp  irradiation  color  \
    2020-05-15 12:00:00         31.31        49.18         0.51  green
    2020-05-15 12:15:00         31.51        49.84         0.79  green
    2020-05-15 12:30:00         32.15        52.35         0.65  green
    2020-05-15 12:45:00         32.39        50.63         0.76  green
    2020-05-15 13:00:00         32.62        49.61         0.42   blue

                         module_temp_lag_30min  irradiation_lag_30min
    2020-05-15 12:00:00                    NaN                    NaN
    2020-05-15 12:15:00                    NaN                    NaN
    2020-05-15 12:30:00                  49.18                   0.51
    2020-05-15 12:45:00                  49.84                   0.79
    2020-05-15 13:00:00                  52.35                   0.65

We can replace the nan in the lag features with a number like this:

.. code:: python

    lag_f = LagFeatures(
        variables=["module_temp", "irradiation"], freq="30min", fill_value=100)

    X_tr = lag_f.fit_transform(X)

    print(X_tr.head())

Here, we replaced nan by 100:

.. code:: python

                         ambient_temp  module_temp  irradiation  color  \
    2020-05-15 12:00:00         31.31        49.18         0.51  green
    2020-05-15 12:15:00         31.51        49.84         0.79  green
    2020-05-15 12:30:00         32.15        52.35         0.65  green
    2020-05-15 12:45:00         32.39        50.63         0.76  green
    2020-05-15 13:00:00         32.62        49.61         0.42   blue

                         module_temp_lag_30min  irradiation_lag_30min
    2020-05-15 12:00:00                 100.00                 100.00
    2020-05-15 12:15:00                 100.00                 100.00
    2020-05-15 12:30:00                  49.18                   0.51
    2020-05-15 12:45:00                  49.84                   0.79
    2020-05-15 13:00:00                  52.35                   0.65

Alternatively, we can remove the nan introduced in the lag features and adjust the target:

.. code:: python

    lag_f = LagFeatures(
        variables=["module_temp", "irradiation"], freq="30min", drop_na=True)

    lag_f.fit(X)

    X_tr, y_tr = lag_f.transform_x_y(X, y)

    X_tr.shape, y_tr.shape, X.shape, y.shape

Two rows were removed from the training data set and the target:

.. code:: python

    ((6, 6), (6,), (8, 4), (8,))

Drop variable after lagging features
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Similarly, we can lag multiple time intervals forward, but this time, let's drop the
original variable after creating the lag features.

.. code:: python

    lag_f = LagFeatures(variables="irradiation",
                        freq=["30min", "45min"],
                        drop_original=True,
                        )

    X_tr = lag_f.fit_transform(X)

    X_tr.head()

We now see the multiple lag features at the back of the dataframe, and also that the
original variable is not present in the output dataframe.

.. code:: python

                         ambient_temp  module_temp  color  irradiation_lag_30min  \
    2020-05-15 12:00:00         31.31        49.18  green                    NaN
    2020-05-15 12:15:00         31.51        49.84  green                    NaN
    2020-05-15 12:30:00         32.15        52.35  green                   0.51
    2020-05-15 12:45:00         32.39        50.63  green                   0.79
    2020-05-15 13:00:00         32.62        49.61   blue                   0.65

                         irradiation_lag_45min
    2020-05-15 12:00:00                    NaN
    2020-05-15 12:15:00                    NaN
    2020-05-15 12:30:00                    NaN
    2020-05-15 12:45:00                   0.51
    2020-05-15 13:00:00                   0.79

This is super useful in time series forecasting, because the original variable is usually
the one that we are trying to forecast, that is, the target variable. The original variables
also contain values that are **NOT** available at the time points that we are forecasting.

Working with pandas series
~~~~~~~~~~~~~~~~~~~~~~~~~~

If your time series is a pandas Series instead of a pandas Dataframe, you need to
transform it into a dataframe before using :class:`LagFeatures`.

The following is a pandas Series:

.. code:: python

    X['ambient_temp']

.. code:: python

    2020-05-15 12:00:00    31.31
    2020-05-15 12:15:00    31.51
    2020-05-15 12:30:00    32.15
    2020-05-15 12:45:00    32.39
    2020-05-15 13:00:00    32.62
    2020-05-15 13:15:00    32.50
    2020-05-15 13:30:00    32.52
    2020-05-15 13:45:00    32.68
    Freq: 15T, Name: ambient_temp, dtype: float64

We can use :class:`LagFeatures` to create, for example, 3 features by lagging the
pandas Series if we convert it to a pandas Dataframe using the method `to_frame()`:

.. code:: python

    lag_f = LagFeatures(periods=[1, 2, 3])

    X_tr = lag_f.fit_transform(X['ambient_temp'].to_frame())

    X_tr.head()

.. code:: python

                         ambient_temp  ambient_temp_lag_1  ambient_temp_lag_2  \
    2020-05-15 12:00:00         31.31                 NaN                 NaN
    2020-05-15 12:15:00         31.51               31.31                 NaN
    2020-05-15 12:30:00         32.15               31.51               31.31
    2020-05-15 12:45:00         32.39               32.15               31.51
    2020-05-15 13:00:00         32.62               32.39               32.15

                         ambient_temp_lag_3
    2020-05-15 12:00:00                 NaN
    2020-05-15 12:15:00                 NaN
    2020-05-15 12:30:00                 NaN
    2020-05-15 12:45:00               31.31
    2020-05-15 13:00:00               31.51

And if we do not want the original values of time series in the returned dataframe, we
just need to remember to drop the original series after the transformation:

.. code:: python

    lag_f = LagFeatures(periods=[1, 2, 3], drop_original=True)

    X_tr = lag_f.fit_transform(X['ambient_temp'].to_frame())

    X_tr.head()

.. code:: python

                         ambient_temp_lag_1  ambient_temp_lag_2  \
    2020-05-15 12:00:00                 NaN                 NaN
    2020-05-15 12:15:00               31.31                 NaN
    2020-05-15 12:30:00               31.51               31.31
    2020-05-15 12:45:00               32.15               31.51
    2020-05-15 13:00:00               32.39               32.15

                         ambient_temp_lag_3
    2020-05-15 12:00:00                 NaN
    2020-05-15 12:15:00                 NaN
    2020-05-15 12:30:00                 NaN
    2020-05-15 12:45:00               31.31
    2020-05-15 13:00:00               31.51

Getting the name of the lag features
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We can easily obtain the name of the original and new variables with the method
`get_feature_names_out`. By using the method with the default parameters, we obtain
all the features in the output dataframe.

.. code:: python

    lag_f = LagFeatures(periods=[1, 2])

    lag_f.fit(X)

    lag_f.get_feature_names_out()

.. code:: python

    ['ambient_temp',
     'module_temp',
     'irradiation',
     'color',
     'ambient_temp_lag_1',
     'module_temp_lag_1',
     'irradiation_lag_1',
     'ambient_temp_lag_2',
     'module_temp_lag_2',
     'irradiation_lag_2']

Determining the right lag
-------------------------

We can create multiple lag features by utilizing various lags. But how do we decide which
lag is a good lag?

There are multiple ways to do this.

We can create features by using multiple lags and then determine the best features by using
feature selection.

Alternatively, we can determine the best lag through time series analysis by evaluating
the autocorrelation or partial autocorrelation of the time series.

For tutorials on how to create lag features for forecasting, check the course
`Feature Engineering for Time Series Forecasting <https://www.trainindata.com/p/feature-engineering-for-forecasting>`_.
In the course, we also show how to detect and remove outliers from time series data, how
to use features that capture seasonality and trend, and much more.

Lags from the target vs lags from predictor variables
-----------------------------------------------------
Very often, we want to forecast the values of just one time series. For example, we want
to forecast sales in the next month. The sales variable is our target variable, and we can
create features by lagging past sales values.

We could also create lag features from accompanying predictive variables. For example, if we
want to predict pollutant concentration in the next few hours, we can create lag features
from past pollutant concentrations. In addition, we can create lag features from accompanying
time series values, like the concentrations of other gases, or the temperature or humidity.

See also
--------

Check out the additional transformers to create window features through the use of
rolling windows (:class:`WindowFeatures`) or expanding windows (:class:`ExpandingWindowFeatures`).

If you want to use :class:`LagFeatures` as part of a feature engineering pipeline,
check out Feature-engine's `Pipeline`.

Tutorials and courses
---------------------

For tutorials about this and other feature engineering methods for time series forecasting
check out our online courses:

.. figure::  ../../../images/fetsf.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Time Series Forecasting

.. figure::  ../../../images/fwml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Forecasting with Machine Learning

|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

Our courses are suitable for beginners and more advanced data scientists looking to
forecast time series using traditional machine learning models, like linear regression
or gradient boosting machines.

By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/timeseries/forecasting/WindowFeatures.rst
================================================
.. _window_features:

.. currentmodule:: feature_engine.timeseries.forecasting

WindowFeatures
==============

Window features are commonly used in data science to forecast time series with traditional
machine learning models, like linear regression or gradient boosting machines. Window features
are created by performing mathematical operations over windows of past data.

For example, the mean “sales” value of the previous 3 months of data is a window feature.
The maximum “revenue” of the previous three rows of data is another window feature.

In time series forecasting, we want to predict future values of the time series. To do this,
we can create window features by performing mathematical operations over windows of past
values of the time series data. Then, we would use this features to predict the time series
with any regression model.

Rolling window features with pandas
-----------------------------------

Window features are the result of window operations over the variables. Rolling window operations are
operations that perform an aggregation over a **sliding partition** of past values of the time
series data.

A window feature is, then, a feature created after computing mathematical
functions (e.g., mean, min, max, etc.) within a window over the past data.

In Python, we can create window features by utilizing pandas method `rolling`. For example,
by executing:

.. code:: python

    X[["var_1", "var_2"].rolling(window=3).agg(["max", "mean"])

With the previous command, we create 2 window features for each variable, `var_1` and
`var_2`, by taking the maximum and average value of the current and 2 previous rows of data.

If we want to use those features for forecasting using traditional machine learning
algorithms, we also need to shift the window forward with pandas method `shift`:

.. code:: python

    X[["var_1", "var_2"].rolling(window=3).agg(["max", "mean"]).shift(period=1)

Shifting is important to ensure that we are using values strictly in the past, respect
to the point that we want to forecast.

Sliding window features with Feature-engine
-------------------------------------------

:class:`WindowFeatures` can automatically create and add window features to the dataframe, by performing
multiple mathematical operations over different window sizes over various numerical variables.

Thus, :class:`WindowFeatures` creates and adds new features to the data set automatically
through the use of windows over historical data.

Window features: parameters
~~~~~~~~~~~~~~~~~~~~~~~~~~~

To create window features we need to determine a number of parameters. First, we need to
identify the size of the window or windows in which we will perform the operations. For
example, we can take the average of the variable over 3 months, or 6 weeks.

We also need to determine how far back is the window located respect to the data point we
want to forecast. For example, I can take the average of the last 3 weeks of data to forecast
this week of data, or I can take the average of the last 3 weeks of data to forecast next
weeks data, leaving a gap of a window in between the window feature and the forecasting point.

WindowFeatures: under the hood
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

:class:`WindowFeatures` works on top of `pandas.rolling`, `pandas.aggregate` and
`pandas.shift`. With `pandas.rolling`, :class:`WindowFeatures` determines the size
of the windows for the operations. With `pandas.rolling` we can specify the window size
with an integer, a string or a function. With :class:`WindowFeatures`, in addition, we
can pass a list of integers, strings or functions, to perform computations over multiple
window sizes.

:class:`WindowFeatures` uses `pandas.aggregate` to perform the mathematical operations
over the windows. Therefore, you can use any operation supported
by pandas. For supported aggregation functions, see Rolling Window
`Functions <https://pandas.pydata.org/docs/reference/window.html>`_.

With `pandas.shift`, :class:`WindowFeatures` places the value derived from the past
window, at the place of the value that we want to forecast. So if we want to forecast
this week with the average of the past 3 weeks of data, we should shift the value 1
week forward. If we want to forecast next week with the last 3 weeks of data, we should
forward the value 2 weeks forward.

:class:`WindowFeatures` will add the new features with a representative name to the
original dataframe. It also has the methods `fit()` and `transform()` that make it
compatible with the Scikit-learn's `Pipeline` and cross-validation functions.

Note that, in the current implementation, :class:`WindowFeatures` only works with dataframes whose index,
containing the time series timestamp, contains unique values and no NaN.

Examples
--------

Let's create a time series dataset to see how to create window features with
:class:`WindowFeatures`. The dataframe contains 3 numerical variables, a categorical
variable, and a datetime index. We also create a target variable.

.. code:: python

    import pandas as pd

    X = {"ambient_temp": [31.31, 31.51, 32.15, 32.39, 32.62, 32.5, 32.52, 32.68],
         "module_temp": [49.18, 49.84, 52.35, 50.63, 49.61, 47.01, 46.67, 47.52],
         "irradiation": [0.51, 0.79, 0.65, 0.76, 0.42, 0.49, 0.57, 0.56],
         "color": ["green"] * 4 + ["blue"] * 4,
         }

    X = pd.DataFrame(X)
    X.index = pd.date_range("2020-05-15 12:00:00", periods=8, freq="15min")

    y = pd.Series([1,2,3,4,5,6,7,8])
    y.index = X.index

    X.head()

Below we see the dataframe:

.. code:: python

                         ambient_temp  module_temp  irradiation  color
    2020-05-15 12:00:00         31.31        49.18         0.51  green
    2020-05-15 12:15:00         31.51        49.84         0.79  green
    2020-05-15 12:30:00         32.15        52.35         0.65  green
    2020-05-15 12:45:00         32.39        50.63         0.76  green
    2020-05-15 13:00:00         32.62        49.61         0.42   blue

Let's now print out the target:

.. code:: python

    y

Below we see the target variable:

.. code:: python

    2020-05-15 12:00:00    1
    2020-05-15 12:15:00    2
    2020-05-15 12:30:00    3
    2020-05-15 12:45:00    4
    2020-05-15 13:00:00    5
    2020-05-15 13:15:00    6
    2020-05-15 13:30:00    7
    2020-05-15 13:45:00    8
    Freq: 15min, dtype: int64

Now we will create window features from the numerical variables. By setting
`window=["30min", "60min"]` we perform calculations over windows of 30 and 60
minutes, respectively.

If you look at our toy dataframe, you'll notice that 30 minutes corresponds to 2 rows of
data, and 60 minutes are 4 rows of data. So, we will perform calculations over 2 and then
4 rows of data, respectively.

In `functions`, we indicate all the operations that we want to perform over those windows.
In our example below, we want to calculate the mean and the standard deviation of the
data within those windows and also find the maximum value within the windows.

With `freq="15min"` we indicate that we need to shift the calculations 15 minutes
forward. In other words, we are using the data available in windows defined up to 15 minutes
before the forecasting point.

.. code:: python

    from feature_engine.timeseries.forecasting import WindowFeatures

    win_f = WindowFeatures(
        window=["30min", "60min"], functions=["mean", "max", "std"], freq="15min",
    )

    X_tr = win_f.fit_transform(X)

    X_tr.head()

We find the window features on the right side of the dataframe.

.. code:: python

                         ambient_temp  module_temp  irradiation  color  \
    2020-05-15 12:00:00         31.31        49.18         0.51  green
    2020-05-15 12:15:00         31.51        49.84         0.79  green
    2020-05-15 12:30:00         32.15        52.35         0.65  green
    2020-05-15 12:45:00         32.39        50.63         0.76  green
    2020-05-15 13:00:00         32.62        49.61         0.42   blue

                         ambient_temp_window_30min_mean  \
    2020-05-15 12:00:00                             NaN
    2020-05-15 12:15:00                           31.31
    2020-05-15 12:30:00                           31.41
    2020-05-15 12:45:00                           31.83
    2020-05-15 13:00:00                           32.27

                         ambient_temp_window_30min_max  \
    2020-05-15 12:00:00                            NaN
    2020-05-15 12:15:00                          31.31
    2020-05-15 12:30:00                          31.51
    2020-05-15 12:45:00                          32.15
    2020-05-15 13:00:00                          32.39

                         ambient_temp_window_30min_std  \
    2020-05-15 12:00:00                            NaN
    2020-05-15 12:15:00                            NaN
    2020-05-15 12:30:00                       0.141421
    2020-05-15 12:45:00                       0.452548
    2020-05-15 13:00:00                       0.169706

                         module_temp_window_30min_mean  \
    2020-05-15 12:00:00                            NaN
    2020-05-15 12:15:00                         49.180
    2020-05-15 12:30:00                         49.510
    2020-05-15 12:45:00                         51.095
    2020-05-15 13:00:00                         51.490

                         module_temp_window_30min_max  \
    2020-05-15 12:00:00                           NaN
    2020-05-15 12:15:00                         49.18
    2020-05-15 12:30:00                         49.84
    2020-05-15 12:45:00                         52.35
    2020-05-15 13:00:00                         52.35

                         module_temp_window_30min_std  ...  \
    2020-05-15 12:00:00                           NaN  ...
    2020-05-15 12:15:00                           NaN  ...
    2020-05-15 12:30:00                      0.466690  ...
    2020-05-15 12:45:00                      1.774838  ...
    2020-05-15 13:00:00                      1.216224  ...

                         irradiation_window_30min_std  \
    2020-05-15 12:00:00                           NaN
    2020-05-15 12:15:00                           NaN
    2020-05-15 12:30:00                      0.197990
    2020-05-15 12:45:00                      0.098995
    2020-05-15 13:00:00                      0.077782

                         ambient_temp_window_60min_mean  \
    2020-05-15 12:00:00                             NaN
    2020-05-15 12:15:00                       31.310000
    2020-05-15 12:30:00                       31.410000
    2020-05-15 12:45:00                       31.656667
    2020-05-15 13:00:00                       31.840000

                         ambient_temp_window_60min_max  \
    2020-05-15 12:00:00                            NaN
    2020-05-15 12:15:00                          31.31
    2020-05-15 12:30:00                          31.51
    2020-05-15 12:45:00                          32.15
    2020-05-15 13:00:00                          32.39

                         ambient_temp_window_60min_std  \
    2020-05-15 12:00:00                            NaN
    2020-05-15 12:15:00                            NaN
    2020-05-15 12:30:00                       0.141421
    2020-05-15 12:45:00                       0.438786
    2020-05-15 13:00:00                       0.512640

                         module_temp_window_60min_mean  \
    2020-05-15 12:00:00                            NaN
    2020-05-15 12:15:00                      49.180000
    2020-05-15 12:30:00                      49.510000
    2020-05-15 12:45:00                      50.456667
    2020-05-15 13:00:00                      50.500000

                         module_temp_window_60min_max  \
    2020-05-15 12:00:00                           NaN
    2020-05-15 12:15:00                         49.18
    2020-05-15 12:30:00                         49.84
    2020-05-15 12:45:00                         52.35
    2020-05-15 13:00:00                         52.35

                         module_temp_window_60min_std  \
    2020-05-15 12:00:00                           NaN
    2020-05-15 12:15:00                           NaN
    2020-05-15 12:30:00                      0.466690
    2020-05-15 12:45:00                      1.672553
    2020-05-15 13:00:00                      1.368381

                         irradiation_window_60min_mean  \
    2020-05-15 12:00:00                            NaN
    2020-05-15 12:15:00                         0.5100
    2020-05-15 12:30:00                         0.6500
    2020-05-15 12:45:00                         0.6500
    2020-05-15 13:00:00                         0.6775

                         irradiation_window_60min_max  \
    2020-05-15 12:00:00                           NaN
    2020-05-15 12:15:00                          0.51
    2020-05-15 12:30:00                          0.79
    2020-05-15 12:45:00                          0.79
    2020-05-15 13:00:00                          0.79

                         irradiation_window_60min_std
    2020-05-15 12:00:00                           NaN
    2020-05-15 12:15:00                           NaN
    2020-05-15 12:30:00                      0.197990
    2020-05-15 12:45:00                      0.140000
    2020-05-15 13:00:00                      0.126853

    [5 rows x 22 columns]

The variables used as input for the window features are stored in the `variables_`
attribute of the :class:`WindowFeatures`:

.. code:: python

    win_f.variables_

.. code:: python

    ['ambient_temp', 'module_temp', 'irradiation']

We can obtain the names of the variables in the transformed dataframe using the
`get_feature_names_out()` method:

.. code:: python

    win_f.get_feature_names_out()

.. code:: python

    ['ambient_temp',
     'module_temp',
     'irradiation',
     'color',
     'ambient_temp_window_30min_mean',
     'ambient_temp_window_30min_max',
     'ambient_temp_window_30min_std',
     'module_temp_window_30min_mean',
     'module_temp_window_30min_max',
     'module_temp_window_30min_std',
     'irradiation_window_30min_mean',
     'irradiation_window_30min_max',
     'irradiation_window_30min_std',
     'ambient_temp_window_60min_mean',
     'ambient_temp_window_60min_max',
     'ambient_temp_window_60min_std',
     'module_temp_window_60min_mean',
     'module_temp_window_60min_max',
     'module_temp_window_60min_std',
     'irradiation_window_60min_mean',
     'irradiation_window_60min_max',
     'irradiation_window_60min_std']

Dropping rows with nan
~~~~~~~~~~~~~~~~~~~~~~

When we create window features, we may introduce nan values for those data points where
there isn't enough data in the past to create the windows. We can automatically drop
the rows with nan values in the window features both in the train set and in the target
variable as follows:

.. code:: python

    win_f = WindowFeatures(
        window=["30min", "60min"],
        functions=["mean", ],
        freq="15min",
        drop_na=True,
    )

    win_f.fit(X)

    X_tr, y_tr = win_f.transform_x_y(X, y)

    X.shape, y.shape, X_tr.shape, y_tr.shape

We see that the resulting dataframe contains less rows than the original dataframe:

.. code:: python

    ((8, 4), (8,), (7, 10), (7,))

Imputing rows with nan
~~~~~~~~~~~~~~~~~~~~~~

If instead of removing the row with nan in the window features, we want to impute those
values, we can do so with any of Feature-engine's imputers. Here, we will replace nan with
the arbitrary value -99, using the `ArbitraryNumberImputer` within a pipeline:

.. code:: python

    from feature_engine.imputation import ArbitraryNumberImputer
    from feature_engine.pipeline import Pipeline

    win_f = WindowFeatures(
        window=["30min", "60min"],
        functions=["mean", ],
        freq="15min",
    )

    pipe = Pipeline([
        ("windows", win_f),
        ("imputer", ArbitraryNumberImputer(arbitrary_number=-99))
    ])

    X_tr = pipe.fit_transform(X, y)

    print(X_tr.head())

We see the resulting dataframe, where the nan values were replaced by -99:

.. code:: python

                         ambient_temp  module_temp  irradiation  color  \
    2020-05-15 12:00:00         31.31        49.18         0.51  green
    2020-05-15 12:15:00         31.51        49.84         0.79  green
    2020-05-15 12:30:00         32.15        52.35         0.65  green
    2020-05-15 12:45:00         32.39        50.63         0.76  green
    2020-05-15 13:00:00         32.62        49.61         0.42   blue

                         ambient_temp_window_30min_mean  \
    2020-05-15 12:00:00                          -99.00
    2020-05-15 12:15:00                           31.31
    2020-05-15 12:30:00                           31.41
    2020-05-15 12:45:00                           31.83
    2020-05-15 13:00:00                           32.27

                         module_temp_window_30min_mean  \
    2020-05-15 12:00:00                        -99.000
    2020-05-15 12:15:00                         49.180
    2020-05-15 12:30:00                         49.510
    2020-05-15 12:45:00                         51.095
    2020-05-15 13:00:00                         51.490

                         irradiation_window_30min_mean  \
    2020-05-15 12:00:00                        -99.000
    2020-05-15 12:15:00                          0.510
    2020-05-15 12:30:00                          0.650
    2020-05-15 12:45:00                          0.720
    2020-05-15 13:00:00                          0.705

                         ambient_temp_window_60min_mean  \
    2020-05-15 12:00:00                      -99.000000
    2020-05-15 12:15:00                       31.310000
    2020-05-15 12:30:00                       31.410000
    2020-05-15 12:45:00                       31.656667
    2020-05-15 13:00:00                       31.840000

                         module_temp_window_60min_mean  \
    2020-05-15 12:00:00                     -99.000000
    2020-05-15 12:15:00                      49.180000
    2020-05-15 12:30:00                      49.510000
    2020-05-15 12:45:00                      50.456667
    2020-05-15 13:00:00                      50.500000

                         irradiation_window_60min_mean
    2020-05-15 12:00:00                       -99.0000
    2020-05-15 12:15:00                         0.5100
    2020-05-15 12:30:00                         0.6500
    2020-05-15 12:45:00                         0.6500
    2020-05-15 13:00:00                         0.6775

Working with pandas series
~~~~~~~~~~~~~~~~~~~~~~~~~~

If your time series is a pandas Series instead of a pandas Dataframe, you need to
transform it into a dataframe before using :class:`WindowFeatures`.

The following is a pandas Series:

.. code:: python

    X['ambient_temp']

.. code:: python

    2020-05-15 12:00:00    31.31
    2020-05-15 12:15:00    31.51
    2020-05-15 12:30:00    32.15
    2020-05-15 12:45:00    32.39
    2020-05-15 13:00:00    32.62
    2020-05-15 13:15:00    32.50
    2020-05-15 13:30:00    32.52
    2020-05-15 13:45:00    32.68
    Freq: 15T, Name: ambient_temp, dtype: float64

We can use :class:`WindowFeatures` to create, for example, 2 new window features by finding
the mean and maximum value within a 45 minute windows of a pandas Series if we convert it
to a pandas Dataframe using the method `to_frame()`:

.. code:: python

    win_f = WindowFeatures(
        window=["45min"],
        functions=["mean", "max"],
        freq="30min",
    )

    X_tr = win_f.fit_transform(X['ambient_temp'].to_frame())

    X_tr.head()

.. code:: python

                         ambient_temp  ambient_temp_window_45min_mean  \
    2020-05-15 12:00:00         31.31                             NaN
    2020-05-15 12:15:00         31.51                             NaN
    2020-05-15 12:30:00         32.15                       31.310000
    2020-05-15 12:45:00         32.39                       31.410000
    2020-05-15 13:00:00         32.62                       31.656667

                         ambient_temp_window_45min_max
    2020-05-15 12:00:00                            NaN
    2020-05-15 12:15:00                            NaN
    2020-05-15 12:30:00                          31.31
    2020-05-15 12:45:00                          31.51
    2020-05-15 13:00:00                          32.15

And if we do not want the original values of time series in the returned dataframe, we
just need to remember to drop the original series after the transformation:

.. code:: python

    win_f = WindowFeatures(
        window=["45min"],
        functions=["mean", "max"],
        freq="30min",
        drop_original=True,
    )

    X_tr = win_f.fit_transform(X['ambient_temp'].to_frame())

    X_tr.head()

.. code:: python

                         ambient_temp_window_45min_mean  \
    2020-05-15 12:00:00                             NaN
    2020-05-15 12:15:00                             NaN
    2020-05-15 12:30:00                       31.310000
    2020-05-15 12:45:00                       31.410000
    2020-05-15 13:00:00                       31.656667

                         ambient_temp_window_45min_max
    2020-05-15 12:00:00                            NaN
    2020-05-15 12:15:00                            NaN
    2020-05-15 12:30:00                          31.31
    2020-05-15 12:45:00                          31.51
    2020-05-15 13:00:00                          32.15

Getting the name of the new features
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We can easily obtain the name of the original and new variables with the method
`get_feature_names_out`. By using the method with the default parameters, we obtain
all the features in the output dataframe.

.. code:: python

    win_f = WindowFeatures()

    win_f.fit(X)

    win_f.get_feature_names_out()

.. code:: python

    ['ambient_temp',
     'module_temp',
     'irradiation',
     'color',
     'ambient_temp_window_3_mean',
     'module_temp_window_3_mean',
     'irradiation_window_3_mean']

Windows from the target vs windows from predictor variables
-----------------------------------------------------------

Very often, we work with univariate time series, for example, the total sales revenue of a
retail company. We want to forecast future sales values. The sales variable is our target
variable, and we can extract features from windows of past sales values.

We could also work with multivariate time series, where we have sales in different
countries, or alternatively, multiple time series, like pollutant concentration in the
air, accompanied by concentrations of other gases, temperature, and humidity.

When working with multivariate time series, like sales in multiple countries, we would
extract features from windows of past data for each country separately.

When working with multiple time series, like pollutant concentration, gas concentration,
temperature, and humidity, pollutant concentration is our target variable, and the other
time series are accompanying predictive variables. We can create window features from
past pollutant concentrations, that is, from past time steps of our target variable.
And, in addition, we can create features from windows of past data from accompanying
time series, like the concentrations of other gases or the temperature or humidity.

The process of feature extraction from time series data, to create a table of predictors
and a target variable to forecast using supervised learning models like linear regression
or random forest, is called “tabularizing” the time series.

See also
--------

Check out the additional transformers to create expanding window features
(:class:`ExpandingWindowFeatures`) or lag features, by lagging past values of the time
series data (:class:`LagFeatures`).

Other open-source packages for window features
----------------------------------------------

- `tsfresh <https://tsfresh.readthedocs.io/en/latest/text/forecasting.html>`_
- `featuretools <https://featuretools.alteryx.com/en/stable/guides/time_series.html>`_

Tutorials and courses
---------------------

For tutorials about this and other feature engineering methods for time series forecasting
check out our online courses:

.. figure::  ../../../images/fetsf.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Time Series Forecasting

.. figure::  ../../../images/fwml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Forecasting with Machine Learning

|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|
|

Our courses are suitable for beginners and more advanced data scientists looking to
forecast time series using traditional machine learning models, like linear regression
or gradient boosting machines.

By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/transformation/ArcsinTransformer.rst
================================================
.. _arcsin:

.. currentmodule:: feature_engine.transformation

ArcsinTransformer
=================

The :class:`ArcsinTransformer()` applies the arcsin transformation to
numerical variables.

The arcsine transformation, also called arcsin square root transformation, or
angular transformation, takes the form of arcsin(sqrt(x)) where x is a real number
between 0 and 1.

The arcsin square root transformation helps in dealing with probabilities,
percentages, and proportions.

The :class:`ArcsinTransformer()` only works with numerical variables with values
between 0 and 1. If the variable contains a value outside of this range, the
transformer will raise an error.

Example
~~~~~~~

Let's load the breast cancer dataset from scikit-learn and  separate it into train and
test sets.

.. code:: python

    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    from sklearn.model_selection import train_test_split
    from sklearn.datasets import load_breast_cancer

    from feature_engine.transformation import ArcsinTransformer
      
    #Load dataset
    breast_cancer = load_breast_cancer()
    X = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)
    y = breast_cancer.target

    # Separate data into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

Now we want to apply the arcsin transformation to some of the variables in the
dataframe. These variables values are in the range 0-1, as we will see in coming
histograms.

First, let's make a list with the variable names:

.. code:: python

    vars_ = [
      'mean compactness',
      'mean concavity',
      'mean concave points',
      'mean fractal dimension',
      'smoothness error',
      'compactness error',
      'concavity error',
      'concave points error',
      'symmetry error',
      'fractal dimension error',
      'worst symmetry',
      'worst fractal dimension']

Now, let's set up the arscin transformer to modify only the previous variables:

.. code:: python

    # set up the arcsin transformer
    tf = ArcsinTransformer(variables = vars_)

    # fit the transformer
    tf.fit(X_train)
    
The transformer does not learn any parameters when applying the fit method. It does
check however that the variables are numericals and with the correct value range.

We can now go ahead and transform the variables:

.. code:: python

    # transform the data
    train_t = tf.transform(X_train)
    test_t = tf.transform(X_test)

And that's it, now the variables have been transformed with the arscin formula.

Finally, let's make a histogram for each of the original variables to examine their
distribution:

.. code:: python

    # original variables
    X_train[vars_].hist(figsize=(20,20))

.. image:: ../../images/breast_cancer_raw.png

You can see in the previous image that many of the variables are skewed. Note however,
that all variables had values between 0 and 1.

Now, let's examine the distribution after the transformation:

.. code:: python

    # transformed variable
    train_t[vars_].hist(figsize=(20,20))

.. image:: ../../images/breast_cancer_arcsin.png

You can see in the previous image that many variables have after the transformation a
more Gaussian looking shape.

Additional resources
--------------------

For more details about this and other feature engineering methods check out these resources:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/transformation/BoxCoxTransformer.rst
================================================
.. _box_cox:

.. currentmodule:: feature_engine.transformation

BoxCoxTransformer
=================

The Box-Cox transformation is a generalization of the power transformations family and is
defined as follows:

.. code:: python

   y = (x**λ - 1) / λ,      for λ != 0
   y = log(x),              for λ = 0

Here, y is the transformed data, x is the variable to transform and λ is the transformation
parameter.

The Box Cox transformation is used to reduce or eliminate variable skewness and obtain
features that better approximate a normal distribution.

The Box Cox transformation evaluates commonly used transformations. When λ = 1 then we
have the original variable, when λ = 0, we have the logarithm transformation, when λ = - 1
we have the reciprocal transformation, and when λ = 0.5 we have the square root.

The Box-Cox transformation evaluates several values of λ using the maximum likelihood,
and selects the optimal value of the λ parameter, which is the one that returns the best
transformation. The best transformation occurs when the transformed data better
approximates a normal distribution.

The Box Cox transformation is defined for strictly positive variables. If your variables
are not strictly positive, you can add a constant or use the Yeo-Johnson transformation
instead.

Uses of the Box Cox Transformation
----------------------------------

Many statistical methods that we use for data analysis make assumptions about the data.
For example, the linear regression model assumes that the values of the dependent variable
are independent, that there is a linear relationship between the response variable and the
independent variables, and that the residuals are normally distributed and centered at 0.

When these assumptions are not met, we can't fully trust the results of our regression
analyses. To make data meet the assumptions and improve the trust in the models, it is
common practice in data science projects to transform the variables before the analysis.

In time series forecasting, we use the Box Cox transformation to make non-stationary time
series stationary.

References
----------

George Box and David Cox. "An Analysis of Transformations". Read at a RESEARCH MEETING,
1964. https:

BoxCoxTransformer
-----------------

The :class:`BoxCoxTransformer()` applies the BoxCox transformation to numerical variables.
It uses `SciPy.stats <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html>`_ under the hood to apply the transformation.

The BoxCox transformation works only for strictly positive variables (>=0). If the
variable contains 0 or negative values, the :class:`BoxCoxTransformer()` will return an
error. To apply this transformation to non-positive variables, you can add a constant
value. Alternatively, you can apply the Yeo-Johnson transformation with the
:class:`YeoJohnsonTransformer()`.

Python code examples
--------------------

In this section, we will apply this data transformation to 2 variables of the Ames house
prices dataset.

Let's start by importing the modules, classes and functions and then loading the house
prices dataset and separating it into train and test sets.

.. code:: python

    import matplotlib.pyplot as plt
    from sklearn.datasets import fetch_openml
    from sklearn.model_selection import train_test_split

    from feature_engine.transformation import BoxCoxTransformer

    data = fetch_openml(name='house_prices', as_frame=True)
    data = data.frame

    X = data.drop(['SalePrice', 'Id'], axis=1)
    y = data['SalePrice']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42)

    print(X_train.head())

In the following output we see the predictor variables of the house prices dataset:

.. code:: python

          MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \
    254           20       RL         70.0     8400   Pave   NaN      Reg
    1066          60       RL         59.0     7837   Pave   NaN      IR1
    638           30       RL         67.0     8777   Pave   NaN      Reg
    799           50       RL         60.0     7200   Pave   NaN      Reg
    380           50       RL         50.0     5000   Pave  Pave      Reg

         LandContour Utilities LotConfig  ... ScreenPorch PoolArea PoolQC  Fence  \
    254          Lvl    AllPub    Inside  ...           0        0    NaN    NaN
    1066         Lvl    AllPub    Inside  ...           0        0    NaN    NaN
    638          Lvl    AllPub    Inside  ...           0        0    NaN  MnPrv
    799          Lvl    AllPub    Corner  ...           0        0    NaN  MnPrv
    380          Lvl    AllPub    Inside  ...           0        0    NaN    NaN

         MiscFeature MiscVal  MoSold  YrSold  SaleType  SaleCondition
    254          NaN       0       6    2010        WD         Normal
    1066         NaN       0       5    2009        WD         Normal
    638          NaN       0       5    2008        WD         Normal
    799          NaN       0       6    2007        WD         Normal
    380          NaN       0       5    2010        WD         Normal

    [5 rows x 79 columns]

Let's inspect the distribution of 2 variables in the original data with histograms.

.. code:: python

    X_train[['LotArea', 'GrLivArea']].hist(figsize=(10,5))
    plt.show()

In the following plots we see that the variables are non-normally distributed:

.. image:: ../../images/nonnormalvars2.png

Now we apply the BoxCox transformation to the 2 indicated variables. First, we set up
the transformer and fit it to the train set, so that it finds the optimal lambda value.

.. code:: python

    boxcox = BoxCoxTransformer(variables = ['LotArea', 'GrLivArea'])
    boxcox.fit(X_train)

With `fit()`, the :class:`BoxCoxTransformer()` learns the optimal lambda for the
transformation. We can inspect these values as follows:

.. code:: python

    boxcox.lambda_dict_

We see the optimal lambda values below:

.. code:: python

    {'LotArea': 0.0028222323212918547, 'GrLivArea': -0.006312580181375803}

Now, we can go ahead and transform the data:

.. code:: python

   train_t = boxcox.transform(X_train)
   test_t = boxcox.transform(X_test)

Let's now examine the variable distribution after the transformation with histograms:

.. code:: python

    train_t[['LotArea', 'GrLivArea']].hist(figsize=(10,5))
    plt.show()

In the following histograms we see that the variables approximate better the normal distribution.

.. image:: ../../images/nonnormalvars2transformed.png

If we want to recover the original data representation, we can also do so as follows:

.. code:: python

    train_unt = boxcox.inverse_transform(train_t)
    test_unt = boxcox.inverse_transform(test_t)

    train_unt[['LotArea', 'GrLivArea']].hist(figsize=(10,5))
    plt.show()

In the following plots we see that the variables are non-normally distributed, because they contain the original values, prior to the data transformation:

.. image:: ../../images/nonnormalvars2.png

Tutorials, books and courses
----------------------------

You can find more details about the Box Cox transformation technique with the :class:`BoxCoxTransformer()` here:

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/transformation/BoxCoxTransformer.ipynb>`_

For tutorials about this and other data transformation techniques and feature engineering
methods check out our online courses:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

.. figure::  ../../images/fetsf.png
   :width: 300
   :figclass: align-center
   :align: right
   :target: https:

   Feature Engineering for Time Series Forecasting

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Our book and courses are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/transformation/index.rst
================================================
.. -*- mode: rst -*-

Variance Stabilizing Transformations
====================================

Feature-engine's variable transformers transform numerical variables with various
mathematical transformations.

Variable transformations are commonly used to spread the values of the original variables
over a wider value range. See the following illustration:

.. figure::  ../../images/Variable_Transformation.png
   :align:   center

Article
-------

We added a lot of information about **variance stabilizing transformations** in this
`article <https://www.blog.trainindata.com/variance-stabilizing-transformations-in-machine-learning/>`_.

**Note**

Note however, that improving the value spread is not always possible and it depends
on the nature of the variable.

**Transformers**

.. toctree::
   :maxdepth: 1

   LogTransformer
   LogCpTransformer
   ReciprocalTransformer
   ArcsinTransformer
   PowerTransformer
   BoxCoxTransformer
   YeoJohnsonTransformer

================================================
FILE: docs/user_guide/transformation/LogCpTransformer.rst
================================================
.. _log_cp:

.. currentmodule:: feature_engine.transformation

LogCpTransformer
================

:class:`LogCpTransformer()` applies the transformation log(x + C), where x is the
variable to transform and C is a positive constant that shifts the distribution towards
positive values.

:class:`LogCpTransformer()` is an extension of :class:`LogTransformer()` that allows
adding a constant to move distributions towards positive values. For more details about
the logarithm transformation, check out the :ref:`LogTransformer()'s user Guide <log_transformer>`.

Defining C
----------

You can enter the positive quantity to add to the variable as a dictionary, where the
keys are the variable names, and the values are the constant to add to each variable. If you
want to add the same value to all variables, you can pass an integer or float, instead.

Alternatively, the :class:`LogCpTransformer()` will find the necessary value to make all
values of the variable positive. For strictly positive variables, C will be 0, and the
transformation will be log(x).

Python example
--------------

Let's check out the functionality of :class:`LogCpTransformer()`.

Transforming strictly positive variables
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Let's load the California housing dataset that comes with Scikit-learn and separate it
into train and test sets.

.. code:: python

    import pandas as pd
    import matplotlib.pyplot as plt
    from sklearn.model_selection import train_test_split
    from sklearn.datasets import fetch_california_housing

    from feature_engine.transformation import LogCpTransformer

    # Load dataset
    X, y = fetch_california_housing( return_X_y=True, as_frame=True)

    # Separate into train and test sets
    X_train, X_test, y_train, y_test =  train_test_split(
        X, y, test_size=0.3, random_state=0)

Now we want to apply the logarithm to 2 of the variables in the dataset using the
:class:`LogCpTransformer()`. We want the transformer to detect automatically the
quantity "C" that needs to be added to the variable:

.. code:: python

    # set up the variable transformer
    tf = LogCpTransformer(variables = ["MedInc", "HouseAge"], C="auto")

    # fit the transformer
    tf.fit(X_train)

With `fit()` the :class:`LogCpTransformer()` learns the quantity "C" and stores it as
an attribute. We can visualise the learned parameters as follows:

.. code:: python

    # learned constant C
    tf.C_

As these variables are strictly positive, the transformer will add 0 to the variables
before applying the logarithm transformation:

.. code:: python

    {'MedInc': 0, 'HouseAge': 0}

In this case, the transformation applied by :class:`LogCpTransformer()` is the same as
using :class:`LogTransformer()` because these variables are strictly positive.

We can now go ahead and transform the variables:

.. code:: python

    # transform the data
    train_t= tf.transform(X_train)
    test_t= tf.transform(X_test)

Then we can plot the original variable distribution:

.. code:: python

    # un-transformed variable
    X_train["MedInc"].hist(bins=20)
    plt.title("MedInc - original distribution")
    plt.ylabel("Number of observations")

.. image:: ../../images/logcpraw.png

|

And the distribution of the transformed variable:

.. code:: python

    # transformed variable
    train_t["MedInc"].hist(bins=20)
    plt.title("MedInc - transformed distribution")
    plt.ylabel("Number of observations")

.. image:: ../../images/logcptransform.png

|

Transforming non-strictly positive variables
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Let's now show the functionality of :class:`LogCpTransformer()` with variables that contain
values lower or equal to 0. Let's load the diabetes dataset:

.. code:: python

    import pandas as pd
    import matplotlib.pyplot as plt
    from sklearn.model_selection import train_test_split
    from sklearn.datasets import load_diabetes
    from feature_engine.transformation import LogCpTransformer

    # Load dataset
    X, y = load_diabetes( return_X_y=True, as_frame=True)

    # Separate into train and test sets
    X_train, X_test, y_train, y_test =  train_test_split(
        X, y, test_size=0.3, random_state=0)

Let's print out a summary of the main characteristics of 2 of the variables:

.. code:: python

    print(X_train[["bmi", "s3"]].describe())

In the following output we see that the variables contain negative values:

.. code:: python

                  bmi          s3
    count  309.000000  309.000000
    mean    -0.001298    0.000511
    std      0.048368    0.048294
    min     -0.084886   -0.102307
    25%     -0.036385   -0.032356
    50%     -0.008362   -0.006584
    75%      0.030440    0.030232
    max      0.170555    0.181179

Let's now set up :class:`LogCpTransformer()` to shift the variables' distribution to
positive values and then apply the logarithm:

.. code:: python

    tf = LogCpTransformer(variables = ["bmi", "s3"], C="auto")
    tf.fit(X_train)

We can inspect the constant values that will be added to each variable:

.. code:: python

    tf.C_

Since these variables were not strictly positive, :class:`LogCpTransformer()` found
the minimum value needed to make their values positive:

.. code:: python

    {'bmi': 1.0848862355291056, 's3': 1.102307050517416}

We can now transform the data:

.. code:: python

    train_t= tf.transform(X_train)
    test_t= tf.transform(X_test)

Let's plot `bmi` before the transformation:

.. code:: python

    X_train["bmi"].hist(bins=20)
    plt.title("bmi - original distribution")
    plt.ylabel("Number of observations")

In the following image we see the original distribution of `bmi`:

.. image:: ../../images/bmiraw.png

|

Let's now plot the transformed variable:

.. code:: python

    # transformed variable
    train_t["bmi"].hist(bins=20)
    plt.title("bmi - transformed distribution")
    plt.ylabel("Number of observations")

In the following image we see the distribution of `bmi` after the transformation:

.. image:: ../../images/bmilogcp.png

|

Adding the same constant to all variables
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

You can add the same constant to all variables by setting up :class:`LogCpTransformer()`
as follows:

.. code:: python

    tf = LogCpTransformer(C=5)
    tf.fit(X_train)

In this case, all numerical variables will be transformed. We can find the variables that
will be transformed in the `variables_` attribute:

.. code:: python

    tf.variables_

All numerical variables were selected for the transformation:

.. code:: python

    ['age', 'sex', 'bmi', 'bp', 's1', 's2', 's3', 's4', 's5', 's6']

You can now apply `transform()` to transform all these variables.

Adding different user defined constants
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

If you want to add specific values to specific variables, you can do so by setting
:class:`LogCpTransformer()` as follows:

.. code:: python

    tf = LogCpTransformer(C={"bmi": 2, "s3": 3, "s4": 4})
    tf.fit(X_train)

In this case, :class:`LogCpTransformer()` will only modify the variables indicated in the
dictionary:

.. code:: python

    tf.variables_

The variables in the dictionary will be transformed:

.. code:: python

    ['bmi', 's3', 's4']

And the constant values will be those from the dictionary:

.. code:: python

    tf.C_

`C_` coincides with the values entered in `C`:

.. code:: python

    {'bmi': 2, 's3': 3, 's4': 4}

You can now apply `transform()` to transform all these variables.

Tutorials, books and courses
----------------------------

You can find more details about the :class:`LogCpTransformer()` here:

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/transformation/LogCpTransformer.ipynb>`_

For tutorials about this and other data transformation methods, like the square root transformation, power transformations, the box cox transformation, check out our online course:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike.

================================================
FILE: docs/user_guide/transformation/LogTransformer.rst
================================================
.. _log_transformer:

.. currentmodule:: feature_engine.transformation

LogTransformer
==============

The log transformation is used to transform skewed data so that the values are more evenly distributed across the value range.

Some regression models, like linear regression, t-test and ANOVA, make assumptions about the data. When the assumptions are not met, we can't trust the results. Applying data transformations is common practice during regression analysis because it can help make the data meet those assumptions and hence obtain more reliable results.

The logarithm function is helpful for dealing with positive data with a right-skewed distribution. That is, those variables whose observations accumulate towards lower values. A common example is the variable income, with a heavy accumulation of values toward lower salaries.

More generally, when data follows a log-normal distribution, then its log-transformed version approximates a normal distribution.

Other useful transformations are the square root transformation, power transformations and the box cox transformation.

In statistical analysis, we can apply the logarithmic transformation to both the dependent variable (that is, the target) and the independent variables (that is, the predictors). These can help meet the linear regression model assumptions and unmask a linear relationship between predictors and response variable.

With Feature-engine, we can only log transform input features. You can easily transform the target variable by applying `np.log(y)`.

The LogTransformer
------------------

The :class:`LogTransformer()` applies the natural logarithm or the logarithm in base 10 to numerical variables. Note that the logarithm can only be applied to positive values. Thus, if the variable contains 0 or negative variables, this transformer will return and error.

To transform non-positive variables you can add a constant to shift the data points towards positive values. You can do this from within the transformer by using :class:`LogCpTransformer()`.

Python implementation
---------------------

In this section, we will apply the logarithmic transformation to some independent variables from the Ames house prices dataset.

Let's start by importing the required libraries and transformers for data analysis and then load the dataset and separate it into train and test sets.

.. code:: python

    import matplotlib.pyplot as plt
    from sklearn.datasets import fetch_openml
    from sklearn.model_selection import train_test_split

    from feature_engine.transformation import LogTransformer

    data = fetch_openml(name='house_prices', as_frame=True)
    data = data.frame

    X = data.drop(['SalePrice', 'Id'], axis=1)
    y = data['SalePrice']

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42)

    print(X_train.head())

In the following output we see the predictor variables of the house prices dataset:

.. code:: python

          MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \
    254           20       RL         70.0     8400   Pave   NaN      Reg
    1066          60       RL         59.0     7837   Pave   NaN      IR1
    638           30       RL         67.0     8777   Pave   NaN      Reg
    799           50       RL         60.0     7200   Pave   NaN      Reg
    380           50       RL         50.0     5000   Pave  Pave      Reg

         LandContour Utilities LotConfig  ... ScreenPorch PoolArea PoolQC  Fence  \
    254          Lvl    AllPub    Inside  ...           0        0    NaN    NaN
    1066         Lvl    AllPub    Inside  ...           0        0    NaN    NaN
    638          Lvl    AllPub    Inside  ...           0        0    NaN  MnPrv
    799          Lvl    AllPub    Corner  ...           0        0    NaN  MnPrv
    380          Lvl    AllPub    Inside  ...           0        0    NaN    NaN

         MiscFeature MiscVal  MoSold  YrSold  SaleType  SaleCondition
    254          NaN       0       6    2010        WD         Normal
    1066         NaN       0       5    2009        WD         Normal
    638          NaN       0       5    2008        WD         Normal
    799          NaN       0       6    2007        WD         Normal
    380          NaN       0       5    2010        WD         Normal

    [5 rows x 79 columns]

Let's inspect the distribution of 2 variables from the original data with histograms.

.. code:: python

    X_train[['LotArea', 'GrLivArea']].hist(figsize=(10,5))
    plt.show()

In the following plots we see that the variables show a right-skewed distribution, so they are good candidates for the log transformation:

.. image:: ../../images/nonnormalvars2.png

We want to apply the natural logarithm to these 2 variables in the dataset using the
:class:`LogTransformer()`. We set up the transformer as follows:

.. code:: python

    logt = LogTransformer(variables = ['LotArea', 'GrLivArea'])

    logt.fit(X_train)

With `fit()`, this transformer does not learn any parameters, but it checks that the variables you entered are numerical, or if no variable was entered, it will automatically find all numerical variables.

To apply the logarithm in base 10, pass `'10'` to the `base` parameter when setting up the transformer.

Now, we can go ahead and transform the data:

.. code:: python

   train_t = logt.transform(X_train)
   test_t = logt.transform(X_test)

Let's now examine the variable distribution in the log-transformed data with histograms:

.. code:: python

    train_t[['LotArea', 'GrLivArea']].hist(figsize=(10,5))
    plt.show()

In the following histograms we see that the natural log transformation helped make the variables better approximate a normal distribution.

.. image:: ../../images/nonnormalvars2logtransformed.png

Note that the transformed variable has a more Gaussian looking distribution.

If we want to recover the original data representation, with the method `inverse_transform`, the :class:`LogTransformer()` will apply the exponential function to obtain the variable in its original scale:

.. code:: python

    train_unt = logt.inverse_transform(train_t)
    test_unt = logt.inverse_transform(test_t)

    train_unt[['LotArea', 'GrLivArea']].hist(figsize=(10,5))
    plt.show()

In the following plots we see histograms showing the variables in their original scale:

.. image:: ../../images/nonnormalvars2.png

Following the transformations with scatter plots and residual analysis of the regression models helps understand if the transformations are useful in our regression analysis.

Tutorials, books and courses
----------------------------

You can find more details about the :class:`LogTransformer()` here:

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/transformation/LogTransformer.ipynb>`_

For tutorials about this and other data transformation methods, like the square root transformation, power transformations, the box cox transformation, check out our online course:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/transformation/PowerTransformer.rst
================================================
.. _power:

.. currentmodule:: feature_engine.transformation

PowerTransformer
================

Power transformations are a family of mathematical functions used to transform numerical
variables into a more suitable shape for modeling. The transformation function is 
typically represented as :math:`x' = x^{\lambda}`, where :math:`x` is the original 
variable and :math:`\lambda` (lambda) is the transformation parameter.

These transformations help stabilize the variance, make the data adopt a more normal 
distribution-like shape, and/or improve the linearity of relationships.

Use of Power transformations
----------------------------

Power transformations are particularly useful for meeting the assumptions of 
statistical tests, and models that require linear relationships between variables and 
homoscedasticity (constant variance across values). They can also help in reducing 
skewness in the data, i.e., by normalizing distributions.

Power transformations differ from scalers in that they modify the distribution of the 
data, typically to stabilize variance and normalize the distribution, whereas scalers 
simply adjust the scale of the data without altering its underlying distribution.

In short, power functions provide an excellent data analysis toolkit, especially for 
linear models (regression or classification).

Special cases of power transformations
--------------------------------------

Most variable transformations, like the logarithm, the reciprocal and the square root, 
are special cases of power transformations, where the exponent (lambda) is 
0, -1 and 0.5, respectively.

You can apply these transformations with :class:`PowerTransformer`, as we will see 
later in this page, or through dedicated transformers, like :class:`LogTransformer` 
and :class:`ReciprocalTransformer`.

Which lambda should I choose?
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The challenge of power transformations resides in finding the right lambda for the 
transformation. In general, this consists of trial and error, or using generalization 
functions like the Box-Cox or the Yeo-Johnson transformation.

As general guidelines, if the variables are right-skewed we'd use lambda <1, and if the 
variables are left-skewed we'd use lambda >1.

Box-Cox transformation
~~~~~~~~~~~~~~~~~~~~~~

The Box-Cox transformation is a generalization of power transformations that finds 
an optimal lambda to stabilize variance and make the data more normally distributed. 
This transformation only accepts positive values.

Feature-engine's :class:`BoxCoxTransformer()` applies the Box-Cox transformation.

Yeo-Johnson transformation
~~~~~~~~~~~~~~~~~~~~~~~~~~

The Yeo-Johnson transformation extends the Box-Cox transformation to handle both 
positive and negative values, to find an optimal lambda for the transformation.

:class:`YeoJohnsonTransformer()` applies the Yeo-Johnson transformation.

Other transformers
~~~~~~~~~~~~~~~~~~

Feature-engine also provides the following power transformers:

- :class:`LogTransformer`
- :class:`LogCpTransformer`
- :class:`ReciprocalTransformer`
- :class:`ArcsinTransformer`

For more details about these variance stabilizing transformations, check the article 
`Variance stabilizing transformations in machine learning <https://www.blog
.trainindata.com/variance-stabilizing-transformations-in-machine-learning/>`_.

Python example
--------------

:class:`PowerTransformer()` applies power transformations to numerical independent 
variables. We'll use the Ames House Prices' dataset to see it in action. 
First, let's load the dataset and split it into train and test sets:

.. code:: python

	import numpy as np
	import pandas as pd
	import seaborn as sns
	import matplotlib.pyplot as plt
	from sklearn.preprocessing import scale
	from sklearn.datasets import fetch_openml
	from sklearn.model_selection import train_test_split

	from feature_engine.transformation import PowerTransformer

	# Load dataset
	X, y = fetch_openml(name='house_prices', version=1, return_X_y=True, as_frame=True)
	X.set_index('Id', inplace=True)

	# Separate into train and test sets
	X_train, X_test, y_train, y_test =  train_test_split(
		X, y, test_size=0.3, random_state=42
	)

Now, let's visualize the distribution of the `LotArea` variable:

.. code:: python

	sns.histplot(X_train['LotArea'], kde=True, bins=50)

In the following output, we can see that the original feature distribution is highly
right-skewed:

.. image:: ../../images/lotarea_raw.png

|

Finding the right lambda for the power transformation is challenging, 
and it often requires trial an error. So let's begin by trying the default 
coefficient (lambda), which is 0.5 (i.e., we're applying a square root transformation):

.. code:: python

	# Set up the variable transformer (tf)
	tf = PowerTransformer(variables = ['LotArea', 'GrLivArea'])

	# Fit the transformer
	X_train_transformed = tf.fit_transform(X_train)

	# Plot histogram
	sns.histplot(X_train_transformed['LotArea'], kde=True, bins=50)

And here's the transformed feature distribution:

.. image:: ../../images/lotarea_pt.png

|

It looks better, huh!? It's still right-skewed, but the variation is lower
(let's confirm it soon).
Now, let's try to pass an "optimal value" for the parameter λ (exp):

.. code:: python

	# Set up the variable transformer (tf)
	tf_custom = PowerTransformer(variables = ['LotArea', 'GrLivArea'], exp=0.001)

	# Fit the transformer
	X_train_transformed_custom = tf_custom.fit_transform(X_train)

	# Plot histogram
	sns.histplot(X_train_transformed_custom['LotArea'], kde=True, bins=50)

In the following output, we can see the data now has a more Gaussian-like distribution,
and the variance seems lower. Therefore, we can see that by using a custom lambda
we can transform the variable's distribution:

.. image:: ../../images/lotarea_pt_custom_exp.png

|

Power transformations are expected to reshape the data distribution, reducing the 
impact of extreme outliers and, therefore, lowering the variance.

Since the power transformation changes the scale of the data, we cannot directly 
compare the variance. Instead, we'll compute the coefficient of variation (CV). 
The CV for a sample is defined as the ratio of the standard deviation to the mean, 
and it's expressed as :math:`CV = \left(\frac{s}{\overline{x}}\right)`.

Let's now use the CV to assess the impact of the data transformations on the variance.

.. code:: python

	# Compute coefficient of variation (CV)

	def compute_cv(data):
		"""Compute the coefficient of variation (CV) for a given dataset."""
		return np.std(data, ddof=1) / np.mean(data) if np.mean(data) != 0 else np.inf

	cv_raw_data = compute_cv(X_train['LotArea'])
	cv_transformed_data = compute_cv(X_train_transformed['LotArea'])
	cv_transformed_data_custom = compute_cv(X_train_transformed_custom['LotArea'])

	print(f"""
	Raw data CV: {cv_raw_data:.2%}
	Transformed data exp:0.5 CV: {cv_transformed_data:.2%}
	Transformed data exp:0.001 CV (custom): {cv_transformed_data_custom:.2%}
	""")

In the following output, we can see the resulting CV for both 
original and transformed data:

.. code:: text

	Raw data CV: 105.44%
	Transformed data exp:0.5 CV: 30.91%
	Transformed data exp:0.001 CV (custom): 0.05%

By comparing the coefficient of variation (CV) for the raw and transformed data, 
the effectiveness of the transformation is noticeable. The CV for the original CV is 
higher than 1 (100%), which means that the variance is higher than the mean 
(due to the highly skewed data). The transformation with the squared root 
transformation (default exp parameter) resulted in a CV of approximately 31%. 
Finally, a power transformation with a lower exp parameter value resulted in 0.05% CV, 
drastically lower than the original and square-root transformed data.

It's worth noting that despite exhibiting a low coefficient of variation (CV), 
which measures variability relative to the mean, a feature can retain enough absolute 
variance to effectively contribute to the performance of machine learning models, 
especially in algorithms that hinge on the assumption of data variability, 
like linear regression and other regression-based models.

Choosing lambda accordingly to the distribution
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In this section, we'll further explore the impact of the lambda parameter for 
left- and right-skewed distributions.

First, let's create a toy dataset with these distributions:

.. code:: python

	# Set random seed for reproducibility
	np.random.seed(42)

	# Generating right-skewed data using exponential distribution
	right_skewed_data = np.random.exponential(scale=2, size=1000)

	# Generating left-skewed data by flipping the right-skewed data
	left_skewed_data = -np.random.gamma(shape=2, scale=2, size=1000) \
		+ np.max(np.random.gamma(shape=2, scale=2, size=1000))

	# Create dataframe with simulated data
	df_sim = pd.DataFrame({
		'left_skewed': left_skewed_data,
		'right_skewed': right_skewed_data}
	)

	# Plotting the distributions
	fig, axes = plt.subplots(ncols=2, figsize=(12, 4))

	hist_params = dict(kde=True, bins=30, alpha=0.7)
	sns.histplot(df_sim.left_skewed, ax=axes[0], color='blue', **hist_params)
	sns.histplot(df_sim.right_skewed, ax=axes[1], color='red', **hist_params)

	axes[0].set_title('Left-skewed data')
	axes[1].set_title('Right-skewed data')

	plt.show()

We see the distributions of the variables we created in the following output:

.. image:: ../../images/toydata_pt_raw.png

Now, let's transform the data using :class:`PowerTransformer()` 
with the default lambda parameter (exp = 0.5):

.. code:: python

	# Set up the variable transformer (tf)
	tf = PowerTransformer(variables = ['left_skewed', 'right_skewed'])

	# Fit the transformer
	df_sim_transformed = tf.fit_transform(df_sim)

	# Plot histograms
	fig,axes = plt.subplots(ncols=2, figsize=(12,4))

	sns.histplot(
		df_sim_transformed['left_skewed'], ax=axes[0], color='blue', **hist_params
	)
	sns.histplot(
		df_sim_transformed['right_skewed'], ax=axes[1], color='red', **hist_params
	)

	axes[0].set_title('Transformed left-skewed data')
	axes[1].set_title('Transformed right-skewed data')

	plt.show()

In the following output we can see the distributions for each transformed variable:

.. image:: ../../images/toydata_pt_transformed.png

It improved the distribution, but we can do way better!

As per the guidelines we mentioned earlier, we'll use a lambda <1 for the 
right-skew distribution and a lambda >1 for the left-skew distribution:

.. code:: python

	# Set up the variable transformer (tf)
	tf_right = PowerTransformer(variables = ['right_skewed'], exp=0.246)
	tf_left = PowerTransformer(variables = ['left_skewed'], exp=4.404)

	# Fit the transformers
	tf_right.fit(df_sim)
	tf_left.fit(df_sim)

	# Plot histograms
	fig,axes = plt.subplots(ncols=2, figsize=(12,4))

	sns.histplot(
		tf_left.transform(df_sim)['left_skewed'], ax=axes[0],
		color='blue', **hist_params
	)
	sns.histplot(
		tf_right.transform(df_sim)['right_skewed'], ax=axes[1],
		color='red', **hist_params
	)

	axes[0].set_title('Transformed left-skewed data')
	axes[1].set_title('Transformed right-skewed data')

	plt.show()

In the following output we see the distribution of the transformed variables:

.. image:: ../../images/toydata_pt_transformed_custom_exp.png

Now, the distribution looks more like a Gaussian one :)

Inverse transformation
~~~~~~~~~~~~~~~~~~~~~~

Feature-engine power transformers can reverse the transformation to obtain 
the original data representation. So for example, if we apply the square root 
transformation, the transformer can square the transformed data to obtain the 
original variable. This is useful to interpret the results of the machine learning 
models and present the results of the data analysis.

In this section, we will examine how to use inverse transformations.

First, let's fit the transformer once again:

.. code:: python

	# Set up the variable transformer (tf)
	tf = PowerTransformer(variables = ['left_skewed', 'right_skewed'])

	# Fit the transformer
	df_sim_transformed = tf.fit_transform(df_sim)

Now, let's see the first rows of the original data:

.. code:: python

	df_sim.head(3)

In the following output we see the first rows of the original data:

.. code:: text

		left_skewed	right_skewed
	0	23.406936	0.938536
	1	26.282836	6.020243
	2	22.222784	2.633491

Let's see the first rows of the transformed data:

.. code:: python

	df_sim_transformed.head(3)

In the following output we see the first rows of the transformed data:

.. code:: text

		left_skewed	right_skewed
	0	4.838072	0.968781
	1	5.126679	2.453618
	2	4.714105	1.622804

Finally, let's see how we can reverse the transformation to obtain the original values:

.. code:: python

	tf.inverse_transform(df_sim_transformed).head(3)

Result of the inverse transformation:

.. code:: text

		left_skewed	right_skewed
	0	23.406936	0.938536
	1	26.282836	6.020243
	2	22.222784	2.633491

As we can see, the original data and the inverse transformed one are identical.

Considerations
--------------

Power transformations are a powerful tool to transform data to meet the assumptions 
of statistical tests and linear regression models.

In practice, we'd use the :class:`BoxCoxTransformer()` or 
:class:`YeoJohnsonTransformer()`, because they automatically find the best lambda 
for the transformation. But automation is not always better. 
Often the transformations do not return the desired output.

We should always follow up with an analysis of transformations, comparing the original 
and transformed distributions, to ensure that we obtain the results we expect.

Additional resources
--------------------

You can find more details about the :class:`PowerTransformer()` here:

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/transformation/PowerTransformer.ipynb>`_

For more details about this and other feature engineering methods
check out these resources:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/transformation/ReciprocalTransformer.rst
================================================
.. _reciprocal:

.. currentmodule:: feature_engine.transformation

ReciprocalTransformer
=====================

A reciprocal transformation involves replacing each data value x, with its reciprocal, 1/x​. This transformation is
useful for addressing heteroscedasticity, where the variability of errors in a regression model differs across values
of an independent variable, and for transforming skewed distributions into more symmetric ones. It can also linearize
certain nonlinear relationships, making them easier to model with linear regression, and improve the overall fit of a
linear model by reducing the influence of outliers or normalizing residuals.

Applications
------------

The reciprocal transformation is useful for ratios, where the values of a variable result from the division of two v
ariables. Some examples include variables like student-teacher ratio (students per teacher) or crop yield (tons per acre).

By calculating the inverse of these variables, we shift from representing students per teacher to teachers per student,
or from tons per acre to acres per ton. This transformation still makes intuitive sense and can result in a better spread
of values, that follow closer a normal distribution.

Properties
----------

- Reciprocal transformation of x is 1 / x
- The inverse of the reciprocal transformation is also the reciprocal transformation
- The range of the reciprocal function includes all real numbers except 0

Although in theory, the reciprocal function is defined for both positive and negative values, in practice, it's mostly
used to transform strictly positive variables.

ReciprocalTransformer
---------------------

The :class:`ReciprocalTransformer` applies the reciprocal transformation to numerical variables. By default, it will
find and transform all numerical variables in the dataset. A better practice would be to apply the transformer to a
selected group of variables, which you can do by passing a list with the variable names to the `variables` parameter
when setting up the transformer.

If any of the variables contains 0 as value, the transformer will raise an error.

Python examples
---------------

In the next sections, we'll demonstrate how to apply the reciprocal transformation with :class:`ReciprocalTransformer`.

We'll load the Ames house prices dataset and create a new variable that represents the square foots per car in the house
garage. Next, we'll separate the data into train and test sets:

.. code:: python

    import matplotlib.pyplot as plt

    from sklearn.datasets import fetch_openml
    from sklearn.model_selection import train_test_split
    from feature_engine.transformation import ReciprocalTransformer

    data = fetch_openml(name='house_prices', as_frame=True)
    data = data.frame

    data["sqrfootpercar"] = data['GarageArea'] / data['GarageCars']
    data = data[~data["sqrfootpercar"].isna()]

    y = data['SalePrice']
    X = data[['GarageCars', 'GarageArea', "sqrfootpercar"]]

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42)

    print(X_train.head())

In the following output we see the resulting dataset:

.. code:: python

          GarageCars  GarageArea  sqrfootpercar
    1170           1         358          358.0
    330            1         352          352.0
    969            1         264          264.0
    726            2         540          270.0
    1308           2         528          264.0

Let's plot the distribution of the variable with the square foot area per car in a garage:

.. code:: python

    X_train["sqrfootpercar"].hist(bins=50, figsize=(4,4))
    plt.title("sqrfootpercar")
    plt.show()

In the following image we can see the skewness of the variable:

.. image:: ../../images/reciprocal_transformer/reciprocal_transfomer_original.png

|

Let's now apply the reciprocal transformation to this variable:

.. code:: python

    tf = ReciprocalTransformer(variables="sqrfootpercar")

    train_t = tf.fit_transform(X_train)
    test_t = tf.transform(X_test)

Finally, let's plot the distribution after the reciprocal transformation:

.. code:: python

    train_t["sqrfootpercar"].hist(bins=50, figsize=(4,4))
    plt.title("sqrfootpercar")
    plt.show()

In the following image, we see that the reciprocal transformation made the variable's values follow more closer a
symmetric or normal distribution:

.. image:: ../../images/reciprocal_transformer/reciprocal_transfomer_new.png

Inverse transformation
~~~~~~~~~~~~~~~~~~~~~~

With :class:`ReciprocalTransformer`, we can easily revert the transformed data to it's original representation, by using
the method `inverse_transform`:

.. code:: python

	train_unt = tf.inverse_transform(train_t)
	test_unt = tf.inverse_transform(test_t)

Let's check out the reverted transformation:

.. code:: python

    train_unt["sqrfootpercar"].hist(bins=50, figsize=(4,4))
    plt.title("sqrfootpercar")
    plt.show()

As you can see in the following image, we obtained the original data by re-applying the reciprocal function to the
transformed variable:

.. image:: ../../images/reciprocal_transformer/reciprocal_transfomer_inverse.png

Pipeline of transformations
~~~~~~~~~~~~~~~~~~~~~~~~~~~

As we mentioned previously, the reciprocal transformation is suitable, in general for ratio variables, so we need to
transform other variables in the data set with other type of transformations.

Let's not plot the distribution of the 3 variables in the original data to see which transformations could be suitable
for them:

.. code:: python

    X_train.hist(bins=50, figsize=(10,10))
    plt.show()

In the following plot, we can see that, as expected, `GarageCounts` contains counts (potentially following a Poisson
distribution), and `GarageArea` is a continuous variable:

.. image:: ../../images/reciprocal_transformer/reciprocal_transformer_3plots_original.png

|

Let's then create a pipeline to apply the square root transformation to `GarageCounts` and the Box-Cox transformation
to `GarageArea`, while applying the reciprocal transformation to "sqrfootpercar":

.. code:: python

    from feature_engine.pipeline import Pipeline
    from feature_engine.transformation import PowerTransformer, BoxCoxTransformer

    from feature_engine.pipeline import Pipeline
    from feature_engine.transformation import PowerTransformer, BoxCoxTransformer

    pipe = Pipeline([
        ("reciprocal", ReciprocalTransformer(variables="sqrfootpercar")),
        ("sqrroot", PowerTransformer(variables="GarageCars", exp=1/2)),
        ("boxcox", BoxCoxTransformer(variables="GarageArea")),
    ])

Let's now fit the pipeline and transform the datasets:

.. code:: python

    train_t = pipe.fit_transform(X_train)
    test_t = pipe.transform(X_test)

And now, we can corroborate how these transformations improved the value spread across all variables by plotting the
histograms for the transformed data:

.. code:: python

    train_t.hist(bins=50, figsize=(10,10))
    plt.show()

In the following image, we can see that the variables no longer show the right-skewness, and now their values are more
symmetrically distributed across their value ranges:

.. image:: ../../images/reciprocal_transformer/reciprocal_transformer_3plots_new.png

|

An that's it! We've now applied different mathematical functions to stabilize the variance of the variables in the
dataset.

Alternatives to the reciprocal function
---------------------------------------

We mentioned that the reciprocal function is used, in practice, with positive values. If the variable contains negative
values, the Yeo-Johnson transformation, or adding a constant followed by the Box-Cox transformation might be better choices.

If the variable does not come from ratios, then, the log transform or the arcsine transformation can be employed to
handle these cases.

If the variable contains counts, then the square root transformation is better suited.

The Box-Cox transformation automates the process of finding the best transformation by exploring several functions
automatically.

All these functions are considered variance stabilizing transformations, and have been designed to transform data, to
meet the assumptions of statistical parametric tests and linear regression models.

You can apply all these functions out-of-the-box with the transformers from Feature-engine's transformation module.
Remember to follow up the transformations with proper data analysis, to ensure that the transformations returned the desired effect, otherwise, we are adding complexity to the feature engineering pipeline for now added benefit.

Alternatives with Feature-engine
--------------------------------

You can apply other variance data transformation functions with the following transformers:

- :class:`LogTransformer`: applies logarithmic transformation
- :class:`ArcsinTransformer`: applies arcsin transformation
- :class:`PowerTransformer`: applies power transformation including sqrt
- :class:`BoxCoxTransformer`: applies the Box-Cox transformation
- :class:`YeoJohnsonTransformer`: applies the Yeo-Johnson transformation

Additional resources
--------------------

You can find more details about the :class:`ReciprocalTransformer()` here:

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/transformation/ReciprocalTransformer.ipynb>`_

For more details about this and other feature engineering methods check out these resources:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/transformation/YeoJohnsonTransformer.rst
================================================
.. _yeojohnson:

.. currentmodule:: feature_engine.transformation

YeoJohnsonTransformer
=====================

The Yeo-Johnson transformation is an extension of the Box-Cox transformation, enabling power transformations on
variables with zero and negative values, in addition to positive values.

The Box-Cox transformation, on the other hand, is suitable for numeric variables that are strictly positive. When variables
include negative values, we have two options:

- shift the distribution toward positive values by adding a constant
- use the Yeo-Johnson transformation

The Yeo-Johnson transformation is defined as:

|

.. image:: ../../images/yeojohnsonformula.png

|

where **Y** is the independent variable and **λ** is the transformation parameter.

Uses of the Yeo-Johnson and Box-Cox transformation
--------------------------------------------------

Both the Yeo-Johnson and Box-Cox transformations automate the process of identifying the optimal power transformation to
approximate a Gaussian distribution. They evaluate various power transformations, including the logarithmic and reciprocal
functions, estimating the transformation parameter through maximum likelihood.

These transformations are commonly applied during data preprocessing, particularly when parametric statistical tests or
linear models for regression are employed. Such tests and models often have underlying assumptions about the data that
may not be inherently satisfied, making these transformations essential for meeting those assumptions.

Yeo-Johnson vs Box-Cox transformation
-------------------------------------

How does the Yeo-Johnson transformation relate to the Box-Cox transformation?

The Yeo-Johnson transformation extends the Box-Cox transformation to handle variables with zero, negative, and positive
values.

- For strictly positive values: The Yeo-Johnson transformation is equivalent to the Box-Cox transformation applied to (X + 1).

- For strictly negative values: The Yeo-Johnson transformation corresponds to the Box-Cox transformation applied to (-X + 1) with a power of (2 — λ), where λ is the transformation parameter.

- For variables with both positive and negative values: The Yeo-Johnson transformation combines the two approaches, using different powers for the positive and negative segments of the variable.

To apply the Yeo-Johnson transformation in Python, you can use `scipy.stats.yeojohnson`, which can transform one variable
at a time. For transforming multiple variables simultaneously, libraries like scikit-klearn and Feature-engine are more suitable.

The YeoJohnsonTransformer
-------------------------

Feature-engine's :class:`YeoJohnsonTransformer()` applies the Yeo-Johnson transformation to numeric variables.

Under the hood, :class:`YeoJohnsonTransformer()` uses `scipy.stats.yeojohnson <https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.yeojohnson.html>`_
to apply the transformations to each variable.

Python Implementation
----------------------

In this section, we will apply the Yeo-Johnson transformation to several variables from the Ames house prices dataset.
After performing the transformations, we will carry out data analysis to understand the impact on the variable distributions.

Let's begin by importing the necessary libraries and transformers, loading the dataset, and splitting it into training
and testing sets.

.. code:: python

    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    from sklearn.datasets import fetch_openml
    from sklearn.model_selection import train_test_split
    from feature_engine.transformation import YeoJohnsonTransformer

    # Load dataset
    data = fetch_openml(name='house_prices', as_frame=True)
    data = data.frame

    # Separate into train and test sets
    X_train, X_test, y_train, y_test =  train_test_split(
                data.drop(['Id', 'SalePrice'], axis=1),
                data['SalePrice'], test_size=0.3, random_state=0)

    X_train.head()

In the following output we see the predictor variables of the house prices dataset:

.. code:: python

          MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \
    254           20       RL         70.0     8400   Pave   NaN      Reg
    1066          60       RL         59.0     7837   Pave   NaN      IR1
    638           30       RL         67.0     8777   Pave   NaN      Reg
    799           50       RL         60.0     7200   Pave   NaN      Reg
    380           50       RL         50.0     5000   Pave  Pave      Reg

         LandContour Utilities LotConfig  ... ScreenPorch PoolArea PoolQC  Fence  \
    254          Lvl    AllPub    Inside  ...           0        0    NaN    NaN
    1066         Lvl    AllPub    Inside  ...           0        0    NaN    NaN
    638          Lvl    AllPub    Inside  ...           0        0    NaN  MnPrv
    799          Lvl    AllPub    Corner  ...           0        0    NaN  MnPrv
    380          Lvl    AllPub    Inside  ...           0        0    NaN    NaN

         MiscFeature MiscVal  MoSold  YrSold  SaleType  SaleCondition
    254          NaN       0       6    2010        WD         Normal
    1066         NaN       0       5    2009        WD         Normal
    638          NaN       0       5    2008        WD         Normal
    799          NaN       0       6    2007        WD         Normal
    380          NaN       0       5    2010        WD         Normal

    [5 rows x 79 columns]

Let's now set up the transformer to apply the Yeo-Johnson transformation to 2 variables; `LotArea` and `GrLivArea`:

.. code:: python

	tf = YeoJohnsonTransformer(variables = ['LotArea', 'GrLivArea'])

	tf.fit(X_train)

With `fit()`, :class:`YeoJohnsonTransformer()` learns the optimal lambda for the yeo-johnson power transformation. We
can inspect these values as follows:

.. code:: python

         tf.lambda_dict_

We see the optimal lambda values below:

.. code:: python

         {'LotArea': 0.02258978732751055, 'GrLivArea': 0.06781061353154169}

We can now go ahead and apply the data transformation to get closer to normal distributions.

.. code:: python

	train_t = tf.transform(X_train)
	test_t = tf.transform(X_test)

We'll check out the effect of the transformation in the next section.

Effect of the transformation on the variable distribution
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Let's carry out an analysis of transformations. We'll explore the variables distribution before and after applying the
transformation described by Yeo and Johnson.

Let's make histograms of the original data to check out the original variables distribution:

.. code:: python

    X_train[['LotArea', 'GrLivArea']].hist(bins=50, figsize=(10,4))
    plt.show()

In the following image, we can observe the skewness in the distribution of 'LotArea' and 'GrLivArea' in the original data:

.. image:: ../../images/untransformedcoupleYJ.png

|
|

Now, let's plot histograms of the transformed variables:

.. code:: python

    train_t[['LotArea', 'GrLivArea']].hist(bins=50, figsize=(10,4))
    plt.show()

We see that in the transformed data, both variables have a more symmetric, Gaussian-like distribution.

.. image:: ../../images/transformedcoupleYJ.png

|
|

Recovering the original data
~~~~~~~~~~~~~~~~~~~~~~~~~~~~

After applying the Yeo-Johnson transformation, we can restore the original data representation, that is, the original variable
values, using the `inverse_transform` method.

.. code:: python

        train_unt = tf.inverse_transform(train_t)
        test_unt = tf.inverse_transform(test_t)

Additional resources
--------------------

You can find more details about the :class:`YeoJohnsonTransformer()` here:

- `Jupyter notebook <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/transformation/YeoJohnsonTransformer.ipynb>`_

For more details about this and other feature engineering methods check out these resources:

.. figure::  ../../images/feml.png
   :width: 300
   :figclass: align-center
   :align: left
   :target: https:

   Feature Engineering for Machine Learning

|
|
|
|
|
|
|
|
|
|

Or read our book:

.. figure::  ../../images/cookbook.png
   :width: 200
   :figclass: align-center
   :align: left
   :target: https:

   Python Feature Engineering Cookbook

|
|
|
|
|
|
|
|
|
|
|
|
|

Both our book and course are suitable for beginners and more advanced data scientists
alike. By purchasing them you are supporting Sole, the main developer of Feature-engine.

================================================
FILE: docs/user_guide/variable_handling/check_all_variables.rst
================================================
.. _check_all_vars:

.. currentmodule:: feature_engine.variable_handling

check_all_variables
===================

With :class:`check_all_variables()` we can check that the variables in a list are
present in the dataframe.

Let's create a toy dataset with numerical, categorical and datetime variables:

.. code:: python

    import pandas as pd
    from sklearn.datasets import make_classification

    X, y = make_classification(
        n_samples=1000,
        n_features=4,
        n_redundant=1,
        n_clusters_per_class=1,
        weights=[0.50],
        class_sep=2,
        random_state=1,
    )

    # transform arrays into pandas df and series
    colnames = [f"num_var_{i+1}" for i in range(4)]
    X = pd.DataFrame(X, columns=colnames)

    X["cat_var1"] = ["Hello"] * 1000
    X["cat_var2"] = ["Bye"] * 1000

    X["date1"] = pd.date_range("2020-02-24", periods=1000, freq="T")
    X["date2"] = pd.date_range("2021-09-29", periods=1000, freq="H")
    X["date3"] = ["2020-02-24"] * 1000

    print(X.head())

We see the resulting dataframe below:

.. code:: python

       num_var_1  num_var_2  num_var_3  num_var_4 cat_var1 cat_var2  \
    0  -1.558594   1.634123   1.556932   2.869318    Hello      Bye
    1   1.499925   1.651008   1.159977   2.510196    Hello      Bye
    2   0.277127  -0.263527   0.532159   0.274491    Hello      Bye
    3  -1.139190  -1.131193   2.296540   1.189781    Hello      Bye
    4  -0.530061  -2.280109   2.469580   0.365617    Hello      Bye

                    date1               date2       date3
    0 2020-02-24 00:00:00 2021-09-29 00:00:00  2020-02-24
    1 2020-02-24 00:01:00 2021-09-29 01:00:00  2020-02-24
    2 2020-02-24 00:02:00 2021-09-29 02:00:00  2020-02-24
    3 2020-02-24 00:03:00 2021-09-29 03:00:00  2020-02-24
    4 2020-02-24 00:04:00 2021-09-29 04:00:00  2020-02-24

We can use :class:`check_all_variables()` with a list of variable names to verify that
the variables in the list are in the dataframe.

.. code:: python

    from feature_engine.variable_handling import check_all_variables

    checked_vars = check_all_variables(X, ["num_var_1", "cat_var1", "date1"])

    checked_vars

The output is the list of variable names passed to the function:

.. code:: python

    ['num_var_1', 'cat_var1', 'date1']

If we pass the name of a variable that is not in the dataframe, :class:`check_all_variables()`
will return an error:

.. code:: python

    check_all_variables(X, ["hola", "cat_var1", "date1"])

Below we see the error message:

.. code:: python

    KeyError: 'Some of the variables are not in the dataframe.'

================================================
FILE: docs/user_guide/variable_handling/check_categorical_variables.rst
================================================
.. _check_cat_vars:

.. currentmodule:: feature_engine.variable_handling

check_categorical_variables
===========================

:class:`check_categorical_variables()` checks that the variables in the list are of
type object or categorical.

Let's create a toy dataset with numerical, categorical and datetime variables:

.. code:: python

    import pandas as pd
    from sklearn.datasets import make_classification

    X, y = make_classification(
        n_samples=1000,
        n_features=4,
        n_redundant=1,
        n_clusters_per_class=1,
        weights=[0.50],
        class_sep=2,
        random_state=1,
    )

    # transform arrays into pandas df and series
    colnames = [f"num_var_{i+1}" for i in range(4)]
    X = pd.DataFrame(X, columns=colnames)

    X["cat_var1"] = ["Hello"] * 1000
    X["cat_var2"] = ["Bye"] * 1000

    X["date1"] = pd.date_range("2020-02-24", periods=1000, freq="T")
    X["date2"] = pd.date_range("2021-09-29", periods=1000, freq="H")
    X["date3"] = ["2020-02-24"] * 1000

    print(X.head())

We see the resulting dataframe below:

.. code:: python

       num_var_1  num_var_2  num_var_3  num_var_4 cat_var1 cat_var2  \
    0  -1.558594   1.634123   1.556932   2.869318    Hello      Bye
    1   1.499925   1.651008   1.159977   2.510196    Hello      Bye
    2   0.277127  -0.263527   0.532159   0.274491    Hello      Bye
    3  -1.139190  -1.131193   2.296540   1.189781    Hello      Bye
    4  -0.530061  -2.280109   2.469580   0.365617    Hello      Bye

                    date1               date2       date3
    0 2020-02-24 00:00:00 2021-09-29 00:00:00  2020-02-24
    1 2020-02-24 00:01:00 2021-09-29 01:00:00  2020-02-24
    2 2020-02-24 00:02:00 2021-09-29 02:00:00  2020-02-24
    3 2020-02-24 00:03:00 2021-09-29 03:00:00  2020-02-24
    4 2020-02-24 00:04:00 2021-09-29 04:00:00  2020-02-24

Let's now check that 3 of the variables are of type numerical:

.. code:: python

    from feature_engine.variable_handling import check_categorical_variables

    var_cat = check_categorical_variables(X, ["cat_var1", "date3"])

    var_cat

Both variables are of type object and hence, will be in the resulting list:

.. code:: python

    ['cat_var1', 'date3']

If we pass a variable that is not of type object or categorical,
:class:`check_categorical_variables()` will return an error:

.. code:: python

    check_categorical_variables(X, ["cat_var1", "num_var_1"])

Below we see the error message:

.. code:: python

    TypeError: Some of the variables are not categorical. Please cast them as object
    or categorical before using this transformer.

================================================
FILE: docs/user_guide/variable_handling/check_datetime_variables.rst
================================================
.. _check_datetime_vars:

.. currentmodule:: feature_engine.variable_handling

check_datetime_variables
========================

:class:`check_datetime_variables()` checks that the variables in the list are, or can
be parsed as datetime.

Let's create a toy dataset with numerical, categorical and datetime variables:

.. code:: python

    import pandas as pd
    from sklearn.datasets import make_classification

    X, y = make_classification(
        n_samples=1000,
        n_features=4,
        n_redundant=1,
        n_clusters_per_class=1,
        weights=[0.50],
        class_sep=2,
        random_state=1,
    )

    # transform arrays into pandas df and series
    colnames = [f"num_var_{i+1}" for i in range(4)]
    X = pd.DataFrame(X, columns=colnames)

    X["cat_var1"] = ["Hello"] * 1000
    X["cat_var2"] = ["Bye"] * 1000

    X["date1"] = pd.date_range("2020-02-24", periods=1000, freq="T")
    X["date2"] = pd.date_range("2021-09-29", periods=1000, freq="H")
    X["date3"] = ["2020-02-24"] * 1000

    print(X.head())

We see the resulting dataframe below:

.. code:: python

       num_var_1  num_var_2  num_var_3  num_var_4 cat_var1 cat_var2  \
    0  -1.558594   1.634123   1.556932   2.869318    Hello      Bye
    1   1.499925   1.651008   1.159977   2.510196    Hello      Bye
    2   0.277127  -0.263527   0.532159   0.274491    Hello      Bye
    3  -1.139190  -1.131193   2.296540   1.189781    Hello      Bye
    4  -0.530061  -2.280109   2.469580   0.365617    Hello      Bye

                    date1               date2       date3
    0 2020-02-24 00:00:00 2021-09-29 00:00:00  2020-02-24
    1 2020-02-24 00:01:00 2021-09-29 01:00:00  2020-02-24
    2 2020-02-24 00:02:00 2021-09-29 02:00:00  2020-02-24
    3 2020-02-24 00:03:00 2021-09-29 03:00:00  2020-02-24
    4 2020-02-24 00:04:00 2021-09-29 04:00:00  2020-02-24

The dataframe has 3 datetime variables, two of them are of type datetime and one of type
object.

Let's check that a list of variables can be parsed as datetime:

.. code:: python

    from feature_engine.variable_handling import check_datetime_variables

    var_date = check_datetime_variables(X, ["date2", "date3"])

    var_date

In this case, both variables, if they can be parsed as datetime, will be in the
resulting list:

.. code:: python

    ['date2', 'date3']

If we pass a variable that can't be parsed as datetime, :class:`check_datetime_variables()`
will return an error:

.. code:: python

    check_datetime_variables(X, ["date2", "cat_var1"])

Below the error message:

.. code:: python

    TypeError: Some of the variables are not or cannot be parsed as datetime.

================================================
FILE: docs/user_guide/variable_handling/check_numerical_variables.rst
================================================
.. _check_num_vars:

.. currentmodule:: feature_engine.variable_handling

check_numerical_variables
=========================

:class:`check_numerical_variables()` checks that the variables in the list are of
type numerical.

Let's create a toy dataset with numerical, categorical and datetime variables:

.. code:: python

    import pandas as pd
    df = pd.DataFrame({
        "Name": ["tom", "nick", "krish", "jack"],
        "City": ["London", "Manchester", "Liverpool", "Bristol"],
        "Age": [20, 21, 19, 18],
        "Marks": [0.9, 0.8, 0.7, 0.6],
        "dob": pd.date_range("2020-02-24", periods=4, freq="T"),
    })

    print(df.head())

We see the resulting dataframe below:

.. code:: python

        Name        City  Age  Marks                 dob
    0    tom      London   20    0.9 2020-02-24 00:00:00
    1   nick  Manchester   21    0.8 2020-02-24 00:01:00
    2  krish   Liverpool   19    0.7 2020-02-24 00:02:00
    3   jack     Bristol   18    0.6 2020-02-24 00:03:00

Let's now check that 2 of the variables are of type numerical:

.. code:: python

    from feature_engine.variable_handling import check_numerical_variables

    var_num = check_numerical_variables(df, ['Age', 'Marks'])

    var_num

If the variables are numerical, the function returns their names in a list:

.. code:: python

    ['Age', 'Marks']

If we pass a variable that is not of type numerical,
:class:`check_numerical_variables()` will return an error:

.. code:: python

    check_numerical_variables(df, ['Age', 'Name'])

Below we see the error message:

.. code:: python

    TypeError: Some of the variables are not numerical. Please cast them as numerical
    before using this transformer.

================================================
FILE: docs/user_guide/variable_handling/find_all_variables.rst
================================================
﻿.. _find_all_vars:

.. currentmodule:: feature_engine.variable_handling

find_all_variables
==================

With :class:`find_all_variables()` you can automatically capture in a list the names of
all the variables in the dataset.

Let's create a toy dataset with numerical, categorical and datetime variables:

.. code:: python

    import pandas as pd
    from sklearn.datasets import make_classification

    X, y = make_classification(
        n_samples=1000,
        n_features=4,
        n_redundant=1,
        n_clusters_per_class=1,
        weights=[0.50],
        class_sep=2,
        random_state=1,
    )

    # transform arrays into pandas df and series
    colnames = [f"num_var_{i+1}" for i in range(4)]
    X = pd.DataFrame(X, columns=colnames)

    X["cat_var1"] = ["Hello"] * 1000
    X["cat_var2"] = ["Bye"] * 1000

    X["date1"] = pd.date_range("2020-02-24", periods=1000, freq="T")
    X["date2"] = pd.date_range("2021-09-29", periods=1000, freq="H")
    X["date3"] = ["2020-02-24"] * 1000

    print(X.head())

We see the resulting dataframe below:

.. code:: python

       num_var_1  num_var_2  num_var_3  num_var_4 cat_var1 cat_var2  \
    0  -1.558594   1.634123   1.556932   2.869318    Hello      Bye
    1   1.499925   1.651008   1.159977   2.510196    Hello      Bye
    2   0.277127  -0.263527   0.532159   0.274491    Hello      Bye
    3  -1.139190  -1.131193   2.296540   1.189781    Hello      Bye
    4  -0.530061  -2.280109   2.469580   0.365617    Hello      Bye

                    date1               date2       date3
    0 2020-02-24 00:00:00 2021-09-29 00:00:00  2020-02-24
    1 2020-02-24 00:01:00 2021-09-29 01:00:00  2020-02-24
    2 2020-02-24 00:02:00 2021-09-29 02:00:00  2020-02-24
    3 2020-02-24 00:03:00 2021-09-29 03:00:00  2020-02-24
    4 2020-02-24 00:04:00 2021-09-29 04:00:00  2020-02-24

We can now use :class:`find_all_variables()` to capture all the variable names in a list.
So let's do that and then display the items in the list:

.. code:: python

    from feature_engine.variable_handling import find_all_variables

    vars_all = find_all_variables(X)

    vars_all

We see the variable names in the list below:

.. code:: python

    ['num_var_1',
     'num_var_2',
     'num_var_3',
     'num_var_4',
     'cat_var1',
     'cat_var2',
     'date1',
     'date2',
     'date3']

We have the option to return the name of the variables of type categorical, object and
numerical only, or in other words, to exclude datetime variables. We can do so as
follows:

.. code:: python

    vars_all = find_all_variables(X, exclude_datetime=True)

    vars_all

In the list below, we can see that variables of type datetime were ignored:

.. code:: python

    ['num_var_1',
     'num_var_2',
     'num_var_3',
     'num_var_4',
     'cat_var1',
     'cat_var2',
     'date3']

================================================
FILE: docs/user_guide/variable_handling/find_categorical_and_numerical_variables.rst
================================================
﻿.. _find_cat_and_num_vars:

.. currentmodule:: feature_engine.variable_handling

find_categorical_and_numerical_variables
========================================

With :class:`find_categorical_and_numerical_variables()` you can automatically capture in
2 separate lists the names of all the categorical and numerical variables in the dataset,
respectively.

Let's create a toy dataset with numerical, categorical and datetime variables:

.. code:: python

    import pandas as pd
    from sklearn.datasets import make_classification

    X, y = make_classification(
        n_samples=1000,
        n_features=4,
        n_redundant=1,
        n_clusters_per_class=1,
        weights=[0.50],
        class_sep=2,
        random_state=1,
    )

    # transform arrays into pandas df and series
    colnames = [f"num_var_{i+1}" for i in range(4)]
    X = pd.DataFrame(X, columns=colnames)

    X["cat_var1"] = ["Hello"] * 1000
    X["cat_var2"] = ["Bye"] * 1000

    X["date1"] = pd.date_range("2020-02-24", periods=1000, freq="T")
    X["date2"] = pd.date_range("2021-09-29", periods=1000, freq="H")
    X["date3"] = ["2020-02-24"] * 1000

    print(X.head())

Below we see the resulting dataframe:

.. code:: python

       num_var_1  num_var_2  num_var_3  num_var_4 cat_var1 cat_var2  \
    0  -1.558594   1.634123   1.556932   2.869318    Hello      Bye
    1   1.499925   1.651008   1.159977   2.510196    Hello      Bye
    2   0.277127  -0.263527   0.532159   0.274491    Hello      Bye
    3  -1.139190  -1.131193   2.296540   1.189781    Hello      Bye
    4  -0.530061  -2.280109   2.469580   0.365617    Hello      Bye

                    date1               date2       date3
    0 2020-02-24 00:00:00 2021-09-29 00:00:00  2020-02-24
    1 2020-02-24 00:01:00 2021-09-29 01:00:00  2020-02-24
    2 2020-02-24 00:02:00 2021-09-29 02:00:00  2020-02-24
    3 2020-02-24 00:03:00 2021-09-29 03:00:00  2020-02-24
    4 2020-02-24 00:04:00 2021-09-29 04:00:00  2020-02-24

We can now use :class:`find_categorical_and_numerical_variables()` to capture categorical
and numerical variables in separate lists. So let's do that and then display the lists:

.. code:: python

    from feature_engine.variable_handling import find_categorical_and_numerical_variables

    var_cat, var_num = find_categorical_and_numerical_variables(X)

    var_cat, var_num

Below we see the names of the categorical variables, followed by the names of the numerical
variables:

.. code:: python

    (['cat_var1', 'cat_var2'],
     ['num_var_1', 'num_var_2', 'num_var_3', 'num_var_4'])

We can also use :class:`find_categorical_and_numerical_variables()` with a list of variables,
to indentify their types:

.. code:: python

    var_cat, var_num = find_categorical_and_numerical_variables(X, ["num_var_1", "cat_var1"])

    var_cat, var_num

We see the resulting lists below:

.. code:: python

    (['cat_var1'], ['num_var_1'])

If we pass a variable that is not of type numerical or categorical, :class:`find_categorical_and_numerical_variables()`
will return an error:

.. code:: python

    find_categorical_and_numerical_variables(X, ["num_var_1", "cat_var1", "date1"])

Below the error message:

.. code:: python

    TypeError: Some of the variables are neither numerical nor categorical.

================================================
FILE: docs/user_guide/variable_handling/find_categorical_variables.rst
================================================
﻿.. _find_cat_vars:

.. currentmodule:: feature_engine.variable_handling

find_categorical_variables
==========================

With :class:`find_categorical_variables()` you can capture in a list the names of all
the variables of type object or categorical in the dataset.

Let's create a toy dataset with numerical, categorical and datetime variables:

.. code:: python

    import pandas as pd
    from sklearn.datasets import make_classification

    X, y = make_classification(
        n_samples=1000,
        n_features=4,
        n_redundant=1,
        n_clusters_per_class=1,
        weights=[0.50],
        class_sep=2,
        random_state=1,
    )

    # transform arrays into pandas df and series
    colnames = [f"num_var_{i+1}" for i in range(4)]
    X = pd.DataFrame(X, columns=colnames)

    X["cat_var1"] = ["Hello"] * 1000
    X["cat_var2"] = ["Bye"] * 1000

    X["date1"] = pd.date_range("2020-02-24", periods=1000, freq="T")
    X["date2"] = pd.date_range("2021-09-29", periods=1000, freq="H")
    X["date3"] = ["2020-02-24"] * 1000

    print(X.head())

We see the resulting dataframe below:

.. code:: python

       num_var_1  num_var_2  num_var_3  num_var_4 cat_var1 cat_var2  \
    0  -1.558594   1.634123   1.556932   2.869318    Hello      Bye
    1   1.499925   1.651008   1.159977   2.510196    Hello      Bye
    2   0.277127  -0.263527   0.532159   0.274491    Hello      Bye
    3  -1.139190  -1.131193   2.296540   1.189781    Hello      Bye
    4  -0.530061  -2.280109   2.469580   0.365617    Hello      Bye

                    date1               date2       date3
    0 2020-02-24 00:00:00 2021-09-29 00:00:00  2020-02-24
    1 2020-02-24 00:01:00 2021-09-29 01:00:00  2020-02-24
    2 2020-02-24 00:02:00 2021-09-29 02:00:00  2020-02-24
    3 2020-02-24 00:03:00 2021-09-29 03:00:00  2020-02-24
    4 2020-02-24 00:04:00 2021-09-29 04:00:00  2020-02-24

We can use :class:`find_categorical_variables()` to capture the names of all
variables of type object or categorical in a list.

So let's do that and then display the list:

.. code:: python

    from feature_engine.variable_handling import find_categorical_variables

    var_cat = find_categorical_variables(X)

    var_cat

We see the variable names in the list below:

.. code:: python

    ['cat_var1', 'cat_var2']

Note that :class:`find_categorical_variables()` will not return variables cast as
object or categorical that could be parsed as datetime. That's why, the variable
`date3` was excluded from the returned list.

If there are no categorical variables in the dataset, this function will raise an
error.

================================================
FILE: docs/user_guide/variable_handling/find_datetime_variables.rst
================================================
﻿.. _find_datetime_vars:

.. currentmodule:: feature_engine.variable_handling

find_datetime_variables
=======================

With :class:`find_datetime_variables()` you can automatically capture in a list
the names of all datetime variables in a dataset, whether they are parsed as datetime
or object.

Let's create a toy dataset with numerical, categorical and datetime variables:

.. code:: python

    import pandas as pd
    from sklearn.datasets import make_classification

    X, y = make_classification(
        n_samples=1000,
        n_features=4,
        n_redundant=1,
        n_clusters_per_class=1,
        weights=[0.50],
        class_sep=2,
        random_state=1,
    )

    # transform arrays into pandas df and series
    colnames = [f"num_var_{i+1}" for i in range(4)]
    X = pd.DataFrame(X, columns=colnames)

    X["cat_var1"] = ["Hello"] * 1000
    X["cat_var2"] = ["Bye"] * 1000

    X["date1"] = pd.date_range("2020-02-24", periods=1000, freq="T")
    X["date2"] = pd.date_range("2021-09-29", periods=1000, freq="H")
    X["date3"] = ["2020-02-24"] * 1000

    print(X.head())

We see the resulting dataframe below:

.. code:: python

       num_var_1  num_var_2  num_var_3  num_var_4 cat_var1 cat_var2  \
    0  -1.558594   1.634123   1.556932   2.869318    Hello      Bye
    1   1.499925   1.651008   1.159977   2.510196    Hello      Bye
    2   0.277127  -0.263527   0.532159   0.274491    Hello      Bye
    3  -1.139190  -1.131193   2.296540   1.189781    Hello      Bye
    4  -0.530061  -2.280109   2.469580   0.365617    Hello      Bye

                    date1               date2       date3
    0 2020-02-24 00:00:00 2021-09-29 00:00:00  2020-02-24
    1 2020-02-24 00:01:00 2021-09-29 01:00:00  2020-02-24
    2 2020-02-24 00:02:00 2021-09-29 02:00:00  2020-02-24
    3 2020-02-24 00:03:00 2021-09-29 03:00:00  2020-02-24
    4 2020-02-24 00:04:00 2021-09-29 04:00:00  2020-02-24

The dataframe has 3 datetime variables, two of them are of type datetime and one of type
object.

We can now use :class:`find_datetime_variables()` to capture all datetime variables
regardless of their data type. So let's do that and then display the list:

.. code:: python

    from feature_engine.variable_handling import find_datetime_variables

    var_date = find_datetime_variables(X)

    var_date

Below we see the variable names in the list:

.. code:: python

    ['date1', 'date2', 'date3']

Note that :class:`find_datetime_variables()` captures all 3 datetime variables.
The first 2 are of type datetime, whereas the third variable is of type object. But as it
can be parsed as datetime, it will be captured in the list as well.

================================================
FILE: docs/user_guide/variable_handling/find_numerical_variables.rst
================================================
﻿.. _find_num_vars:

.. currentmodule:: feature_engine.variable_handling

find_numerical_variables
========================

:class:`find_numerical_variables()` returns a list with the names of the numerical
variables in the dataset.

Let's create a toy dataset with numerical, categorical and datetime variables:

.. code:: python

    import pandas as pd
    df = pd.DataFrame({
        "Name": ["tom", "nick", "krish", "jack"],
        "City": ["London", "Manchester", "Liverpool", "Bristol"],
        "Age": [20, 21, 19, 18],
        "Marks": [0.9, 0.8, 0.7, 0.6],
        "dob": pd.date_range("2020-02-24", periods=4, freq="T"),
    })

    print(df.head())

We see the resulting dataframe below:

.. code:: python

        Name        City  Age  Marks                 dob
    0    tom      London   20    0.9 2020-02-24 00:00:00
    1   nick  Manchester   21    0.8 2020-02-24 00:01:00
    2  krish   Liverpool   19    0.7 2020-02-24 00:02:00
    3   jack     Bristol   18    0.6 2020-02-24 00:03:00

With :class:`find_numerical_variables()` we capture the names of all numerical
variables in a list. So let's do that and then display the list:

.. code:: python

    from feature_engine.variable_handling import find_numerical_variables

    var_num = find_numerical_variables(df)

    var_num

We see the names of the numerical variables in the list below:

.. code:: python

    ['Age', 'Marks']

If there are no numerical variables in the dataset, :class:`find_numerical_variables()`
will raise an error.

================================================
FILE: docs/user_guide/variable_handling/index.rst
================================================
.. -*- mode: rst -*-

Variable handling functions
===========================

This set of functions find variables of a specific type in a dataframe, or check that a
list of variables is of a specified data type.

The `find` functions take a dataframe as an argument and returns a list with the names
of the variables of the desired type.

The `check` functions check that the list of variables are all of the desired data type.

The `retain` functions select the variables in a list if they fulfill a condition.

You can use these functions to identify different sets of variables based on their
data type to streamline your feature engineering pipelines or create your own
Feature-engine or Scikit-learn compatible transformers.

.. toctree::
   :maxdepth: 1

   find_all_variables
   find_categorical_variables
   find_datetime_variables
   find_numerical_variables
   find_categorical_and_numerical_variables
   check_all_variables
   check_categorical_variables
   check_datetime_variables
   check_numerical_variables
   retain_variables_if_in_df

================================================
FILE: docs/user_guide/variable_handling/retain_variables_if_in_df.rst
================================================
.. _retain_vars:

.. currentmodule:: feature_engine.variable_handling

retain_variables_if_in_df
=========================

:class:`retain_variables_if_in_df()` returns the subset of variables in a list that
is present in the dataset.

Let's create a toy dataset with numerical, categorical and datetime variables:

.. code:: python

    import pandas as pd
    df = pd.DataFrame({
        "Name": ["tom", "nick", "krish", "jack"],
        "City": ["London", "Manchester", "Liverpool", "Bristol"],
        "Age": [20, 21, 19, 18],
        "Marks": [0.9, 0.8, 0.7, 0.6],
        "dob": pd.date_range("2020-02-24", periods=4, freq="T"),
    })

    print(df.head())

We see the resulting dataframe below:

.. code:: python

        Name        City  Age  Marks                 dob
    0    tom      London   20    0.9 2020-02-24 00:00:00
    1   nick  Manchester   21    0.8 2020-02-24 00:01:00
    2  krish   Liverpool   19    0.7 2020-02-24 00:02:00
    3   jack     Bristol   18    0.6 2020-02-24 00:03:00

With :class:`retain_variables_if_in_df()` we capture in a list, the names of the
variables that are present in the dataset. So let's do that and then display the
resulting list:

.. code:: python

    from feature_engine.variable_handling import retain_variables_if_in_df

    vars_in_df = retain_variables_if_in_df(df, variables = ["Name", "City", "Dogs"])

    var_in_df

We see the names of the subset of variables that are in the dataframe below:

.. code:: python

    ['Name', 'City']

If none of variables in the list are in the dataset, :class:`retain_variables_if_in_df()`
will raise an error.

Uses
----

This function was originally developed for internal use.

When we run various feature selection transformers one after the other, for example,
`DropConstantFeatures`, then `DropDuplicateFeatures`, and finally
`RecursiveFeatureElimination`, we can't anticipate which variables will be dropped by
each transformer. Hence, these transformers use :class:`retain_variables_if_in_df()`
under the hood, to select those variables that were entered by the user and that still
remain in the dataset, before applying the selection algorithm.

We've now decided to expose this function as part of the `variable_handling` module. It
might be useful, for example, if you are creating `Feature-engine` compatible selection
transformers.

================================================
FILE: docs/user_guide/wrappers/index.rst
================================================
.. -*- mode: rst -*-

Scikit-learn Wrapper
====================

Feature-engine's Scikit-learn wrappers wrap Scikit-learn transformers allowing their
implementation only on a selected subset of features.

.. toctree::
   :maxdepth: 1

   Wrapper

================================================
FILE: docs/user_guide/wrappers/Wrapper.rst
================================================
.. _sklearn_wrapper:

.. currentmodule:: feature_engine.wrappers

SklearnTransformerWrapper
=========================

The :class:`SklearnTransformerWrapper()` applies Scikit-learn transformers to a selected
group of variables. It works with transformers like the SimpleImputer, OrdinalEncoder,
OneHotEncoder, KBinsDiscretizer, all scalers and also transformers for feature selection.
Other transformers have not been tested, but we think it should work with most of them.

The :class:`SklearnTransformerWrapper()` offers similar functionality to the
`ColumnTransformer <https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html>`_
class available in Scikit-learn. They differ in the implementation to select the
variables and the output.

The :class:`SklearnTransformerWrapper()` returns a pandas dataframe with the variables
in the order of the original data. The
`ColumnTransformer <https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html>`_
returns a Numpy array, and the order of the variables may not coincide with that of the
original dataset.

In the next code snippet we show how to wrap the SimpleImputer from Scikit-learn to
impute only the selected variables.

.. code:: python

    import pandas as pd
    import numpy as np
    from sklearn.model_selection import train_test_split
    from sklearn.impute import SimpleImputer
    from feature_engine.wrappers import SklearnTransformerWrapper
	
    # Load dataset
    data = pd.read_csv('houseprice.csv')
    
    # Separate into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(
    	data.drop(['Id', 'SalePrice'], axis=1),
    	data['SalePrice'], test_size=0.3, random_state=0)
    	
    # set up the wrapper with the SimpleImputer
    imputer = SklearnTransformerWrapper(transformer = SimpleImputer(strategy='mean'),
                                        variables = ['LotFrontage', 'MasVnrArea'])
    
    # fit the wrapper + SimpleImputer                              
    imputer.fit(X_train)
	
    # transform the data
    X_train = imputer.transform(X_train)
    X_test = imputer.transform(X_test)

In the next snippet of code we show how to wrap the StandardScaler from Scikit-learn
to standardize only the selected variables.

.. code:: python

    import pandas as pd
    import numpy as np
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
    from feature_engine.wrappers import SklearnTransformerWrapper

    # Load dataset
    data = pd.read_csv('houseprice.csv')

    # Separate into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(
    	data.drop(['Id', 'SalePrice'], axis=1),
    	data['SalePrice'], test_size=0.3, random_state=0)

    # set up the wrapper with the StandardScaler
    scaler = SklearnTransformerWrapper(transformer = StandardScaler(),
                                        variables = ['LotFrontage', 'MasVnrArea'])

    # fit the wrapper + StandardScaler
    scaler.fit(X_train)

    # transform the data
    X_train = scaler.transform(X_train)
    X_test = scaler.transform(X_test)

In the next snippet of code we show how to wrap the SelectKBest from Scikit-learn
to select only a subset of the variables.

.. code:: python

    import pandas as pd
    import numpy as np
    from sklearn.model_selection import train_test_split
    from sklearn.feature_selection import f_regression, SelectKBest
    from feature_engine.wrappers import SklearnTransformerWrapper

    # Load dataset
    data = pd.read_csv('houseprice.csv')

    # Separate into train and test sets
    X_train, X_test, y_train, y_test = train_test_split(
    	data.drop(['Id', 'SalePrice'], axis=1),
    	data['SalePrice'], test_size=0.3, random_state=0)

    cols = [var for var in X_train.columns if X_train[var].dtypes !='O']

    # let's apply the standard scaler on the above variables

    selector = SklearnTransformerWrapper(
        transformer = SelectKBest(f_regression, k=5),
        variables = cols)

    selector.fit(X_train.fillna(0), y_train)

    # transform the data
    X_train_t = selector.transform(X_train.fillna(0))
    X_test_t = selector.transform(X_test.fillna(0))

Even though Feature-engine has its own implementation of OneHotEncoder, you may want 
to use Scikit-Learn's transformer in order to access different options, 
such as drop first Category. 
In the following example, we show you how to apply Scikit-learn's OneHotEncoder to a 
subset of categories using the :class:SklearnTransformerWrapper().

.. code:: python

    import pandas as pd
    import numpy as np
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import OneHotEncoder
    from feature_engine.wrappers import SklearnTransformerWrapper

    # Load dataset
    def load_titanic():
        data = pd.read_csv('https://www.openml.org/data/get_csv/16826755/phpMYEkMl')
        data = data.replace('?', np.nan)
        data['cabin'] = data['cabin'].astype(str).str[0]
        data['pclass'] = data['pclass'].astype('O')
        data['embarked'].fillna('C', inplace=True)
        data.drop(["name", "home.dest", "ticket", "boat", "body"], axis=1, inplace=True)
        return data

    df = load_titanic()

    X_train, X_test, y_train, y_test= train_test_split(
        df.drop("survived", axis=1),
        df["survived"],
        test_size=0.2,
        random_state=42,
    )

    ohe = SklearnTransformerWrapper(
            OneHotEncoder(sparse=False, drop='first'),
            variables = ['pclass','sex'])

    ohe.fit(X_train)

    X_train_transformed = ohe.transform(X_train)
    X_test_transformed = ohe.transform(X_test)

We can examine the result by executing the following:

.. code:: python

   print(X_train_transformed.head())

The resulting dataframe is:

.. code:: python

         age  sibsp  parch     fare cabin embarked  pclass_2  pclass_3  sex_male
    772   17      0      0   7.8958     n        S       0.0       1.0       1.0
    543   36      0      0     10.5     n        S       1.0       0.0       1.0
    289   18      0      2    79.65     E        S       0.0       0.0       0.0
    10    47      1      0  227.525     C        C       0.0       0.0       1.0
    147  NaN      0      0     42.4     n        S       0.0       0.0       1.0

Let's say you want to use :class:`SklearnTransformerWrapper()` in a more complex 
context. As you may note there are `?` signs to denote unknown values. Due to the 
complexity of the transformations needed we'll use a Pipeline to impute missing values, 
encode categorical features and create interactions for specific variables using 
Scikit-Learn's PolynomialFeatures.

.. code:: python
    
    import pandas as pd
    import numpy as np
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import PolynomialFeatures
    from sklearn.pipeline import Pipeline
    from feature_engine.datasets import load_titanic
    from feature_engine.imputation import CategoricalImputer, MeanMedianImputer
    from feature_engine.encoding import OrdinalEncoder
    from feature_engine.wrappers import SklearnTransformerWrapper

    X, y = load_titanic(
        return_X_y_frame=True,
        predictors_only=True,
        cabin="letter_only",
    )

    X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=42)

    pipeline = Pipeline(steps = [
        ('ci', CategoricalImputer(imputation_method='frequent')),
        ('mmi', MeanMedianImputer(imputation_method='mean')),
        ('od', OrdinalEncoder(encoding_method='arbitrary')),
        ('pl', SklearnTransformerWrapper(
            PolynomialFeatures(interaction_only = True, include_bias=False),
            variables=['pclass','sex']))
    ])

    pipeline.fit(X_train)
    X_train_transformed = pipeline.transform(X_train)
    X_test_transformed = pipeline.transform(X_test)

    print(X_train_transformed.head())

.. code:: python

               age  sibsp  parch      fare  cabin  embarked  pclass  sex  \
    772  17.000000      0      0    7.8958      0         0     3.0  0.0
    543  36.000000      0      0   10.5000      0         0     2.0  0.0
    289  18.000000      0      2   79.6500      1         0     1.0  1.0
    10   47.000000      1      0  227.5250      2         1     1.0  0.0
    147  29.532738      0      0   42.4000      0         0     1.0  0.0

         pclass sex
    772         0.0
    543         0.0
    289         1.0
    10          0.0
    147         0.0

More details
^^^^^^^^^^^^

In the following Jupyter notebooks you can find more details about how to navigate the
parameters of the :class:`SklearnTransformerWrapper()` and also access the parameters
of the Scikit-learn transformer wrapped, as well as the output of the transformations.

- `Wrap sklearn categorical encoder <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/wrappers/Sklearn-wrapper-plus-Categorical-Encoding.ipynb>`_
- `Wrap sklearn KBinsDiscretizer <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/wrappers/Sklearn-wrapper-plus-KBinsDiscretizer.ipynb>`_
- `Wrap sklearn SimpleImputer <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/wrappers/Sklearn-wrapper-plus-SimpleImputer.ipynb>`_
- `Wrap sklearn feature selectors <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/wrappers/Sklearn-wrapper-plus-feature-selection.ipynb>`_
- `Wrap sklearn scalers <https://nbviewer.org/github/feature-engine/feature-engine-examples/blob/main/wrappers/Sklearn-wrapper-plus-scalers.ipynb>`_

The notebooks can be found in a `dedicated repository <https://github.com/feature-engine/feature-engine-examples>`_.

